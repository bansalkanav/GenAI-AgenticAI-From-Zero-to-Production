{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b6774b1-ead3-468b-a0fe-fdf162c56871",
   "metadata": {},
   "source": [
    "# **Model Context Protocol**\n",
    "\n",
    "### **What is MCP?**\n",
    "MCP is a standardised way to integrate tools with your applications. As defined by Anthropic:\n",
    "> **MCP is an open protocol that standardizes how your LLM Applicaitons connect to and work with your tools and data sources.**\n",
    "\n",
    "### **The Problem**\n",
    "Since we have been using LangChain, we know this pain intimately.\n",
    "Imagine you are building an AI assistant for your company. You want it to access:\n",
    "- PostgreSQL (for customer data).\n",
    "- Slack (to read internal chats).\n",
    "- GitHub (to look at code).\n",
    "\n",
    "**Without MCP:** We have to find (or write) a specific Python library for Postgres, another for Slack, and another for GitHub. We then have to wrap each one in a LangChain Tool.\n",
    "- Imagine there are 10 different AI applications (Host) and 100 different data sources, developers have to build 1,000 different integrations. It is unscalable.\n",
    "\n",
    "**Another Example:** Imagine you have 3 favorite AI models: Claude, ChatGPT, and Gemini. And you have 3 data sources: Google Drive, Slack, and GitHub. Without MCP, if you want Claude to talk to all three, you have to write 3 specific integrations. If you want Gemini to talk to them, you have to write 3 different integrations. If you want ChatGPT to talk to them... you get the point.\n",
    "\n",
    "### **The Pain Points**\n",
    "1. Wasted Effort: Developers are rebuilding the exact same \"Google Drive Connector\" for every different AI app.\n",
    "2. More Maintanance: If Google Drive API changes, developer has to do changes in each of the \"M\" integrations.\n",
    "\n",
    "### **The Solution: The \"USB-C\" Analogy**\n",
    "Think of MCP exactly like a USB-C port.\n",
    "- Before USB-C: You had a separate charger for your phone, your laptop, your camera, and your headphones. It was messy.\n",
    "- With USB-C: You have one standard port. If you buy a hard drive with a USB-C connector, you know it will work with your MacBook, your Windows PC, or your iPad. The hard drive manufacturer doesn't need to know which computer you own.\n",
    "\n",
    "**With MCP:** You build a \"Google Drive MCP Server\" once.\n",
    "- Your LangChain Agent can plug into it.\n",
    "- Claude Desktop can plug into it.\n",
    "- Cursor/Windsurf IDE can plug into it.\n",
    "\n",
    "The **\"Server\"** (the hard drive) doesn't care who the **\"Host\"** (the computer) is. It just speaks the standard language.\n",
    "\n",
    "**Important Note:** Once you write an MCP Server, it works in LangChain, but it also works everywhere else.\n",
    "\n",
    "### **Core Architecture**\n",
    "This is the most critical part for a developer. In the MCP world, there are three distinct roles.\n",
    "1. **The Host (The \"Interface\")**\n",
    "- **What it is:** This is the application the user interacts with.\n",
    "- **Example:** In your case, your LangChain Python script is the Host. Other examples include the Claude Desktop app or an IDE like VS Code (with an MCP extension).\n",
    "- **Role:** It decides when to call a tool or ask for context. It holds the LLM key.\n",
    "\n",
    "2. **The Client (The \"Connector\")**\n",
    "- **What it is:** This is a hidden software component that lives inside the Host.\n",
    "- **Example:** langchain-mcp-adapters provides the Client logic.\n",
    "- **Role:** It maintains the 1:1 connection with the Server. It handles the handshake and sends messages back and forth. You rarely write this from scratch; you just use a library.\n",
    "\n",
    "3. **The Server (The \"Toolkit\")**\n",
    "- **What it is:** A standalone process (script) that exposes **tools**, **resources (i.e. read only data)** and **prompts**.\n",
    "- **Example:** A Python script running fastmcp that connects to a SQLite database.\n",
    "- **Role:** It waits for commands. It has no AI. It is dumb but capable. It says, \"I have a tool called get_customer_id. If you give me a name, I will return an ID.\"\n",
    "\n",
    "### **The Transport Layer: How Client and Server talk**\n",
    "The Client and Server need a wire to communicate. MCP supports two main types of \"wires.\"\n",
    "1. **Stdio (Standard Input/Output) -> Local**\n",
    "- **How it works:** The Host actually spawns the Server process directly on the same machine. They talk by printing text to the console (stdin/stdout).\n",
    "- **Analogy:** A distinct neurological connection. The brain (Host) is directly hardwired to the hand (Server).\n",
    "- **Pros:** Extremely fast, very secure (data never leaves your machine), easiest to set up.\n",
    "- **Cons:** Both must be on the same computer.\n",
    "- **Your Use Case:** We will start here. Your LangChain script will run a generic Python subprocess to talk to your tools.\n",
    "\n",
    "2. **SSE (Server-Sent Events) -> Remote/Web**\n",
    "- **How it works:** The Server runs on a distant cloud machine (like AWS or Render). The Host connects via HTTP.\n",
    "- **Analogy:** A Walkie-Talkie. The Brain is in one building, the Hand is in another. They talk over the airwaves.\n",
    "- **Pros:** You can share one Server with your whole team.\n",
    "- **Cons:** Requires authentication, network management, and is slightly more complex to deploy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d605b239-8f11-4a1e-9e05-2a4c0080ccb5",
   "metadata": {},
   "source": [
    "## **Under the Hood**\n",
    "\n",
    "### **JSON-RPC 2.0: The Language**\n",
    "MCP doesn't invent a new format; it uses the standard JSON-RPC 2.0 specification. Every message is a JSON object with a specific structure.\n",
    "- **Request:** Has an `id` and `method`\n",
    "- **Response:** Has the same `id` and `result`\n",
    "- **Notification:** Has `method` but **no id**. No reply expected.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"jsonrpc\": \"2.0\",\n",
    "  \"id\": 1,\n",
    "  \"method\": \"tools/call\",\n",
    "  \"params\": { ... }\n",
    "}\n",
    "```\n",
    "\n",
    "### **The Handshake: Establishing Client and Server Connection**\n",
    "Before any tools can be used, the Client and Server must agree on a version and capabilities. This happens automatically when the process starts.\n",
    "\n",
    "**Step A:Client (LangChain) says Hello** \n",
    "- The Client sends an initialize request. It tells the Server what protocol version it supports and what it can do.\n",
    "\n",
    "```JSON\n",
    "// CLIENT -> SERVER\n",
    "{\n",
    "  \"jsonrpc\": \"2.0\",\n",
    "  \"id\": 0,\n",
    "  \"method\": \"initialize\",\n",
    "  \"params\": {\n",
    "    \"protocolVersion\": \"2024-11-05\",\n",
    "    \"capabilities\": {\n",
    "      \"roots\": { \"listChanged\": true }, // \"I can handle file system roots\"\n",
    "      \"sampling\": {} // \"I can handle LLM sampling/generation requests\"\n",
    "    },\n",
    "    \"clientInfo\": { \"name\": \"LangChainClient\", \"version\": \"1.0\" }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Step B: Server Responds** \n",
    "- The Server replies with its own capabilities. This is crucial. If the Server says \"I don't support resources,\" the Client knows not to ask for them.\n",
    "\n",
    "```JSON\n",
    "// SERVER -> CLIENT\n",
    "{\n",
    "  \"jsonrpc\": \"2.0\",\n",
    "  \"id\": 0,\n",
    "  \"result\": {\n",
    "    \"protocolVersion\": \"2024-11-05\",\n",
    "    \"capabilities\": {\n",
    "      \"tools\": {},      // \"I have tools\"\n",
    "      \"resources\": {},  // \"I have resources\"\n",
    "      \"prompts\": {}     // \"I have prompts\"\n",
    "    },\n",
    "    \"serverInfo\": { \"name\": \"my-weather-server\", \"version\": \"0.1\" }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Step C: Client Acknowledges The Client sends a notification saying, \"Okay, we are initialized.\"**\n",
    "\n",
    "```JSON\n",
    "// CLIENT -> SERVER\n",
    "{\n",
    "  \"jsonrpc\": \"2.0\",\n",
    "  \"method\": \"notifications/initialized\"\n",
    "}\n",
    "```\n",
    "\n",
    "### **The Three Primitives (Resources, Tools and Prompts)**\n",
    "This is the most important conceptual part. MCP divides the world into three specific types of interactions.\n",
    "\n",
    "1. **Resources (Read Only Data)**\n",
    "- **Concept:** Think of this as Data retrieval. It corresponds to a `GET request`.\n",
    "- **Behavior:** The Client (LLM) asks to \"read\" something. It is strictly read-only.\n",
    "- **Use Case:** Reading a file, fetching the current rows of a database, reading a log file.\n",
    "- **LangChain Equivalent:** This maps directly to LangChain's `DocumentLoader`.\n",
    "- **The Raw Call:**\n",
    "```JSON\n",
    "{\n",
    "  \"jsonrpc\": \"2.0\",\n",
    "  \"id\": 2,\n",
    "  \"method\": \"resources/read\",\n",
    "  \"params\": {\n",
    "    \"uri\": \"file:///application/logs/error.txt\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "2. **Tools (Active Capabilities)**\n",
    "- **Concept:** Think of this as Function Calling. It corresponds to a `POST request`.\n",
    "- **Behavior:** The Client asks the Server to perform a calculation or an action.\n",
    "- **Use Case:** \"Calculate sum,\" \"Query API,\" \"Git Commit.\"\n",
    "- **LangChain Equivalent:** This maps directly to LangChain's `@tool` or `BaseTool`.\n",
    "- **The Raw Call:**\n",
    "\n",
    "```JSON\n",
    "{\n",
    "  \"jsonrpc\": \"2.0\",\n",
    "  \"id\": 3,\n",
    "  \"method\": \"tools/call\",\n",
    "  \"params\": {\n",
    "    \"name\": \"get_weather\",\n",
    "    \"arguments\": {\n",
    "      \"city\": \"Hyderabad\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "3. **Prompts (Reusable Templates)**\n",
    "- **Concept:** This is often the most confusing for beginners. These are Server-defined prompt templates.\n",
    "- **Behavior:** The Server stores a prompt structure (like a System Prompt) that the User can select.\n",
    "- **Use Case:**\n",
    "    - A \"Git Server\" might expose a prompt called code_review that automatically pre-fills the prompt with \"Please review the following code changes from the git diff...\".\n",
    "    - This saves the user from typing \"Please act as a code reviewer...\" every time.\n",
    "- **LangChain Equivalent:** This maps directly to LangChain's `PromptTemplate`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d874199-21e4-4adf-a706-47f66cf65c9f",
   "metadata": {},
   "source": [
    "## **Hello World**\n",
    "\n",
    "### **Why FastMCP?**\n",
    "The official Python SDK for MCP is powerful but verbose—it requires setting up async loops, stream handlers, and complex initialization.\n",
    "\n",
    "**FastMCP is a wrapper (inspired by FastAPI)** that handles all the protocol plumbing for you.\n",
    "- **Official SDK:** 50+ lines of code to hello world.\n",
    "- **Using FastMCP:** 5 lines of code.\n",
    "\n",
    "It automatically generates the JSON Schema for your tools based on your Python type hints (str, int, etc.), which the LLM uses to understand how to call your functions.\n",
    "\n",
    "### **Installation**\n",
    "```python\n",
    "! pip install fastmcp\n",
    "```\n",
    "\n",
    "### **Implementing a Server**\n",
    "\n",
    "We will build a simple Weather Server. To keep it simple, we will mock the data so you don't need a real API key yet.\n",
    "\n",
    "Let's now create a file named `weather_server.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793472fb-7f61-4fb5-bb0e-b815bf1e78aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastmcp import FastMCP\n",
    "\n",
    "# Step 1: Initialize the Server\n",
    "mcp = FastMCP(\"My Local Weather Server\")\n",
    "\n",
    "# --- DATA (Mock Database) ---\n",
    "weather_data = {\n",
    "    \"Hyderabad\": \"Sunny, 28°C\",\n",
    "    \"London\": \"Rainy, 12°C\",\n",
    "    \"San Francisco\": \"Foggy, 16°C\"\n",
    "}\n",
    "\n",
    "# --- TOOL: Active Capability (The \"POST\" request) ---\n",
    "@mcp.tool()\n",
    "def get_forecast(city: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the current weather forecast for a specific city.\n",
    "    args:\n",
    "        city: The name of the city (e.g. 'Hyderabad', 'London')\n",
    "    \"\"\"\n",
    "    # Simulate an API call\n",
    "    result = weather_data.get(city, \"Unknown City\")\n",
    "    return f\"The weather in {city} is currently: {result}\"\n",
    "\n",
    "# --- RESOURCE: Passive Context (The \"GET\" request) ---\n",
    "@mcp.resource(\"weather://alerts\")\n",
    "def get_weather_alerts() -> str:\n",
    "    \"\"\"Returns a list of active severe weather alerts.\"\"\"\n",
    "    return \"WARNING: Heatwave approaching Hyderabad. Heavy rain expected in London.\"\n",
    "\n",
    "# 4. Run the server\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
