{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f4d7160-7657-4a66-8c9e-b06dd18c5139",
   "metadata": {},
   "source": [
    "# **Structured Outputs**\n",
    "## **Transform raw LLM responses (like AIMessage object) into structured-usable formats**\n",
    "\n",
    "## **What's Covered?**\n",
    "1. Introduction to Output Parsers\n",
    "    - What is Output Parser?\n",
    "    - Why they are crucial?\n",
    "    - Types of Output Parsers\n",
    "    - What is Pydantic?\n",
    "    - Key methods of LangChain output parser\n",
    "2. CommaSeparatedListOutputParser\n",
    "    - What it does?\n",
    "    - Building an AI System to Auto-Extract Skills from Job Descriptions\n",
    "3. PydanticOutputParser\n",
    "    - What is Pydantic?\n",
    "    - What it does?\n",
    "    - Installation\n",
    "    - Defining a Pydantic Model\n",
    "    - Building an AI Powered Song Recommender using Pydantic Parser\n",
    "    - Step 1: Defining a Pydantic Model\n",
    "    - Step 2: Create PydanticOutputParser\n",
    "    - Step 3: Create Prompt with Format Instructions\n",
    "    - Step 4: Build Chain and Test\n",
    "    - Step 5: Handle Parsing Errors (Production)\n",
    "4. Case Study: Building an AI Powered Text2Movie Metadata Generator\n",
    "    - Step 1: Defining a Pydantic Model\n",
    "    - Step 2: Create PydanticOutputParser\n",
    "    - Step 3: Create Prompt with Format Instructions\n",
    "    - Step 4: Build Chain and Test\n",
    "    - Step 5: Handle Parsing Errors (Production)\n",
    "5. JSONOutputParser with Pydantic\n",
    "    - What it does?\n",
    "    - Building an Intelligent Parser to Translate User Requests into Order Objects\n",
    "    - Step 1: Defining a Pydantic Model\n",
    "    - Step 2: Create JsonOutputParser\n",
    "    - Step 3: Create Prompt with Format Instructions\n",
    "    - Step 4: Build Chain and Test\n",
    "    - Step 5: Handle Parsing Errors (Production)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea223108-6f72-408e-a63d-f9e6dff71446",
   "metadata": {},
   "source": [
    "## **Introduction to Output Parsers**\n",
    "### **What is Output Parser?**\n",
    "For many applications, such as chatbots, models need to respond to users directly in natural language. However, there are scenarios where we need models to output in a structured format. For example, we might want to store the model output in a database and ensure that the output conforms to the database schema. This need motivates the concept of structured output, where models can be instructed to respond with a particular output structure.\n",
    "\n",
    "### **Why they are crucial?**\n",
    "Often we need the output of a LLM in a particular format, for example, you want a python datetime object, or a JSON object. LangChain come with Parse utilities allowing you to easily convert output into precise data types or even your own custom class instance with Pydantic.\n",
    "\n",
    "Output parsers are responsible for taking the output of an LLM and transforming it to a more suitable format. This is very useful when you are using LLMs to generate any form of structured data.\n",
    "\n",
    "### **Types of output parsers**\n",
    "Output Parser Types:\n",
    "- CSL Parser\n",
    "- Pydantic Parser\n",
    "- JSON Parser with Pydantic\n",
    "etc...\n",
    "\n",
    "### **What is Pydantic?**  \n",
    "Pydantic is a library which allows us to define data models, validate the data and type coercion.  \n",
    "Coercion in Pydantic refers to its ability to automatically convert input data into the types specified in the model, as long as the conversion is reasonable. \n",
    "\n",
    "### **Key methods of LangChain output parser**\n",
    "Parser consists of two key elements:\n",
    "- `get_format_instructions()` method:  A method which returns a string containing instructions for how the output of a language model should be formatted.\n",
    "- `parse()` method: A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\n",
    "- (Optional)\"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5567d306-c0bd-4c14-ab46-8710bcf1d132",
   "metadata": {},
   "source": [
    "## **Comma Separated List Parser**\n",
    "\n",
    "### **What it does?**\n",
    "This output parser can be used when you want to return a list of comma-separated items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb66a263-96cc-42d2-9ccb-eecda7950b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanavbansal/Developer/.env_langchain/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "csv_output_parser = CommaSeparatedListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb683e6d-7d19-4bcc-92c1-a7993686d999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As discussed above, lets experiment with get_format_instructions()\n",
    "\n",
    "csv_output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c16ee4e6-2e4b-4d78-82fb-c0c582cb262e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# prompt -> generate a list of modules one must study to become data scientist\n",
    "\n",
    "example_input = \"Python, DA, SQL, ML, DL\"\n",
    "\n",
    "print(type(example_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d19eb38e-6fc3-48c1-bd6e-c33c3790fbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['Python', 'DA', 'SQL', 'ML', 'DL']\n"
     ]
    }
   ],
   "source": [
    "example_input = \"Python, DA, SQL, ML, DL\"\n",
    "\n",
    "# using parse() method\n",
    "parsed_output = csv_output_parser.parse(example_input)\n",
    "\n",
    "print(type(parsed_output))\n",
    "print(parsed_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b08efd7-d316-4765-a53e-8a70cf5f98c7",
   "metadata": {},
   "source": [
    "### **Building an AI System to Auto-Extract Skills from Job Descriptions** \n",
    "\n",
    "**Use Case:** Extract a list of required skills from a job description so you can auto-fill a checklist or tag candidate profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fa2b82d-546f-467d-a336-a5bcf3e972ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['job'], input_types={}, partial_variables={}, template='Extract the key technical skills from the following job description.\\n                Return them as a comma-separated list (no extra text).\\n\\n                Job description:\\n                {job}\\n                ')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: extract skills from a job description\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"Extract the key technical skills from the following job description.\n",
    "                Return them as a comma-separated list (no extra text).\n",
    "                \n",
    "                Job description:\n",
    "                {job}\n",
    "                \"\"\"\n",
    ")\n",
    "\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56417b16-a194-4475-9fee-49a72ad04b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Google ChatModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Setup API Key\n",
    "f = open('keys/.openai_api_key.txt')\n",
    "OPENAI_API_KEY = f.read()\n",
    "\n",
    "# Set the GoogleAI Key and initialize a ChatModel\n",
    "openai_chat_model = ChatOpenAI(api_key=OPENAI_API_KEY, \n",
    "                               model=\"gpt-4o-mini\", \n",
    "                               temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "850c78f2-dee3-4547-bf35-e1407e666d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd41a960-ba88-427b-8a0f-3dee44252214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Description: Snowflake Data Engineer (2+ Years Experience)\n",
      "\n",
      "Role: Snowflake Data Engineer\n",
      "Experience: 2–4 years\n",
      "Location: Hyderabad\n",
      "Employment Type: Full-time\n",
      "\n",
      "About the Role\n",
      "We are looking for a Snowflake Data Engineer with hands-on experience in building data pipelines, developing data models, and working across modern cloud data platforms. The ideal candidate will have strong SQL skills, good understanding of data warehousing concepts, and practical experience implementing solutions on Snowflake.\n",
      "\n",
      "Key Responsibilities\n",
      "Develop, optimize, and maintain ETL/ELT pipelines using Snowflake and related tools.\n",
      "Design and implement Snowflake schemas, views, materialized views, and stored procedures.\n",
      "Manage Snowflake workloads, including Virtual Warehouses, Roles, and Security policies.\n",
      "Work with semi-structured data (JSON, Parquet, Avro) using Snowflake-native functions.\n",
      "Build and manage data ingestion pipelines using tools such as Airflow, DBT, AWS Glue, or Informatica (whatever applies to your company).\n",
      "Implement performance tuning strategies: clustering, micro-partitioning, and query optimization.\n",
      "Collaborate with data analysts, BI engineers, and product teams to deliver scalable datasets.\n",
      "Ensure data quality, governance, and compliance across environments.\n",
      "Work with cloud services like AWS/GCP/Azure for data movement and storage.\n",
      "Participate in code reviews, documentation, and CI/CD processes for data engineering.\n",
      "\n",
      "Required Skills & Qualifications\n",
      "2+ years of hands-on experience with Snowflake (SQL, warehouse management, data ingestion).\n",
      "Strong proficiency in SQL and data modeling (Dimensional modeling, Star/Snowflake schema).\n",
      "Experience with cloud platforms like AWS (preferred), Azure, or GCP.\n",
      "Good understanding of ETL/ELT frameworks and pipeline orchestration.\n",
      "Experience working with Python or Scala for data transformation.\n",
      "Knowledge of data warehousing concepts and best practices.\n",
      "Familiarity with semi-structured data formats (JSON, Parquet).\n",
      "Understanding of CI/CD and Git-based workflows.\n",
      "\n",
      "Good to Have (Optional but Valuable)\n",
      "Experience with DBT for transformations in Snowflake.\n",
      "Exposure to tools like Fivetran / Airbyte / Matillion for data ingestion.\n",
      "Understanding of Snowpipe, Streams & Tasks for automated ingestion.\n",
      "Knowledge of BI tools like Tableau, Power BI, or QuickSight.\n",
      "Basic understanding of ML pipelines and feature preparation.\n",
      "\n",
      "Behavioral Expectations\n",
      "Strong problem-solving and analytical skills.\n",
      "Ability to work in fast-paced, cross-functional teams.\n",
      "Good communication skills and documentation discipline.\n",
      "Ownership mindset with willingness to explore new data tech.\n",
      "\n",
      "What We Offer\n",
      "Opportunity to work on modern data stack (Snowflake, DBT, Airflow, cloud services).\n",
      "Exposure to real-world large-scale analytics projects.\n",
      "Learning-driven environment with mentorship and upskilling support.\n",
      "Flexible work culture and high-growth role.\n"
     ]
    }
   ],
   "source": [
    "jd_file = open(\"data/job_desc.txt\")\n",
    "\n",
    "file_data = jd_file.read()\n",
    "\n",
    "print(file_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5a6a423-ae97-4536-a898-ae9322478d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Snowflake', 'SQL', 'data modeling', 'ETL', 'ELT', 'data ingestion', 'Airflow', 'DBT', 'AWS', 'Azure', 'GCP', 'Python', 'Scala', 'data warehousing', 'semi-structured data', 'CI/CD', 'Git', 'Snowpipe', 'Streams', 'Tasks', 'BI tools', 'Tableau', 'Power BI', 'QuickSight', 'ML pipelines.']\n"
     ]
    }
   ],
   "source": [
    "chain = prompt_template | openai_chat_model | output_parser\n",
    "\n",
    "result = chain.invoke({\"job\": file_data})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d895f2a1-c08a-4582-b9da-393410cd63dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db141db7-78c2-4d7d-801c-869fe9b7c553",
   "metadata": {},
   "source": [
    "## **PydanticOutputParser**\n",
    "\n",
    "### **What is Pydantic?**\n",
    "Pydantic is a **data validation and parsing library** for Python, primarily used with FastAPI, but also great for any application that requires structured data handling.\n",
    "\n",
    "Use Pydantic to declare your data model. Pydantic’s BaseModel is like a Python dataclass, but with actual type checking + coercion. (Think of it as a smart data checker + converter)\n",
    "\n",
    "### **What it does?**\n",
    "This output parser allows users to specify an arbitrary Pydantic Model and query LLMs for outputs that conform to that schema.\n",
    "\n",
    "Pydantic becomes your **guardrail** to catch bad LLM output.\n",
    "\n",
    "You should have some Pydantic knowledge to use it.\n",
    "\n",
    "### **Installation**\n",
    "`pip install pydantic`\n",
    "\n",
    "### **Building an AI Powered Song Recommender using Pydantic Parser**\n",
    "\n",
    "We will follow the following steps to build the Pydantic Parser:\n",
    "- Step 1: Defining a Pydantic Model\n",
    "- Step 2: Create PydanticOutputParser\n",
    "- Step 3: Create Prompt with Format Instructions\n",
    "- Step 4: Build Chain and Test\n",
    "- Step 5: Handle Parsing Errors (Production)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d4b44e-4705-4ff2-ba08-40bcd899eff2",
   "metadata": {},
   "source": [
    "### **Step 1: Defining a Pydantic Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88654381-1109-4e7d-a095-181623aea87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Song(BaseModel):\n",
    "    name: str = Field(description=\"Name of a Song\")\n",
    "    geners: list = Field(description=\"List of Geners\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3516d94-b784-4f1e-ac90-ba7a80b3737c",
   "metadata": {},
   "source": [
    "### **Step 2: Create PydanticOutputParser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8ff0005-7fa5-4c2a-9ed1-701604037f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"name\": {\"description\": \"Name of a Song\", \"title\": \"Name\", \"type\": \"string\"}, \"geners\": {\"description\": \"List of Geners\", \"items\": {}, \"title\": \"Geners\", \"type\": \"array\"}}, \"required\": [\"name\", \"geners\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=Song)\n",
    "\n",
    "print(output_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c50ff60-9db0-48d6-8bd4-37856e41c12b",
   "metadata": {},
   "source": [
    "### **Step 3: Create Prompt with Format Instructions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aeae8a81-db61-498a-8a5a-f50af164b3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['singer_name'], input_types={}, partial_variables={'output_format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"name\": {\"description\": \"Name of a Song\", \"title\": \"Name\", \"type\": \"string\"}, \"geners\": {\"description\": \"List of Geners\", \"items\": {}, \"title\": \"Geners\", \"type\": \"array\"}}, \"required\": [\"name\", \"geners\"]}\\n```'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output_format_instructions'], input_types={}, partial_variables={}, template='You are a helpful AI Song Recommendation Engine.\\n                      You generate output while following the below mentioned format.\\n                      Output Format Instructions:\\n                      {output_format_instructions}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['singer_name'], input_types={}, partial_variables={}, template='What is the most famous song by {singer_name}.'), additional_kwargs={})])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Template\n",
    "chat_template = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        (\"system\", \"\"\"You are a helpful AI Song Recommendation Engine.\n",
    "                      You generate output while following the below mentioned format.\n",
    "                      Output Format Instructions:\n",
    "                      {output_format_instructions}\"\"\"), \n",
    "        (\"human\", \"What is the most famous song by {singer_name}.\")\n",
    "    ],\n",
    "    partial_variables={\"output_format_instructions\": output_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fab1798-80f1-4e25-83eb-afc6497f552b",
   "metadata": {},
   "source": [
    "### **Step 4: Build Chain and Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39483caa-d5b8-4f0e-bed5-d147dd20848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Google ChatModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Setup API Key\n",
    "f = open('keys/.openai_api_key.txt')\n",
    "OPENAI_API_KEY = f.read()\n",
    "\n",
    "# Set the GoogleAI Key and initialize a ChatModel\n",
    "openai_chat_model = ChatOpenAI(api_key=OPENAI_API_KEY, \n",
    "                               model=\"gpt-4o-mini\", \n",
    "                               temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b39be662-5ddf-449b-8c2a-c6269dcc4971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Song(name='Tum Hi Ho', geners=['Bollywood', 'Romantic', 'Pop'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = chat_template | openai_chat_model | output_parser\n",
    "\n",
    "raw_input = {\"singer_name\": \"arijit singh\"}\n",
    "\n",
    "chain.invoke(raw_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fa96ee-8c31-4e78-89dc-3f5f5efc831a",
   "metadata": {},
   "source": [
    "### **Step 5: Handle Parsing Errors (Production)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b196325f-3e35-4f1f-b4a4-1df84c283c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import ValidationError\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "\n",
    "def song_recommendations(text: str) -> Song:\n",
    "    \"\"\"Safely extract movie information with fallback.\"\"\"\n",
    "    try:\n",
    "        return chain.invoke({\"singer_name\": text})\n",
    "    except OutputParserException as e:\n",
    "        print(f\"Output Parser failed:\\n {e}\")\n",
    "    except ValidationError as e:\n",
    "        print(f\"Validation failed:\\n e.json()\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4545ef0-3789-4fb4-bd34-3de7f821a6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Radioactive' geners=['Rock', 'Alternative', 'Indie']\n"
     ]
    }
   ],
   "source": [
    "singer = \"imagine dragons\"\n",
    "\n",
    "song_obj = song_recommendations(singer)\n",
    "\n",
    "print(song_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3944e9ef-c51e-4624-aec2-59e916230e7c",
   "metadata": {},
   "source": [
    "## **Case Study: Building an AI Powered Text2Movie Metadata Generator**\n",
    "\n",
    "**Use Case:** An AI-powered extraction pipeline that parses only what’s explicitly mentioned in the text, delivering reliable structured movie information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bd06a8-5719-4954-8fb6-8cf628b49313",
   "metadata": {},
   "source": [
    "### **Step 1: Defining a Pydantic Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5defcea-ceaa-4bfa-9c4d-14ac292b714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class Actor(BaseModel):\n",
    "    name: str = Field(..., description=\"Actor's full name\")\n",
    "    role: str = Field(..., description=\"Character name\")\n",
    "\n",
    "class Movie(BaseModel):\n",
    "    \"\"\"Movie information extracted from user query.\"\"\"\n",
    "    title: str = Field(..., description=\"Movie title\")\n",
    "    year: int = Field(..., description=\"Release year\", ge=1880, le=2025)\n",
    "    director: str = Field(..., description=\"Director name\")\n",
    "    rating: float = Field(..., description=\"IMDB rating\", ge=0, le=10)\n",
    "    cast: List[Actor] = Field(default_factory=list, description=\"Main cast members\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75ba81f-6757-444d-82bc-f58ef7807962",
   "metadata": {},
   "source": [
    "### **Step 2: Create PydanticOutputParser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0201c668-384f-4817-a23e-a4aba9b6db14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"$defs\": {\"Actor\": {\"properties\": {\"name\": {\"description\": \"Actor's full name\", \"title\": \"Name\", \"type\": \"string\"}, \"role\": {\"description\": \"Character name\", \"title\": \"Role\", \"type\": \"string\"}}, \"required\": [\"name\", \"role\"], \"title\": \"Actor\", \"type\": \"object\"}}, \"description\": \"Movie information extracted from user query.\", \"properties\": {\"title\": {\"description\": \"Movie title\", \"title\": \"Title\", \"type\": \"string\"}, \"year\": {\"description\": \"Release year\", \"maximum\": 2025, \"minimum\": 1880, \"title\": \"Year\", \"type\": \"integer\"}, \"director\": {\"description\": \"Director name\", \"title\": \"Director\", \"type\": \"string\"}, \"rating\": {\"description\": \"IMDB rating\", \"maximum\": 10, \"minimum\": 0, \"title\": \"Rating\", \"type\": \"number\"}, \"cast\": {\"description\": \"Main cast members\", \"items\": {\"$ref\": \"#/$defs/Actor\"}, \"title\": \"Cast\", \"type\": \"array\"}}, \"required\": [\"title\", \"year\", \"director\", \"rating\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=Movie)\n",
    "\n",
    "print(output_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d205c856-101d-4c6b-8d8d-cc7db17f2614",
   "metadata": {},
   "source": [
    "### **Step 3: Create Prompt with Format Instructions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab5e5fb4-a941-4b7e-b2c5-545105f889ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"You extract structured movie information from text.\n",
    "                      You are a strict parser. Extract only what is explicitly present in the text. Never invent facts.\n",
    "                      {format_instructions}\"\"\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Partial to inject format instructions\n",
    "chat_prompt_partial = chat_prompt.partial(\n",
    "    format_instructions=output_parser.get_format_instructions()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1555130-b1ca-4313-8d49-341bc1afde6f",
   "metadata": {},
   "source": [
    "### **Step 4: Build Chain and Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "801dee5b-0304-491a-a71d-359b1731c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Google ChatModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Setup API Key\n",
    "f = open('keys/.openai_api_key.txt')\n",
    "OPENAI_API_KEY = f.read()\n",
    "\n",
    "# Set the GoogleAI Key and initialize a ChatModel\n",
    "openai_chat_model = ChatOpenAI(api_key=OPENAI_API_KEY, \n",
    "                               model=\"gpt-4o-mini\", \n",
    "                               temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4cb1c62-8a7e-48c8-85ec-af4792f7628a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='Inception' year=2010 director='Christopher Nolan' rating=8.8 cast=[Actor(name='Leonardo DiCaprio', role='Lead'), Actor(name='Joseph Gordon-Levitt', role='Arthur'), Actor(name='Ellen Page', role='Ariadne')]\n"
     ]
    }
   ],
   "source": [
    "chain = chat_prompt_partial | openai_chat_model | output_parser\n",
    "\n",
    "# Test with movie description\n",
    "result = chain.invoke({\n",
    "    \"input\": \"\"\"Inception is a 2010 sci-fi film directed by Christopher Nolan. \n",
    "    It stars Leonardo DiCaprio and has an IMDB rating of 8.8. Main cast includes \n",
    "    Joseph Gordon-Levitt as Arthur and Ellen Page as Ariadne.\"\"\"\n",
    "})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f68a5cde-3b14-47ab-aa61-7221d0d05090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception\n",
      "8.8\n"
     ]
    }
   ],
   "source": [
    "print(result.title)\n",
    "print(result.rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955ebcc1-9880-49e7-a920-8ef0a9b91ac8",
   "metadata": {},
   "source": [
    "### **Step 5: Handle Parsing Errors (Production)**\n",
    "\n",
    "**Important Note: If the Pydantic constructor succeeds, the object is guaranteed to have all required fields.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "837c41e0-97cf-48e9-9a34-edd99b2d1480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import ValidationError\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "\n",
    "def extract_movie_info(text: str) -> Movie:\n",
    "    \"\"\"Safely extract movie information with fallback.\"\"\"\n",
    "    try:\n",
    "        return chain.invoke({\"input\": text})\n",
    "    except OutputParserException as e:\n",
    "        print(f\"Output Parser failed:\\n {e}\")\n",
    "    except ValidationError as e:\n",
    "        print(f\"Validation failed:\\n e.json()\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d40da9cd-0b29-4ef6-b54f-1dbbc6e54fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='Inception' year=2010 director='Christopher Nolan' rating=8.8 cast=[Actor(name='Leonardo DiCaprio', role='N/A'), Actor(name='Joseph Gordon-Levitt', role='Arthur'), Actor(name='Ellen Page', role='Ariadne')]\n"
     ]
    }
   ],
   "source": [
    "movie_info = \"\"\"Inception is a 2010 sci-fi film directed by Christopher Nolan. \n",
    "It stars Leonardo DiCaprio and has an IMDB rating of 8.8. Main cast includes \n",
    "Joseph Gordon-Levitt as Arthur and Ellen Page as Ariadne.\"\"\"\n",
    "\n",
    "movie_obj = extract_movie_info(movie_info)\n",
    "\n",
    "print(movie_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29cbc284-6d97-4398-b137-2dd52fa805d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Parser failed:\n",
      " Failed to parse Movie from completion {\"title\": \"Inception\", \"year\": null, \"director\": null, \"rating\": null, \"cast\": []}. Got: 3 validation errors for Movie\n",
      "year\n",
      "  Input should be a valid integer [type=int_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/int_type\n",
      "director\n",
      "  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/string_type\n",
      "rating\n",
      "  Input should be a valid number [type=float_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/float_type\n",
      "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "movie_info = \"\"\"Inception is a great movie\"\"\"\n",
    "\n",
    "movie_obj = extract_movie_info(movie_info)\n",
    "\n",
    "print(movie_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8643f88c-0d30-43a6-8867-e4cc1d155aa0",
   "metadata": {},
   "source": [
    "## **JSONOutputParser with Pydantic**\n",
    "\n",
    "### **What it does?**\n",
    "If you want the parser to return JSON instead of a Pydantic object, you can use a Pydantic schema together with the **JsonOutputParser** to generate structured JSON output.\n",
    "\n",
    "Let's learn how to do it using the following case study.\n",
    "\n",
    "### **Building an Intelligent Parser to Translate User Requests into Order Objects**\n",
    "**Use case:** Parse complex structured user requests into typed Python objects (e.g., Order object used by downstream business logic).\n",
    "\n",
    "Steps involved:\n",
    "- Step 1: Defining a Pydantic Model\n",
    "- Step 2: Create JsonOutputParser\n",
    "- Step 3: Create Prompt with Format Instructions\n",
    "- Step 4: Build Chain and Test\n",
    "- Step 5: Handle Parsing Errors (Production)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f20476b-4d94-4da8-96a4-ad48ce5a672e",
   "metadata": {},
   "source": [
    "### **Step 1: Defining a Pydantic Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88f8c5a6-7448-43d5-95f5-de52beeaf24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define the pydantic model for expected output\n",
    "class OrderItem(BaseModel):\n",
    "    sku: str = Field(description=\"Unique product identifier (Stock Keeping Unit) for the item.\")\n",
    "    quantity: int = Field(description=\"Number of units ordered for this item. Must be greater than zero.\", gt=0)\n",
    "    price: float = Field(description=\"Price of a single unit of the item.\")\n",
    "\n",
    "class Order(BaseModel):\n",
    "    order_id: str = Field(description=\"Unique identifier for the order.\")\n",
    "    customer_email: str = Field(description=\"Email address of the customer who placed the order.\")\n",
    "    items: list[OrderItem] = Field(description=\"List of all items included in the order.\")\n",
    "    total: float = Field(description=\"Total order amount after summing all item costs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c24232-0421-4c47-a901-fed80fae2be7",
   "metadata": {},
   "source": [
    "### **Step 2: Create JsonOutputParser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7772988-1ed3-49e7-8f92-0bc909504048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRICT OUTPUT FORMAT:\n",
      "- Return only the JSON value that conforms to the schema. Do not include any additional text, explanations, headings, or separators.\n",
      "- Do not wrap the JSON in Markdown or code fences (no ``` or ```json).\n",
      "- Do not prepend or append any text (e.g., do not write \"Here is the JSON:\").\n",
      "- The response must be a single top-level JSON value exactly as required by the schema (object/array/etc.), with no trailing commas or comments.\n",
      "\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]} the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema (shown in a code block for readability only — do not include any backticks or Markdown in your output):\n",
      "```\n",
      "{\"$defs\": {\"OrderItem\": {\"properties\": {\"sku\": {\"description\": \"Unique product identifier (Stock Keeping Unit) for the item.\", \"title\": \"Sku\", \"type\": \"string\"}, \"quantity\": {\"description\": \"Number of units ordered for this item. Must be greater than zero.\", \"exclusiveMinimum\": 0, \"title\": \"Quantity\", \"type\": \"integer\"}, \"price\": {\"description\": \"Price of a single unit of the item.\", \"title\": \"Price\", \"type\": \"number\"}}, \"required\": [\"sku\", \"quantity\", \"price\"], \"title\": \"OrderItem\", \"type\": \"object\"}}, \"properties\": {\"order_id\": {\"description\": \"Unique identifier for the order.\", \"title\": \"Order Id\", \"type\": \"string\"}, \"customer_email\": {\"description\": \"Email address of the customer who placed the order.\", \"title\": \"Customer Email\", \"type\": \"string\"}, \"items\": {\"description\": \"List of all items included in the order.\", \"items\": {\"$ref\": \"#/$defs/OrderItem\"}, \"title\": \"Items\", \"type\": \"array\"}, \"total\": {\"description\": \"Total order amount after summing all item costs.\", \"title\": \"Total\", \"type\": \"number\"}}, \"required\": [\"order_id\", \"customer_email\", \"items\", \"total\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# Create parser from pydantic model\n",
    "output_parser = JsonOutputParser(pydantic_object=Order)\n",
    "\n",
    "print(output_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a41079-3381-45b4-b484-23f1380a7eca",
   "metadata": {},
   "source": [
    "### **Step 3: Create Prompt with Format Instructions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6d5bfc5-d8e4-4b9d-82f4-cac05c17163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"User provided this order information in text. \n",
    "    Extract into a JSON format that matches the output format instructions provided below.\n",
    "    Return only a JSON format.\n",
    "    Text:\n",
    "    {text}\n",
    "    \n",
    "    Output Format Instructions:\n",
    "    {output_format_instructions}\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"],\n",
    "    partial_variables={\"output_format_instructions\": output_parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b20903-e292-4376-84ab-0f1ea407fafe",
   "metadata": {},
   "source": [
    "### **Step 4: Build Chain and Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a0f0316-6156-4af2-a04e-b78e0288fc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Google ChatModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Setup API Key\n",
    "f = open('keys/.openai_api_key.txt')\n",
    "OPENAI_API_KEY = f.read()\n",
    "\n",
    "# Set the GoogleAI Key and initialize a ChatModel\n",
    "openai_chat_model = ChatOpenAI(api_key=OPENAI_API_KEY, \n",
    "                               model=\"gpt-4o-mini\", \n",
    "                               temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea02b0f6-1146-4ab7-a83c-580ddabe6e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uh yeah, for order number ORD 1001, the email is jane at example dot com.\n",
      "Need two of item ABC one twenty three, price is nineteen ninety nine each,\n",
      "and one of the X Y Z nine nine nine for hundred twenty nine point five.\n",
      "total should be one sixty nine forty eight.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pd_file = open(\"data/prod_desc_asr_output.txt\")\n",
    "\n",
    "file_data = pd_file.read()\n",
    "\n",
    "print(file_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cde91e80-ed93-4b03-84f2-b1ba93b5f2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'order_id': 'ORD 1001', 'customer_email': 'jane@example.com', 'items': [{'sku': 'ABC123', 'quantity': 2, 'price': 19.99}, {'sku': 'XYZ999', 'quantity': 1, 'price': 129.5}], 'total': 169.48}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt_template | openai_chat_model | output_parser\n",
    "\n",
    "result = chain.invoke({\"text\": file_data})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c381e1e9-055e-4296-846e-5fc7613f654a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
