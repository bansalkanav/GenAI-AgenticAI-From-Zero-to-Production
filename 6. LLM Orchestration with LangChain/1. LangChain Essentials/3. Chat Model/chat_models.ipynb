{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca9e88c5-c154-4834-8175-6862ebc2227b",
   "metadata": {},
   "source": [
    "# **Chat Models**\n",
    "\n",
    "## **What's Covered?**\n",
    "1. Introduction to Chat Models\n",
    "    - What are Chat Models?\n",
    "    - Capabilities\n",
    "    - Integrations\n",
    "2. Building Chat Model - GoogleAI, Groq and OpenAI\n",
    "    - Installing the libraries\n",
    "    - Setting up the API Key\n",
    "    - Instantiating the Chat Model and Standard Parameters\n",
    "    - Get Model's Profile\n",
    "    - Key Methods\n",
    "    - invoke() Method\n",
    "    - stream() Method\n",
    "    - batch(list_of_message_lists) Method\n",
    "3. HuggingFace Chat Models\n",
    "    - What is HuggingFace?\n",
    "    - Why use HuggingFace Chat Models with LangChain?\n",
    "    - Installation\n",
    "    - Hugging Face Local Pipelines\n",
    "    - Inference Providers with HuggingFaceEndpoint and ChatHuggingFace\n",
    "4. LangChain New Unified Chat Model API\n",
    "    - Basic usage\n",
    "    - Initialize a model\n",
    "    - Parameters\n",
    "    - Example Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753337d-8fa1-4590-81a3-bedc01202905",
   "metadata": {},
   "source": [
    "## **Introduction to Chat Models**\n",
    "\n",
    "LangChain provides a unified interface for interacting with various chat models from different providers (OpenAI, Google, Anthropic, Cohere, etc.).\n",
    "\n",
    "LangChain chat models are named with a convention that prefixes \"Chat\" to their class names (e.g., ChatOllama, ChatAnthropic, ChatOpenAI, etc.).\n",
    "\n",
    "\n",
    "### **What are Chat Models?**\n",
    "At their core, chat models are a specialized type of Large Language Model (LLM) designed and fine-tuned to engage in conversational interactions. Unlike older \"text completion\" models (which simply predict the next word given a string of text), chat models understand and operate on a concept of \"messages\" with associated \"roles.\"\n",
    "\n",
    "Modern LLMs are typically accessed through a chat model interface that takes a list of messages as input and returns an AI message as output. Chat Models are customized for conversational usage. **[Click Here](https://python.langchain.com/docs/integrations/chat/)** to check the complete list of LLMs which can be used with LangChain.\n",
    "\n",
    "**Chat Models Input: A list of BaseMessage objects (typically SystemMessage, HumanMessage, AIMessage)**\n",
    "```\n",
    "[SystemMessage(content=\"You are a helpful assistant.\"), HumanMessage(content=\"What is the capital of France?\")]\n",
    "```\n",
    "**Chat Models Output: A single AIMessage object**\n",
    "```\n",
    "AIMessage(content=\"The capital of France is Paris.\")\n",
    "```\n",
    "\n",
    "### **Capabilities**\n",
    "\n",
    "The newest generation of chat models offer additional capabilities:\n",
    "1. [Tool calling](https://python.langchain.com/docs/concepts/tool_calling/): Many popular chat models offer a native tool calling API. This API allows developers to build rich applications that enable LLMs to interact with external services, APIs, and databases. Tool calling can also be used to extract structured information from unstructured data and perform various other tasks.\n",
    "2. [Structured output](https://python.langchain.com/docs/concepts/structured_outputs/): A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\n",
    "3. [Multimodality](https://python.langchain.com/docs/concepts/multimodality/): The ability to work with data other than text; for example, images, audio, and video.\n",
    "\n",
    "### **Integrations**\n",
    "LangChain has many chat model integrations that allow you to use a wide variety of models from different providers. These integrations are one of two types:\n",
    "\n",
    "1. **Official models:** These are models that are officially supported by LangChain and/or model provider. You can find these models in the **`langchain-<provider>`** packages.\n",
    "2. **Community models:** There are models that are mostly contributed and supported by the community. You can find these models in the **`langchain-community`** package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc81099d-8426-4f8e-8e9d-0cccd4915da3",
   "metadata": {},
   "source": [
    "## **Building Chat Model - GoogleAI, Groq and OpenAI**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b45c64-704c-45cd-b193-ed403ebbae43",
   "metadata": {},
   "source": [
    "### **Installing the libraries**\n",
    "\n",
    "```python\n",
    "! pip install --upgrade --quiet langchain-google-genai\n",
    "! pip install --upgrade --quiet langchain-openai\n",
    "! pip install --upgrade --quiet langchain-groq\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1698394c-12ae-4f5a-bf2f-f731f10a61dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade --quiet langchain-google-genai\n",
    "# ! pip install --upgrade --quiet langchain-openai\n",
    "# ! pip install --upgrade --quiet langchain-groq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e337e240-46e5-4667-a9dd-dd969d5771b2",
   "metadata": {},
   "source": [
    "### **Setting up the API Key**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e380313c-365e-4a48-b96b-9828f6ae9f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API Key\n",
    "\n",
    "f = open('keys/.gemini.txt')\n",
    "\n",
    "GOOGLE_API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4ddbf00-1071-45a7-88d4-0885150798f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API Key\n",
    "\n",
    "f = open('keys/.openai_api_key.txt')\n",
    "\n",
    "OPENAI_API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "887160d9-ef43-4642-b425-254cd8aaa999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API Key\n",
    "\n",
    "f = open('keys/.groq_api_key.txt')\n",
    "\n",
    "GROQ_API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952edee7-87d3-4c74-a6e9-f57859fe38fa",
   "metadata": {},
   "source": [
    "### **Instantiating the Chat Model and Standard Parameters**\n",
    "\n",
    "Standard parameters are currently only enforced on integrations that have their own integration packages (e.g. langchain-openai, langchain-anthropic, etc.), they're not enforced on models in langchain-community.\n",
    "\n",
    "| Parameter      | Description |\n",
    "|--------------|-------------|\n",
    "| model        | The name or identifier of the specific AI model you want to use (e.g., \"gpt-3.5-turbo\" or \"gpt-4\"). |\n",
    "| temperature  | Controls the randomness of the model's output. A higher value (e.g., 1.0) makes responses more creative, while a lower value (e.g., 0.0) makes them more deterministic and focused. |\n",
    "| timeout      | The maximum time (in seconds) to wait for a response from the model before canceling the request. Ensures the request doesnâ€™t hang indefinitely. |\n",
    "| max_tokens   | Limits the total number of tokens (words and punctuation) in the response. This controls how long the output can be. |\n",
    "| stop         | Specifies stop sequences that indicate when the model should stop generating tokens. For example, you might use specific strings to signal the end of a response. |\n",
    "| max_retries  | The maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits. |\n",
    "| api_key      | The API key required for authenticating with the model provider. This is usually issued when you sign up for access to the model. |\n",
    "| base_url     | The URL of the API endpoint where requests are sent. This is typically provided by the model's provider and is necessary for directing your requests. |\n",
    "| rate_limiter | An optional BaseRateLimiter to space out requests to avoid exceeding rate limits. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "079087d1-4adc-437a-bd8e-899e8a255294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanavbansal/Developer/.env_langchain/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import ChatModel\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Pass the standard parameters during initialization\n",
    "google_chat_model = ChatGoogleGenerativeAI(api_key=GOOGLE_API_KEY, \n",
    "                                           model=\"gemini-2.5-flash\", \n",
    "                                           temperature=1)\n",
    "\n",
    "openai_chat_model = ChatOpenAI(api_key=OPENAI_API_KEY, \n",
    "                               model=\"gpt-4o-mini\", \n",
    "                               temperature=1)\n",
    "\n",
    "gorq_chat_model = ChatGroq(api_key=GROQ_API_KEY, \n",
    "                           model=\"openai/gpt-oss-20b\", \n",
    "                           temperature=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57146266-0f84-4929-8240-32821b2daf0b",
   "metadata": {},
   "source": [
    "### **Get Model's Profile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7f60f65-ee08-4ab6-a9a0-cdd59348795e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_input_tokens': 1048576,\n",
       " 'max_output_tokens': 65536,\n",
       " 'image_inputs': True,\n",
       " 'audio_inputs': True,\n",
       " 'pdf_inputs': True,\n",
       " 'video_inputs': True,\n",
       " 'image_outputs': False,\n",
       " 'audio_outputs': False,\n",
       " 'video_outputs': False,\n",
       " 'reasoning_output': True,\n",
       " 'tool_calling': True,\n",
       " 'structured_output': True,\n",
       " 'image_url_inputs': True,\n",
       " 'image_tool_message': True,\n",
       " 'tool_choice': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_chat_model.profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b6b943a-fc2e-4fa5-afd1-56adefe45e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_input_tokens': 128000,\n",
       " 'max_output_tokens': 16384,\n",
       " 'image_inputs': True,\n",
       " 'audio_inputs': False,\n",
       " 'video_inputs': False,\n",
       " 'image_outputs': False,\n",
       " 'audio_outputs': False,\n",
       " 'video_outputs': False,\n",
       " 'reasoning_output': False,\n",
       " 'tool_calling': True,\n",
       " 'structured_output': True,\n",
       " 'image_url_inputs': True,\n",
       " 'pdf_inputs': True,\n",
       " 'pdf_tool_message': True,\n",
       " 'image_tool_message': True,\n",
       " 'tool_choice': True}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_chat_model.profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4d35032-e617-443e-a638-329edc18239d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_input_tokens': 131072,\n",
       " 'max_output_tokens': 32768,\n",
       " 'image_inputs': False,\n",
       " 'audio_inputs': False,\n",
       " 'video_inputs': False,\n",
       " 'image_outputs': False,\n",
       " 'audio_outputs': False,\n",
       " 'video_outputs': False,\n",
       " 'reasoning_output': True,\n",
       " 'tool_calling': True}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gorq_chat_model.profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64bc476-f0f8-415c-a506-f90cbe30c15d",
   "metadata": {},
   "source": [
    "### **Key Methods**\n",
    "The key methods of a chat model are:\n",
    "\n",
    "1. **invoke:** The primary method for interacting with a chat model. It takes a list of messages as input and returns a list of messages as output.\n",
    "2. **stream:** A method that allows you to stream the output of a chat model as it is generated, token by token. This is crucial for building responsive user interfaces.\n",
    "3. **batch:** A method that allows you to batch multiple requests to a chat model together for more efficient processing.\n",
    "4. **bind_tools:** A method that allows you to bind a tool to a chat model for use in the model's execution context.\n",
    "5. **with_structured_output:** A wrapper around the invoke method for models that natively support structured output. We will see this in the **Output Parsing** module. Observe that Groq's GPT Model is not having \"structured_output\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7e2c13-246e-46b9-84d3-3edd31bcdbae",
   "metadata": {},
   "source": [
    "### **invoke() Method**\n",
    "\n",
    "The primary method to send a list of messages to the model and get a single response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba51f646-ef6e-4f0d-9ce5-bf6c9936c195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a polite assistant.\"),\n",
    "    HumanMessage(content=\"Hello!\"),\n",
    "]\n",
    "\n",
    "response = openai_chat_model.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7617c07a-9e81-46f2-a280-ddb4221f017e",
   "metadata": {},
   "source": [
    "### **stream() Method** \n",
    "\n",
    "Allows you to receive the model's response incrementally, token by token. This is crucial for building responsive user interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f85f2188-fa50-441d-a186-a7d8c66e4d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Streaming Response ---\n",
      "Hello! How can I assist you today?\n",
      "--- End Streaming ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Streaming Response ---\")\n",
    "\n",
    "for chunk in openai_chat_model.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "    \n",
    "print(\"\\n--- End Streaming ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9e8f97-1f46-4627-9649-d21de0124b78",
   "metadata": {},
   "source": [
    "### **batch(list_of_message_lists) Method** \n",
    "\n",
    "For sending multiple sets of messages in a single API call (if the provider supports it), which can be more efficient.\n",
    "- **Batch = parallel inference**\n",
    "- Used if you want speed, cheaper cost, or higher throughput  \n",
    "\n",
    "\n",
    "Batching has huge applications during:\n",
    "1. Embedding Large Corpora\n",
    "2. Multi-Chunk Summarization (eg: processing a book)\n",
    "3. Batch Evaluation (eg: evaluating the answer sheets)\n",
    "4. Dataset labeling with LLMs\n",
    "5. Chatbots with parallel multi-user requests (eg: You can reduce inference cost by grouping user messages into batches dynamically)\n",
    "\n",
    "**Important Note**  \n",
    "Even if a model can handle 1000 parallel calls, your app may not:\n",
    "- OS has max open sockets\n",
    "- your backend has max thread pool size\n",
    "- network bandwidth becomes a bottleneck\n",
    "\n",
    "**Final Notes**  \n",
    "- Batch processing â‰  Infinite parallel requests\n",
    "- Batch processing = Efficient organization of requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "243e47c1-d055-4e99-93d4-caecf9012272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 + 1 equals 2.\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "batch_messages = [\n",
    "    [HumanMessage(content=\"What is 1+1?\")],\n",
    "    [HumanMessage(content=\"What is the capital of India?\")],\n",
    "]\n",
    "responses = openai_chat_model.batch(batch_messages)\n",
    "\n",
    "for res in responses:\n",
    "    print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be28542c-433a-4347-90b3-d0f435cb82d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 + 1 equals 2.\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "for response in openai_chat_model.batch_as_completed(batch_messages):\n",
    "    print(response[1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2dee74-1dac-47de-9f6d-6472435e4b5a",
   "metadata": {},
   "source": [
    "## **HuggingFace Chat Models**\n",
    "\n",
    "HuggingFace is an incredibly popular platform and community that has democratized access to state-of-the-art machine learning models, especially in Natural Language Processing (NLP). When it comes to chat models, HuggingFace hosts a vast array of models, many of which can be used for conversational AI.\n",
    "\n",
    "### **What is HuggingFace?**\n",
    "\n",
    "HuggingFace is a giant online library and community for machine learning models. \n",
    "\n",
    "They provide:\n",
    "1. **Transformers Library:** A powerful Python library that makes it easy to download, train, and use pre-trained NLP models (including chat models).\n",
    "2. **HuggingFace Hub:** A platform where anyone can share and discover models, datasets, and demos. It's like GitHub, but for ML models.\n",
    "3. **Tools & Ecosystem:** A rich set of tools for fine-tuning, deploying, and evaluating models.\n",
    "\n",
    "Many LLMs, including those capable of chat, are available on the HuggingFace Hub. These can range from smaller, open-source models that you can run locally to larger models that might require more significant computational resources.\n",
    "\n",
    "### **Why use HuggingFace Chat Models with LangChain?**\n",
    "\n",
    "While cloud-based LLMs like OpenAI's GPT models or Google's Gemini are powerful, HuggingFace offers distinct advantages:\n",
    "\n",
    "1. **Open Source & Flexibility:** Many models on HuggingFace are open-source, giving you more control, transparency, and the ability to fine-tune them for very specific tasks.\n",
    "2. **Cost-Effectiveness (Potentially):** If you can run models locally or on your own infrastructure, you can potentially reduce API costs associated with commercial LLM providers.\n",
    "3. **Privacy/Security:** For sensitive data, running models locally or on your private cloud can offer better privacy and security controls.\n",
    "4. **Experimentation:** A vast playground for trying out different model architectures and sizes.\n",
    "5. **Community Support:** A very active and helpful community.\n",
    "\n",
    "LangChain acts as a crucial bridge here. It provides a consistent interface (`ChatHuggingFace` class) that allows you to easily integrate models from the HuggingFace ecosystem into your LangChain applications, abstracting away much of the underlying complexity of the transformers library.\n",
    "\n",
    "### **Installation**\n",
    "```python\n",
    "! pip install --upgrade --quiet langchain-huggingface transformers huggingface_hub\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f53c92e-c423-478d-8a4c-befa1dd93c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade --quiet langchain-huggingface transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876a1814-31c9-4941-ade9-038d4de80129",
   "metadata": {},
   "source": [
    "### **Hugging Face Local Pipelines**\n",
    "\n",
    "Hugging Face models can be run locally through the **HuggingFacePipeline** class.\n",
    "\n",
    "When we use the HuggingFacePipeline, it downloads the complete model from the HFHub into our local system and infer it locally.\n",
    "\n",
    "There are two ways in which you can use the HuggingFacePipeline:\n",
    "- **Way 1:** Models can be loaded by specifying the model parameters using the `from_model_id()` method.\n",
    "- **Way 2:** Models can also be loaded by passing in an existing transformers `pipeline()` directly.\n",
    "\n",
    "**Note: HuggingFacePipeline only supports text-generation, text2text-generation, image-text-to-text, summarization and translation for now. (As per 26th June 2025)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "812c8f53-4045-42c4-9c82-4089e57ced62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become a data scientist?\\n\\nWhat should I study to become'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Way 1\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"gpt2\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 512},\n",
    ")\n",
    "\n",
    "hf.invoke(\"What should I study to become a data scientist?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4446ce20-8a38-4afc-a36c-b888887a803d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"What should I study to become a data scientist? My research needs to demonstrate that the application of data science in data analytics is as important as the application of data science in the analysis of data.\\n\\nWhat should I do if I cannot get a job?\\n\\nAs a data scientist, you should take a series of deep cognitive tests to assess your knowledge of the data and how you can use them to better understand your data.\\n\\nIf you have a technical background or are interested in the data science of analytics, a career in data science is a great way to get a start.\\n\\nIt is also a great place to start if you are interested in working with data science.\\n\\nDo I have to get a PhD to study the data science of analytics?\\n\\nYes. However, you must take an introductory data science course before you can apply, as there are many places available in the data science of analytics.\\n\\nCan I get a job as an analyst with analytics?\\n\\nYes.\\n\\nCan I use my own analytics tools for data analysis?\\n\\nYes.\\n\\nCan I use their tools to improve my data science skills?\\n\\nYes.\\n\\nAre there any other jobs that need the same basic skills as me?\\n\\nYes.\\n\\nAre there any other jobs where you can help someone with data science skills?\\n\\nYes.\\n\\nWhat if I don't have a job?\\n\\nIf you have a job that requires you to take a deep cognitive test, you may be able to apply for one.\\n\\nIf you are an analyst, you can apply for a job in the data science of analytics.\\n\\nWe may be able to give you a job in a data science of analytics job site.\\n\\nQuestions, comments, or suggestions?\\n\\nPlease post your questions in the comments section below.\\n\\nWhat is the best way to get a job in data science?\\n\\nIf you find it difficult to get a job in data science, consider applying to a data science of analytics job site or hiring a data scientist.\\n\\nWe are here to help you find a job in data science of analytics.\\n\\nIf you have any questions, inquiries, or suggestions, please feel free to contact us or leave a comment below.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Way 2\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "hf = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "hf.invoke(\"What should I study to become a data scientist?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f6db42-9d4f-43be-ad70-088ca8197e35",
   "metadata": {},
   "source": [
    "### **Inference Providers with HuggingFaceEndpoint and ChatHuggingFace**\n",
    "Hugging Faceâ€™s Inference Providers give developers streamlined, unified access to hundreds of machine learning models, powered by our serverless inference partners.\n",
    "[Click here](https://huggingface.co/settings/inference-providers) to get the list of all the inference providers.\n",
    "\n",
    "We can use the HuggingFaceEndpoint class to run open source models via serverless Inference Providers or via dedicated Inference Endpoints.\n",
    "\n",
    "This works with any model that supports text generation (i.e. text completion) task. \n",
    "\n",
    "To use this class, do the following first:\n",
    "- Set the environment variable `HUGGINGFACEHUB_API_TOKEN` with your API token, or given as a named parameter to the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c657b81-1a48-4f9a-b679-e6730e1a46dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "# Setup API Key\n",
    "f = open('keys/.hf_api_key.txt')\n",
    "HF_TOKEN = f.read()\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"openai/gpt-oss-20b\",\n",
    "    huggingfacehub_api_token=HF_TOKEN,\n",
    "    provider=\"auto\",\n",
    "    task=\"text-generation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1964b52c-64ac-4429-bb87-5e8733fa4840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## ðŸš€ Roadmap to a Dataâ€‘Science Career\n",
      "\n",
      "> **Goal:** Build a solid, practical skill set that lets you turn raw data into actionable insights and machineâ€‘learning models, and communicate those results to nonâ€‘technical stakeholders.\n",
      "\n",
      "Below is a *stepâ€‘byâ€‘step* learning path that blends theory, handsâ€‘on practice, and realâ€‘world projects. Feel free to skip or accelerate sections that you already know; the key is to finish each block with a tangible deliverable (e.g., a notebook, a Kaggle submission, a dashboard\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace\n",
    "\n",
    "chat_llm = ChatHuggingFace(\n",
    "    llm=llm,\n",
    "    max_length=128,\n",
    "    temperature=0.5,\n",
    "    task=\"conversational\"\n",
    ")\n",
    "\n",
    "response = chat_llm.invoke(\"What should I study to become a data scientist?\")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325bd17c-fae2-4b16-98c7-55c2ec3fc950",
   "metadata": {},
   "source": [
    "## **LangChain New Unified Chat Model API**\n",
    "\n",
    "LangChainâ€™s standard model interfaces give you access to many different provider integrations, which makes it easy to experiment with and switch between models to find the best fit for your use case.\n",
    "\n",
    "### **Basic usage**\n",
    "Models can be utilized in two ways:\n",
    "1. Standalone - Models can be called directly for tasks like text generation, classification, or extraction without the need for an agent framework.\n",
    "2. With agents - Models can be dynamically specified when creating an agent.\n",
    "\n",
    "### **Initialize a model**\n",
    "The easiest way to get started with a standalone model in LangChain is to use init_chat_model to initialize one from a chat model provider of your choice.\n",
    "\n",
    "**Syntax:**\n",
    "```python\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-4o-mini\")\n",
    "```\n",
    "\n",
    "### **Parameters**\n",
    "\n",
    "A chat model takes parameters that can be used to configure its behavior. The full set of supported parameters varies by model and provider, but standard ones include:\n",
    "1. **model** - The name or identifier of the specific model you want to use with a provider. You can also specify both the model and its provider in a single argument using the '{model_provider}:{model}' format, for example, â€˜openai:o1â€™. Will attempt to infer model_provider from model if not specified. The following providers will be inferred based on these model prefixes:\n",
    "    - gpt-... | o1... | o3... -> openai\n",
    "    - claude... -> anthropic\n",
    "    - amazon... -> bedrock\n",
    "    - gemini... -> google_vertexai\n",
    "    - command... -> cohere\n",
    "    - accounts/fireworks... -> fireworks\n",
    "    - mistral... -> mistralai\n",
    "    - deepseek... -> deepseek\n",
    "    - grok... -> xai\n",
    "    - sonar... -> perplexity\n",
    "2. **model_provider** - The model provider if not specified as part of the model arg (see above). Supported model_provider values and the corresponding integration package are:\n",
    "    - openai -> langchain-openai\n",
    "    - anthropic -> langchain-anthropic\n",
    "    - azure_openai -> langchain-openai\n",
    "    - azure_ai -> langchain-azure-ai\n",
    "    - google_vertexai -> langchain-google-vertexai\n",
    "    - google_genai -> langchain-google-genai\n",
    "    - bedrock -> langchain-aws\n",
    "    - bedrock_converse -> langchain-aws\n",
    "    - cohere -> langchain-cohere\n",
    "    - fireworks -> langchain-fireworks\n",
    "    - together -> langchain-together\n",
    "    - mistralai -> langchain-mistralai\n",
    "    - huggingface -> langchain-huggingface\n",
    "    - groq -> langchain-groq\n",
    "    - ollama -> langchain-ollama\n",
    "    - google_anthropic_vertex -> langchain-google-vertexai\n",
    "    - deepseek -> langchain-deepseek\n",
    "    - ibm -> langchain-ibm\n",
    "    - nvidia -> langchain-nvidia-ai-endpoints\n",
    "    - xai -> langchain-xai\n",
    "    - perplexity -> langchain-perplexity\n",
    "\n",
    "3. **api_key** - The key required for authenticating with the modelâ€™s provider. This is usually issued when you sign up for access to the model.\n",
    "4.  ****kwargs** - Additional model-specific keyword args to pass to the underlying chat model's __init__ method. Common parameters include:\n",
    "    - temperature: Model temperature for controlling randomness.\n",
    "    - max_tokens: Maximum number of output tokens.\n",
    "    - timeout: Maximum time (in seconds) to wait for a response.\n",
    "    - max_retries: Maximum number of retry attempts for failed requests.\n",
    "    - base_url: Custom API endpoint URL.\n",
    "    - rate_limiter: A BaseRateLimiter instance to control request rate.\n",
    "\n",
    "**Syntax:**\n",
    "```python\n",
    "model = init_chat_model(\n",
    "    \"claude-sonnet-4-5-20250929\",\n",
    "    # Kwargs passed to the model:\n",
    "    temperature=0.7,\n",
    "    timeout=30,\n",
    "    max_tokens=1000,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d64e167-2cee-40e9-88c7-ea6962610ed4",
   "metadata": {},
   "source": [
    "### **Example Usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62494e9a-eb3b-4ac4-bf1b-06a4a229b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Setting up the API key\n",
    "f = open('keys/.openai_api_key.txt')\n",
    "OPENAI_API_KEY = f.read()\n",
    "\n",
    "model = init_chat_model(model=\"openai:gpt-4o-mini\", \n",
    "                        api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2ecc43e-b821-4a85-b533-6b3f6178cd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a polite assistant.\"),\n",
    "    HumanMessage(content=\"Hello!\"),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
