{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66f5314b-f295-4177-afe0-20d8ac40d9bb",
   "metadata": {},
   "source": [
    "# **ðŸ¤— Transformer Pipeline Inference**\n",
    "\n",
    "## **What's Covered?**\n",
    "1. Pipelines\n",
    "    - What is pipeline()?\n",
    "    - Behind the Scenes\n",
    "    - Key Points\n",
    "2. Pipeline Syntax \n",
    "    - Example Usage\n",
    "    - Garbage Collection\n",
    "3. Default Model List\n",
    "    - Common Tasks Supported\n",
    "    - Identify the Pipeline Supported Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1d26bf-3574-4f2d-a7c5-e30ab428d217",
   "metadata": {},
   "source": [
    "## **Pipelines**\n",
    "\n",
    "### **What is pipeline()?**\n",
    "The `pipeline()` makes it simple to use any model from the `Hub` for inference on any language, computer vision, speech, and multimodal tasks. Even if you donâ€™t have experience with a specific modality or arenâ€™t familiar with the underlying code behind the models, you can still use them for inference with the `pipeline()`! \n",
    "\n",
    "It is the most powerful way to start using pre-trained Hugging Face models. \n",
    "\n",
    "It's a high level API that abstracts away all the complexity of tokenization, model loading, and post-processing, allowing you to perform common tasks with just a few lines of code.\n",
    "\n",
    "### **Behind the Scenes**\n",
    "- Determine framework (py/tf/jax)\n",
    "- Loads tokenizer\n",
    "- Loads model\n",
    "- Choose Device (MPS/CUDA/CPU)\n",
    "- Handles pre/post-processing\n",
    "- Gives results\n",
    "\n",
    "### **Key Points**\n",
    "- The first time you run a pipeline for a specific model, it will download the model weights (which can be several hundred MB to GBs).\n",
    "- Subsequent runs will use the cached version.\n",
    "- You can specify a particular model within the pipeline if you don't want the default.\n",
    "- The output format of the pipeline varies depending on the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ada8379-8486-4df3-a5ec-b19d9d070689",
   "metadata": {},
   "source": [
    "## **Pipeline Syntax**\n",
    "1. Start by importing `pipeline` and `torch`.\n",
    "```python\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "```\n",
    "2. Specify the inference task, model and torch_dtype.\n",
    "    - `torch_dtype` tells the HuggingFace pipeline in which numeric precision the model weights and computations should be loaded.\n",
    "    - `torch_dtype=torch.bfloat16` means Load the model parameters in `bfloat16` precision instead of the default (usually float32).\n",
    "    - This is done for Lower Memory Usage and Faster Inference.\n",
    "```python\n",
    "classifier = pipeline(\n",
    "    task=\"text-classification\", \n",
    "    model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "```\n",
    "3. Pass the input to the `pipeline()`\n",
    "```python\n",
    "classifier(input_text)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962c4306-7a44-4534-836a-601fd7be00bf",
   "metadata": {},
   "source": [
    "### **Example Usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ff2e61a-613e-4a22-8fed-5b23ac6a79ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanavbansal/Developer/.env_jupyter/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9997997879981995}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import pipeline\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Specify the inference task\n",
    "classifier = pipeline(\n",
    "    task=\"text-classification\", \n",
    "    model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "classifier(\"It was a very bad movie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe869da2-c0de-4819-b982-b185bb3c7e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf4441c-168e-47fd-95e2-1159f60fd1d2",
   "metadata": {},
   "source": [
    "**Important**\n",
    "Transformers needs to decide:\n",
    "- Should I load a PyTorch model? (pt)\n",
    "- Should I load a TensorFlow model? (tf)\n",
    "- Which AutoModel class is correct?\n",
    "- If the user didnâ€™t specify a model, which model should I default to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1d7992-d9bb-41bf-a082-56d78454bb9d",
   "metadata": {},
   "source": [
    "### **Garbage Collection**\n",
    "\n",
    "```python\n",
    "del classifier\n",
    "```\n",
    "- This does not delete the object from memory directly.\n",
    "- It only removes the name `classifier` from the current namespace.\n",
    "- `classifier` is just a variable name. That name was pointing to some object. `del` translator removes that reference.\n",
    "\n",
    "```python\n",
    "import gc\n",
    "gc.collect()\n",
    "# Ouput: 10\n",
    "```\n",
    "- This explicitly asks Pythonâ€™s garbage collector to find unreachable objects and free them.\n",
    "- Output represents the number of unreachable objects which were found and collected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2a62d0e-e915-48b6-b2bf-2a40cf72b3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del classifier\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15e5a41-50be-4381-af41-51ffe9649ead",
   "metadata": {},
   "source": [
    "## **Default Model List**\n",
    "\n",
    "### **Common Tasks Supported**\n",
    "- text-classification\n",
    "- text-generation\n",
    "- ner\n",
    "- summarization\n",
    "- translation\n",
    "- question-answering\n",
    "- fill-mask (predicting missing words)\n",
    "- zero-shot-classification (classifying text without specific training examples)\n",
    "- ... and many more!\n",
    "\n",
    "Explore more on:  \n",
    "https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "\n",
    "### **Identify the Pipeline Supported Tasks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6418eec2-87ed-4a38-89e6-16fccc1da480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['audio-classification', 'automatic-speech-recognition', 'text-to-audio', 'feature-extraction', 'text-classification', 'token-classification', 'question-answering', 'table-question-answering', 'visual-question-answering', 'document-question-answering', 'fill-mask', 'summarization', 'translation', 'text2text-generation', 'text-generation', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-audio-classification', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-to-text', 'image-text-to-text', 'object-detection', 'zero-shot-object-detection', 'depth-estimation', 'video-classification', 'mask-generation', 'image-to-image'])\n"
     ]
    }
   ],
   "source": [
    "from transformers.pipelines import SUPPORTED_TASKS\n",
    "print(SUPPORTED_TASKS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7047e106-b6ee-465d-9bbd-2c325eebc0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'impl': transformers.pipelines.audio_classification.AudioClassificationPipeline,\n",
       " 'tf': (),\n",
       " 'pt': (transformers.models.auto.modeling_auto.AutoModelForAudioClassification,),\n",
       " 'default': {'model': {'pt': ('superb/wav2vec2-base-superb-ks', '372e048')}},\n",
       " 'type': 'audio'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUPPORTED_TASKS[\"audio-classification\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dec7624-89dd-49f6-ae3f-49d15d265c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'impl': transformers.pipelines.text_classification.TextClassificationPipeline,\n",
       " 'tf': (transformers.models.auto.modeling_tf_auto.TFAutoModelForSequenceClassification,),\n",
       " 'pt': (transformers.models.auto.modeling_auto.AutoModelForSequenceClassification,),\n",
       " 'default': {'model': {'pt': ('distilbert/distilbert-base-uncased-finetuned-sst-2-english',\n",
       "    '714eb0f'),\n",
       "   'tf': ('distilbert/distilbert-base-uncased-finetuned-sst-2-english',\n",
       "    '714eb0f')}},\n",
       " 'type': 'text'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUPPORTED_TASKS[\"text-classification\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c2c151a-d27d-4770-b891-8ae611be7f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>type</th>\n",
       "      <th>impl</th>\n",
       "      <th>pt_models</th>\n",
       "      <th>tf_models</th>\n",
       "      <th>default_pt_model</th>\n",
       "      <th>default_tf_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>audio-classification</td>\n",
       "      <td>audio</td>\n",
       "      <td>AudioClassificationPipeline</td>\n",
       "      <td>[AutoModelForAudioClassification]</td>\n",
       "      <td>[]</td>\n",
       "      <td>superb/wav2vec2-base-superb-ks</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>automatic-speech-recognition</td>\n",
       "      <td>multimodal</td>\n",
       "      <td>AutomaticSpeechRecognitionPipeline</td>\n",
       "      <td>[AutoModelForCTC, AutoModelForSpeechSeq2Seq]</td>\n",
       "      <td>[]</td>\n",
       "      <td>facebook/wav2vec2-base-960h</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text-to-audio</td>\n",
       "      <td>text</td>\n",
       "      <td>TextToAudioPipeline</td>\n",
       "      <td>[AutoModelForTextToWaveform, AutoModelForTextT...</td>\n",
       "      <td>[]</td>\n",
       "      <td>suno/bark-small</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feature-extraction</td>\n",
       "      <td>multimodal</td>\n",
       "      <td>FeatureExtractionPipeline</td>\n",
       "      <td>[AutoModel]</td>\n",
       "      <td>[TFAutoModel]</td>\n",
       "      <td>distilbert/distilbert-base-cased</td>\n",
       "      <td>distilbert/distilbert-base-cased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text-classification</td>\n",
       "      <td>text</td>\n",
       "      <td>TextClassificationPipeline</td>\n",
       "      <td>[AutoModelForSequenceClassification]</td>\n",
       "      <td>[TFAutoModelForSequenceClassification]</td>\n",
       "      <td>distilbert/distilbert-base-uncased-finetuned-s...</td>\n",
       "      <td>distilbert/distilbert-base-uncased-finetuned-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>text</td>\n",
       "      <td>TokenClassificationPipeline</td>\n",
       "      <td>[AutoModelForTokenClassification]</td>\n",
       "      <td>[TFAutoModelForTokenClassification]</td>\n",
       "      <td>dbmdz/bert-large-cased-finetuned-conll03-english</td>\n",
       "      <td>dbmdz/bert-large-cased-finetuned-conll03-english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>question-answering</td>\n",
       "      <td>text</td>\n",
       "      <td>QuestionAnsweringPipeline</td>\n",
       "      <td>[AutoModelForQuestionAnswering]</td>\n",
       "      <td>[TFAutoModelForQuestionAnswering]</td>\n",
       "      <td>distilbert/distilbert-base-cased-distilled-squad</td>\n",
       "      <td>distilbert/distilbert-base-cased-distilled-squad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>table-question-answering</td>\n",
       "      <td>text</td>\n",
       "      <td>TableQuestionAnsweringPipeline</td>\n",
       "      <td>[AutoModelForTableQuestionAnswering]</td>\n",
       "      <td>[TFAutoModelForTableQuestionAnswering]</td>\n",
       "      <td>google/tapas-base-finetuned-wtq</td>\n",
       "      <td>google/tapas-base-finetuned-wtq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>visual-question-answering</td>\n",
       "      <td>multimodal</td>\n",
       "      <td>VisualQuestionAnsweringPipeline</td>\n",
       "      <td>[AutoModelForVisualQuestionAnswering]</td>\n",
       "      <td>[]</td>\n",
       "      <td>dandelin/vilt-b32-finetuned-vqa</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>document-question-answering</td>\n",
       "      <td>multimodal</td>\n",
       "      <td>DocumentQuestionAnsweringPipeline</td>\n",
       "      <td>[AutoModelForDocumentQuestionAnswering]</td>\n",
       "      <td>[]</td>\n",
       "      <td>impira/layoutlm-document-qa</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fill-mask</td>\n",
       "      <td>text</td>\n",
       "      <td>FillMaskPipeline</td>\n",
       "      <td>[AutoModelForMaskedLM]</td>\n",
       "      <td>[TFAutoModelForMaskedLM]</td>\n",
       "      <td>distilbert/distilroberta-base</td>\n",
       "      <td>distilbert/distilroberta-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>summarization</td>\n",
       "      <td>text</td>\n",
       "      <td>SummarizationPipeline</td>\n",
       "      <td>[AutoModelForSeq2SeqLM]</td>\n",
       "      <td>[TFAutoModelForSeq2SeqLM]</td>\n",
       "      <td>sshleifer/distilbart-cnn-12-6</td>\n",
       "      <td>google-t5/t5-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>translation</td>\n",
       "      <td>text</td>\n",
       "      <td>TranslationPipeline</td>\n",
       "      <td>[AutoModelForSeq2SeqLM]</td>\n",
       "      <td>[TFAutoModelForSeq2SeqLM]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>text2text-generation</td>\n",
       "      <td>text</td>\n",
       "      <td>Text2TextGenerationPipeline</td>\n",
       "      <td>[AutoModelForSeq2SeqLM]</td>\n",
       "      <td>[TFAutoModelForSeq2SeqLM]</td>\n",
       "      <td>google-t5/t5-base</td>\n",
       "      <td>google-t5/t5-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>text-generation</td>\n",
       "      <td>text</td>\n",
       "      <td>TextGenerationPipeline</td>\n",
       "      <td>[AutoModelForCausalLM]</td>\n",
       "      <td>[TFAutoModelForCausalLM]</td>\n",
       "      <td>openai-community/gpt2</td>\n",
       "      <td>openai-community/gpt2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>zero-shot-classification</td>\n",
       "      <td>text</td>\n",
       "      <td>ZeroShotClassificationPipeline</td>\n",
       "      <td>[AutoModelForSequenceClassification]</td>\n",
       "      <td>[TFAutoModelForSequenceClassification]</td>\n",
       "      <td>facebook/bart-large-mnli</td>\n",
       "      <td>FacebookAI/roberta-large-mnli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>zero-shot-image-classification</td>\n",
       "      <td>multimodal</td>\n",
       "      <td>ZeroShotImageClassificationPipeline</td>\n",
       "      <td>[AutoModelForZeroShotImageClassification]</td>\n",
       "      <td>[TFAutoModelForZeroShotImageClassification]</td>\n",
       "      <td>openai/clip-vit-base-patch32</td>\n",
       "      <td>openai/clip-vit-base-patch32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>zero-shot-audio-classification</td>\n",
       "      <td>multimodal</td>\n",
       "      <td>ZeroShotAudioClassificationPipeline</td>\n",
       "      <td>[AutoModel]</td>\n",
       "      <td>[]</td>\n",
       "      <td>laion/clap-htsat-fused</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>image-classification</td>\n",
       "      <td>image</td>\n",
       "      <td>ImageClassificationPipeline</td>\n",
       "      <td>[AutoModelForImageClassification]</td>\n",
       "      <td>[TFAutoModelForImageClassification]</td>\n",
       "      <td>google/vit-base-patch16-224</td>\n",
       "      <td>google/vit-base-patch16-224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>image-feature-extraction</td>\n",
       "      <td>image</td>\n",
       "      <td>ImageFeatureExtractionPipeline</td>\n",
       "      <td>[AutoModel]</td>\n",
       "      <td>[TFAutoModel]</td>\n",
       "      <td>google/vit-base-patch16-224</td>\n",
       "      <td>google/vit-base-patch16-224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>image-segmentation</td>\n",
       "      <td>multimodal</td>\n",
       "      <td>ImageSegmentationPipeline</td>\n",
       "      <td>[AutoModelForImageSegmentation, AutoModelForSe...</td>\n",
       "      <td>[]</td>\n",
       "      <td>facebook/detr-resnet-50-panoptic</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>image-to-text</td>\n",
       "      <td>multimodal</td>\n",
       "      <td>ImageToTextPipeline</td>\n",
       "      <td>[AutoModelForVision2Seq]</td>\n",
       "      <td>[TFAutoModelForVision2Seq]</td>\n",
       "      <td>ydshieh/vit-gpt2-coco-en</td>\n",
       "      <td>ydshieh/vit-gpt2-coco-en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>image-text-to-text</td>\n",
       "      <td>multimodal</td>\n",
       "      <td>ImageTextToTextPipeline</td>\n",
       "      <td>[AutoModelForImageTextToText]</td>\n",
       "      <td>[]</td>\n",
       "      <td>llava-hf/llava-onevision-qwen2-0.5b-ov-hf</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>object-detection</td>\n",
       "      <td>multimodal</td>\n",
       "      <td>ObjectDetectionPipeline</td>\n",
       "      <td>[AutoModelForObjectDetection]</td>\n",
       "      <td>[]</td>\n",
       "      <td>facebook/detr-resnet-50</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>zero-shot-object-detection</td>\n",
       "      <td>multimodal</td>\n",
       "      <td>ZeroShotObjectDetectionPipeline</td>\n",
       "      <td>[AutoModelForZeroShotObjectDetection]</td>\n",
       "      <td>[]</td>\n",
       "      <td>google/owlvit-base-patch32</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>depth-estimation</td>\n",
       "      <td>image</td>\n",
       "      <td>DepthEstimationPipeline</td>\n",
       "      <td>[AutoModelForDepthEstimation]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Intel/dpt-large</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>video-classification</td>\n",
       "      <td>video</td>\n",
       "      <td>VideoClassificationPipeline</td>\n",
       "      <td>[AutoModelForVideoClassification]</td>\n",
       "      <td>[]</td>\n",
       "      <td>MCG-NJU/videomae-base-finetuned-kinetics</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>mask-generation</td>\n",
       "      <td>multimodal</td>\n",
       "      <td>MaskGenerationPipeline</td>\n",
       "      <td>[AutoModelForMaskGeneration]</td>\n",
       "      <td>[]</td>\n",
       "      <td>facebook/sam-vit-huge</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>image-to-image</td>\n",
       "      <td>image</td>\n",
       "      <td>ImageToImagePipeline</td>\n",
       "      <td>[AutoModelForImageToImage]</td>\n",
       "      <td>[]</td>\n",
       "      <td>caidas/swin2SR-classical-sr-x2-64</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              task        type  \\\n",
       "0             audio-classification       audio   \n",
       "1     automatic-speech-recognition  multimodal   \n",
       "2                    text-to-audio        text   \n",
       "3               feature-extraction  multimodal   \n",
       "4              text-classification        text   \n",
       "5             token-classification        text   \n",
       "6               question-answering        text   \n",
       "7         table-question-answering        text   \n",
       "8        visual-question-answering  multimodal   \n",
       "9      document-question-answering  multimodal   \n",
       "10                       fill-mask        text   \n",
       "11                   summarization        text   \n",
       "12                     translation        text   \n",
       "13            text2text-generation        text   \n",
       "14                 text-generation        text   \n",
       "15        zero-shot-classification        text   \n",
       "16  zero-shot-image-classification  multimodal   \n",
       "17  zero-shot-audio-classification  multimodal   \n",
       "18            image-classification       image   \n",
       "19        image-feature-extraction       image   \n",
       "20              image-segmentation  multimodal   \n",
       "21                   image-to-text  multimodal   \n",
       "22              image-text-to-text  multimodal   \n",
       "23                object-detection  multimodal   \n",
       "24      zero-shot-object-detection  multimodal   \n",
       "25                depth-estimation       image   \n",
       "26            video-classification       video   \n",
       "27                 mask-generation  multimodal   \n",
       "28                  image-to-image       image   \n",
       "\n",
       "                                   impl  \\\n",
       "0           AudioClassificationPipeline   \n",
       "1    AutomaticSpeechRecognitionPipeline   \n",
       "2                   TextToAudioPipeline   \n",
       "3             FeatureExtractionPipeline   \n",
       "4            TextClassificationPipeline   \n",
       "5           TokenClassificationPipeline   \n",
       "6             QuestionAnsweringPipeline   \n",
       "7        TableQuestionAnsweringPipeline   \n",
       "8       VisualQuestionAnsweringPipeline   \n",
       "9     DocumentQuestionAnsweringPipeline   \n",
       "10                     FillMaskPipeline   \n",
       "11                SummarizationPipeline   \n",
       "12                  TranslationPipeline   \n",
       "13          Text2TextGenerationPipeline   \n",
       "14               TextGenerationPipeline   \n",
       "15       ZeroShotClassificationPipeline   \n",
       "16  ZeroShotImageClassificationPipeline   \n",
       "17  ZeroShotAudioClassificationPipeline   \n",
       "18          ImageClassificationPipeline   \n",
       "19       ImageFeatureExtractionPipeline   \n",
       "20            ImageSegmentationPipeline   \n",
       "21                  ImageToTextPipeline   \n",
       "22              ImageTextToTextPipeline   \n",
       "23              ObjectDetectionPipeline   \n",
       "24      ZeroShotObjectDetectionPipeline   \n",
       "25              DepthEstimationPipeline   \n",
       "26          VideoClassificationPipeline   \n",
       "27               MaskGenerationPipeline   \n",
       "28                 ImageToImagePipeline   \n",
       "\n",
       "                                            pt_models  \\\n",
       "0                   [AutoModelForAudioClassification]   \n",
       "1        [AutoModelForCTC, AutoModelForSpeechSeq2Seq]   \n",
       "2   [AutoModelForTextToWaveform, AutoModelForTextT...   \n",
       "3                                         [AutoModel]   \n",
       "4                [AutoModelForSequenceClassification]   \n",
       "5                   [AutoModelForTokenClassification]   \n",
       "6                     [AutoModelForQuestionAnswering]   \n",
       "7                [AutoModelForTableQuestionAnswering]   \n",
       "8               [AutoModelForVisualQuestionAnswering]   \n",
       "9             [AutoModelForDocumentQuestionAnswering]   \n",
       "10                             [AutoModelForMaskedLM]   \n",
       "11                            [AutoModelForSeq2SeqLM]   \n",
       "12                            [AutoModelForSeq2SeqLM]   \n",
       "13                            [AutoModelForSeq2SeqLM]   \n",
       "14                             [AutoModelForCausalLM]   \n",
       "15               [AutoModelForSequenceClassification]   \n",
       "16          [AutoModelForZeroShotImageClassification]   \n",
       "17                                        [AutoModel]   \n",
       "18                  [AutoModelForImageClassification]   \n",
       "19                                        [AutoModel]   \n",
       "20  [AutoModelForImageSegmentation, AutoModelForSe...   \n",
       "21                           [AutoModelForVision2Seq]   \n",
       "22                      [AutoModelForImageTextToText]   \n",
       "23                      [AutoModelForObjectDetection]   \n",
       "24              [AutoModelForZeroShotObjectDetection]   \n",
       "25                      [AutoModelForDepthEstimation]   \n",
       "26                  [AutoModelForVideoClassification]   \n",
       "27                       [AutoModelForMaskGeneration]   \n",
       "28                         [AutoModelForImageToImage]   \n",
       "\n",
       "                                      tf_models  \\\n",
       "0                                            []   \n",
       "1                                            []   \n",
       "2                                            []   \n",
       "3                                 [TFAutoModel]   \n",
       "4        [TFAutoModelForSequenceClassification]   \n",
       "5           [TFAutoModelForTokenClassification]   \n",
       "6             [TFAutoModelForQuestionAnswering]   \n",
       "7        [TFAutoModelForTableQuestionAnswering]   \n",
       "8                                            []   \n",
       "9                                            []   \n",
       "10                     [TFAutoModelForMaskedLM]   \n",
       "11                    [TFAutoModelForSeq2SeqLM]   \n",
       "12                    [TFAutoModelForSeq2SeqLM]   \n",
       "13                    [TFAutoModelForSeq2SeqLM]   \n",
       "14                     [TFAutoModelForCausalLM]   \n",
       "15       [TFAutoModelForSequenceClassification]   \n",
       "16  [TFAutoModelForZeroShotImageClassification]   \n",
       "17                                           []   \n",
       "18          [TFAutoModelForImageClassification]   \n",
       "19                                [TFAutoModel]   \n",
       "20                                           []   \n",
       "21                   [TFAutoModelForVision2Seq]   \n",
       "22                                           []   \n",
       "23                                           []   \n",
       "24                                           []   \n",
       "25                                           []   \n",
       "26                                           []   \n",
       "27                                           []   \n",
       "28                                           []   \n",
       "\n",
       "                                     default_pt_model  \\\n",
       "0                      superb/wav2vec2-base-superb-ks   \n",
       "1                         facebook/wav2vec2-base-960h   \n",
       "2                                     suno/bark-small   \n",
       "3                    distilbert/distilbert-base-cased   \n",
       "4   distilbert/distilbert-base-uncased-finetuned-s...   \n",
       "5    dbmdz/bert-large-cased-finetuned-conll03-english   \n",
       "6    distilbert/distilbert-base-cased-distilled-squad   \n",
       "7                     google/tapas-base-finetuned-wtq   \n",
       "8                     dandelin/vilt-b32-finetuned-vqa   \n",
       "9                         impira/layoutlm-document-qa   \n",
       "10                      distilbert/distilroberta-base   \n",
       "11                      sshleifer/distilbart-cnn-12-6   \n",
       "12                                               None   \n",
       "13                                  google-t5/t5-base   \n",
       "14                              openai-community/gpt2   \n",
       "15                           facebook/bart-large-mnli   \n",
       "16                       openai/clip-vit-base-patch32   \n",
       "17                             laion/clap-htsat-fused   \n",
       "18                        google/vit-base-patch16-224   \n",
       "19                        google/vit-base-patch16-224   \n",
       "20                   facebook/detr-resnet-50-panoptic   \n",
       "21                           ydshieh/vit-gpt2-coco-en   \n",
       "22          llava-hf/llava-onevision-qwen2-0.5b-ov-hf   \n",
       "23                            facebook/detr-resnet-50   \n",
       "24                         google/owlvit-base-patch32   \n",
       "25                                    Intel/dpt-large   \n",
       "26           MCG-NJU/videomae-base-finetuned-kinetics   \n",
       "27                              facebook/sam-vit-huge   \n",
       "28                  caidas/swin2SR-classical-sr-x2-64   \n",
       "\n",
       "                                     default_tf_model  \n",
       "0                                                None  \n",
       "1                                                None  \n",
       "2                                                None  \n",
       "3                    distilbert/distilbert-base-cased  \n",
       "4   distilbert/distilbert-base-uncased-finetuned-s...  \n",
       "5    dbmdz/bert-large-cased-finetuned-conll03-english  \n",
       "6    distilbert/distilbert-base-cased-distilled-squad  \n",
       "7                     google/tapas-base-finetuned-wtq  \n",
       "8                                                None  \n",
       "9                                                None  \n",
       "10                      distilbert/distilroberta-base  \n",
       "11                                 google-t5/t5-small  \n",
       "12                                               None  \n",
       "13                                  google-t5/t5-base  \n",
       "14                              openai-community/gpt2  \n",
       "15                      FacebookAI/roberta-large-mnli  \n",
       "16                       openai/clip-vit-base-patch32  \n",
       "17                                               None  \n",
       "18                        google/vit-base-patch16-224  \n",
       "19                        google/vit-base-patch16-224  \n",
       "20                                               None  \n",
       "21                           ydshieh/vit-gpt2-coco-en  \n",
       "22                                               None  \n",
       "23                                               None  \n",
       "24                                               None  \n",
       "25                                               None  \n",
       "26                                               None  \n",
       "27                                               None  \n",
       "28                                               None  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Your dictionary (assign it to a variable)\n",
    "data = SUPPORTED_TASKS   # Replace this with your dict variable name\n",
    "\n",
    "rows = []\n",
    "\n",
    "for task, info in data.items():\n",
    "    impl = info.get(\"impl\", None)\n",
    "\n",
    "    # Extract PyTorch model classes\n",
    "    pt_models = [m.__name__ for m in info.get(\"pt\", [])]\n",
    "\n",
    "    # Extract TensorFlow model classes\n",
    "    tf_models = [m.__name__ for m in info.get(\"tf\", [])]\n",
    "\n",
    "    # Extract default model names\n",
    "    default_entry = info.get(\"default\", {})\n",
    "    default_model_dict = default_entry.get(\"model\", {})\n",
    "\n",
    "    default_pt = None\n",
    "    default_tf = None\n",
    "\n",
    "    if \"pt\" in default_model_dict:\n",
    "        default_pt = default_model_dict[\"pt\"][0]   # (model_name, revision)\n",
    "    if \"tf\" in default_model_dict:\n",
    "        default_tf = default_model_dict[\"tf\"][0]\n",
    "\n",
    "    rows.append({\n",
    "        \"task\": task,\n",
    "        \"type\": info.get(\"type\"),\n",
    "        \"impl\": impl.__name__ if impl else None,\n",
    "        \"pt_models\": pt_models,\n",
    "        \"tf_models\": tf_models,\n",
    "        \"default_pt_model\": default_pt,\n",
    "        \"default_tf_model\": default_tf,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
