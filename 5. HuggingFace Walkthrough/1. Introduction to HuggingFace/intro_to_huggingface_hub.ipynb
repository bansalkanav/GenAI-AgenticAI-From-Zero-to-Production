{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cff9939-7af6-4f28-ba45-ce618cbb1c5e",
   "metadata": {},
   "source": [
    "# **Introduction to Huggingface ü§ó Hub** \n",
    "\n",
    "## **What's Covered?**\n",
    "1. Introduction to HuggingFace\n",
    "    - What is HuggingFace?\n",
    "    - HuggingFace Hub\n",
    "    - Key libraries\n",
    "    - Open Source and Framework Agnostic\n",
    "2. Model Card\n",
    "    - What is a Model Card?\n",
    "    - Model Tags\n",
    "    - Model Details\n",
    "    - Model Files\n",
    "    - Inference Provider\n",
    "3. ü§ó transformers Library\n",
    "    - What is ü§ó transformers?\n",
    "    - Installation\n",
    "    - Verifying Installation\n",
    "    - Checking available GPU's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2329fbb8-6182-44ab-888c-8f8341d28a58",
   "metadata": {},
   "source": [
    "## **Introduction to HuggingFace**\n",
    "\n",
    "<img src=\"images/huggingface_hub.png\">\n",
    "\n",
    "\n",
    "### **What is HuggingFace?**\n",
    "\n",
    "HuggingFace is a company and a community of opensource ML projects, most famous for NLP.  \n",
    "\n",
    "HuggingFace makes advanced AI, especially with large language models, accessible and practical for everyone. It provides the building blocks (pre-trained models, datasets) and the tools (libraries, platform) to quickly build, experiment with, and deploy AI applications without reinventing the wheel.\n",
    "\n",
    "### **HuggingFace Hub**\n",
    "**HuggingFace Hub** is a web-based platform which hosts the following:\n",
    "1. **Models Hub:** Thousands of pre-trained models (BERT, GPT, T5, etc.) for tasks like sentiment analysis, translation, summarization, and more. Go to: [https://huggingface.co/models](https://huggingface.co/models) and click on any model. Pick a task you're interested in (like \"text-generation\"), find a popular model, and read its model card.  When you click on the model name, you'll find:\n",
    "    - Model card (what a model does)\n",
    "    - Usage code\n",
    "    - License\n",
    "    - Downloads\n",
    "    - Metrics\n",
    "    - Files (like pytorch_model.bin, config.json, tokenizer.json)\n",
    "2. **Datasets Hub:** Access and share a vast collection of datasets. High-quality data is the fuel for AI models, and Hugging Face offers a convenient way to find and utilize ready-to-use datasets for various tasks.\n",
    "3. **Spaces:** Create and host interactive demos of your machine learning models or applications directly in your browser. This allows for easy sharing and showcasing of your work.\n",
    "4. **Other Tools:** It also provides various other tools and services for things like automatic model training (AutoTrain) and **Inference APIs** for easy deployment.\n",
    "5. **Hub for Vision, Audio, and Multimodal Models:** Not limited to text anymore.\n",
    "\n",
    "\n",
    "### **Key libraries**\n",
    "1. `ü§ó Transformers:` This is the flagship library. It provides a unified API for working with state-of-the-art \"Transformer\" models (the \"T\" in GPT, BERT, etc.). These models are incredibly powerful for understanding and generating human language. With `transformers`, you can load, use, and fine-tune these models with just a few lines of code, regardless of whether you're using PyTorch or TensorFlow.\n",
    "2. `ü§ó Datasets:` This library simplifies the process of loading, processing, and sharing datasets for machine learning. It's highly efficient for handling large datasets.\n",
    "3. `ü§ó Evaluate:`\n",
    "4. `ü§ó Tokenizers:` Before a language model can understand text, it needs to break it down into smaller pieces called \"tokens.\" The tokenizers library provides highly optimized tokenizers for various models, ensuring your data is in the right format for the models to consume.\n",
    "5. `ü§ó Accelerate:` A library that helps you train your models on different hardware setups (e.g., multiple GPUs, distributed training) with minimal code changes.\n",
    "\n",
    "### **Open Source and Framework Agnostic**\n",
    "- By providing pre-trained models and easy-to-use APIs, it dramatically speeds up the development cycle.\n",
    "- The open-source nature fosters a collaborative environment, leading to faster advancements and more robust solutions.\n",
    "- It supports popular deep learning frameworks like PyTorch and TensorFlow, giving developers flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b78d328-95c5-403d-8183-3272f9c0625a",
   "metadata": {},
   "source": [
    "## **HF Model Card**\n",
    "<img src=\"images/model_card.png\">\n",
    "\n",
    "### **What is a Model Card?**\n",
    "Every model on Hugging Face has a Model Card ‚Äî a documentation page that explains:\n",
    "1. What the model is?\n",
    "2. What task(s) it is designed for?\n",
    "3. How to use it?\n",
    "There are major 4 sections as mentioned below.\n",
    "\n",
    "### **Model Tags**\n",
    "<img src=\"images/model_card_2.png\">\n",
    "\n",
    "These tags tell you:\n",
    "1. Task(s) supported - Eg: Text Generation, Summarization, Automatic Speech Recognition, etc...\n",
    "2. Library supported - Eg: Transformers, Diffusion, etc...\n",
    "3. Framework(s) supported - Eg: PyTorch, Tensorflow or JAX\n",
    "4. Training Data (if known)\n",
    "5. Research paper reference (if any)\n",
    "6. Safetensor (if available)\n",
    "7. Licensing - You cannot use some models commercially.\n",
    "\n",
    "**Note:** Safetensors is a model file format created by HuggingFace. It replaces traditional .bin (PyTorch) or .h5(Tensorflow) or .cpkt(Checkpoint files). Safetensors is a safer, faster, and more memory-efficient way to store model weights.\n",
    "\n",
    "### **Model Details**\n",
    "1. Model Description - Helps to know how the model is trained, training objective, architecture and whether it's good for fine-tuning (small) or inference only (large)\n",
    "2. Intended Use / Limitations - Helps to understand the tasks model can solve. Many beginners pick a model trained for classification and try using it for generation.\n",
    "3. Training Data - If the dataset is multilingual ‚Üí model will work well on many languages\n",
    "4. Evaluation Metrics - Helps understand how good the model is\n",
    "5. Model Usage Examples - Helps with example code snippets. If the model requires special inputs (like image size, audio samples, segmentation masks), this is where they tell you.\n",
    "\n",
    "### **Model Files**\n",
    "1. Model is stored in either `.bin` or `.h5` or `.safetensors`.\n",
    "2. Missing tokenizer.json ‚Üí tokenizer may not work\n",
    "3. Missing config ‚Üí pipeline might fail\n",
    "4. Only .h5 ‚Üí model is TF-only\n",
    "\n",
    "### **Inference Provider**\n",
    "Inference Providers are third-party or Hugging Face‚Äìhosted remote backends that actually run the compute for model inference (GPUs/TPUs/etc.). Hugging Face provides a unified interface so you can call many providers (Together, Replicate, Fal.ai, OpenAI, etc.) the same way.  \n",
    "As big models need big hardware. Many LLMs, image/video and audio models require GPUs/TPUs you don‚Äôt have. This is the core problem Inference Providers solve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0fc587-d930-4356-84cf-49adb25f1eee",
   "metadata": {},
   "source": [
    "## **ü§ó Transformers**\n",
    "\n",
    "### **What is ü§ó Transformers?**\n",
    "ü§ó Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch. These models support common tasks in different modalities, such as:\n",
    "\n",
    "**üìù Natural Language Processing:** text classification, named entity recognition, question answering, language modeling, summarization, translation, multiple choice, and text generation.  \n",
    "**üñºÔ∏è Computer Vision:** image classification, object detection, and segmentation.  \n",
    "**üó£Ô∏è Audio:** automatic speech recognition and audio classification.  \n",
    "**üêô Multimodal:** table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.  \n",
    "\n",
    "ü§ó Transformers support framework interoperability between PyTorch, TensorFlow, and JAX. This provides the flexibility to use a different framework at each stage of a model‚Äôs life; train a model in three lines of code in one framework, and load it for inference in another. Models can also be exported to a format like ONNX and TorchScript for deployment in production environments.\n",
    "\n",
    "### **Installation**\n",
    "1. `transformers` can run on top of either PyTorch or TensorFlow. You need at least one of them installed.\n",
    "2. For PyTorch\n",
    "```\n",
    "! pip install torch\n",
    "```\n",
    "3. For TensorFlow\n",
    "```\n",
    "! pip install tensorflow\n",
    "! pip install tf-keras\n",
    "```\n",
    "4. Installing transformers\n",
    "```\n",
    "! pip install transformers\n",
    "```\n",
    "5. Installing tokenizers for fast tokenization\n",
    "```\n",
    "! pip install tokenizers\n",
    "```\n",
    "6. Installing datasets for efficient data loading and processing\n",
    "```\n",
    "! pip install datasets\n",
    "```\n",
    "7. Installing accelerate for easy distributed training. Accelerate: \n",
    "- chooses best device\n",
    "- distributes model\n",
    "- handles mixed precision\n",
    "```\n",
    "! pip install accelerate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e9c91cc-565b-4081-bf88-dd359066da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44b496c3-9374-45ff-9d81-122727458509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers tokenizers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e83aae-19c3-4f7a-9227-e51a03a2c0d0",
   "metadata": {},
   "source": [
    "### **Verifying Installation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0291c783-d9ca-4e76-bac8-599b2293d884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e93281c0-832c-481b-817e-8d65a3b67fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanavbansal/Developer/.env_jupyter/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.53.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f9738b-ee6c-4420-84d8-f3c05c33a4ee",
   "metadata": {},
   "source": [
    "### **Checking available GPU's**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31dd78b2-c8ba-46a3-8caf-bc5b0b15f669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "acc = Accelerator()\n",
    "print(acc.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80ee1a8d-2dcf-45f6-99f1-9152aca5b75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47deb15b-8e2f-4400-b0d3-8462d999784c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7763a8ee-bacb-4f41-b751-35fd1d6cf126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Total number of GPU's: {torch.cuda.device_count()}\")\n",
    "    print(\"Using CUDA device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS/CUDA not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b78a23f-3b04-4d1f-a68b-c15388b68fa3",
   "metadata": {},
   "source": [
    "**Note**\n",
    "Apple GPUs on M1/M2/M3 are part of a unified architecture. There is only one MPS backend."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
