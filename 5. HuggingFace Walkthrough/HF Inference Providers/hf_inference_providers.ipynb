{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4275c18c-7132-4426-9f7e-c55a958c0a65",
   "metadata": {},
   "source": [
    "# **HuggingFace Inference Providers**\n",
    "\n",
    "## **What's Covered?**\n",
    "1. Inference Providers\n",
    "    - What are Infererence Providers?\n",
    "    - Example Usage - Inferencing Meta Llama Model with \n",
    "2. HuggingFace's Inference Providers\n",
    "    - Why choose HF Inference Providers?\n",
    "    - HF Inference Provider API\n",
    "    - HF Inference Playground\n",
    "3. Initial Set Up\n",
    "    - Authentication\n",
    "    - Installation\n",
    "    - Syntax\n",
    "    - Provider Selection Policy\n",
    "4. Inferencing OpenAI's GPT OSS Model\n",
    "5. Inferencing DeepSeek Model\n",
    "6. Inferencing Text-to-Image Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b85f652-5d20-41a6-956d-35c4bbbc3600",
   "metadata": {},
   "source": [
    "## **Inference Providers**\n",
    "\n",
    "### **What are Infererence Providers?**\n",
    "Inference Providers are AI-focused technology companies, specifically falling into the categories of **inference-as-a-service** providers. They allow developers to run, host, or deploy AI models without managing their own heavy infrastructure. Few popular inference providers are:\n",
    "- Groq\n",
    "- Replicate\n",
    "- Novita AI\n",
    "- Sambanova\n",
    "- etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7ea353-1c96-4dec-bbc4-09bc2f2f4f28",
   "metadata": {},
   "source": [
    "### **Example Usage - Inferencing Meta Llama Model with Groq**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cba76af-eff4-4a88-9c86-16d75805c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca10cfe-7b5f-4150-8195-b1d10106032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=\"ENTER_YOUR_API_KEY\")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"provide 3 generative ai mcq questions with answers?\"\n",
    "      }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_completion_tokens=500,\n",
    "    top_p=1,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa2cbe9-0128-4202-8c1d-9d82b4d4d247",
   "metadata": {},
   "source": [
    "## **HuggingFace's Inference Providers** \n",
    "Hugging Face’s Inference Providers give developers access to hundreds of machine learning models, powered by world-class inference providers. For this HuggingFace has provided a separate API.\n",
    "\n",
    "### **Why choose HF Inference Providers?**\n",
    "When you build AI applications, it’s tough to manage multiple provider APIs, comparing model performance, and dealing with varying reliability. Inference Providers solves these challenges by offering:\n",
    "- **Instant Access to Cutting-Edge Models:** Go beyond mainstream providers to access thousands of specialized models across multiple AI tasks. Whether you need the latest language models, state-of-the-art image generators, or domain-specific embeddings, you’ll find them here.\n",
    "- **Zero Vendor Lock-in:** Unlike being tied to a single provider’s model catalog, you get access to models from Cerebras, Groq, Together AI, Replicate, and more — all through one consistent interface.\n",
    "- **Production-Ready Performance:** Built for enterprise workloads with the reliability your applications demand.\n",
    "- **Get Started for Free:** Inference Providers includes a generous free tier, with additional credits for PRO users and Team & Enterprise organizations. Every Hugging Face user receives monthly credits to experiment with Inference Providers. **Free users get $0.10 monthly** credits **without needing to create provider account**, subject to change.\n",
    "- **Cost-Effective:** No extra markup on provider rates\n",
    "\n",
    "### **HF Inference Provider API**\n",
    "The Inference Providers API acts as a unified proxy layer that sits between your application and multiple AI providers. When using Inference Providers, your requests go through Hugging Face’s proxy infrastructure, which provides several key benefits:\n",
    "- **Unified Authentication & Billing:** Use a single Hugging Face token for all providers\n",
    "- **Automatic Failover:** When using automatic provider selection (provider=\"auto\"), requests are automatically routed to alternative providers if the primary provider is flagged as unavailable by our validation system\n",
    "- **Consistent Interface through client libraries:** When using our client libraries, the same request format works across different providers\n",
    "\n",
    "### **HF Inference Playground**\n",
    "[Click here](https://huggingface.co/playground) to explore more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61e6840-2f6b-4b25-b222-72c137837e8d",
   "metadata": {},
   "source": [
    "## **Initial Set Up**\n",
    "\n",
    "### **Authentication**\n",
    "You’ll need a Hugging Face token to authenticate your requests. Create one by visiting your token settings and generating a fine-grained token with Make calls to Inference Providers permissions.\n",
    "\n",
    "### **Installation**\n",
    "For convenience, the `huggingface_hub` library provides an `InferenceClient` that automatically handles provider selection and request routing.\n",
    "```\n",
    "!pip install huggingface_hub\n",
    "```\n",
    "\n",
    "### **Syntax**\n",
    "```python\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    provider=\"groq\",\n",
    "    api_key=\"ENTER_YOUR_API_KEY_HERE\",\n",
    ")\n",
    "```\n",
    "\n",
    "### **Provider Selection Policy**\n",
    "- \"auto\" (default): Selects the first available provider for the model, sorted by your preference order in Inference Provider settings\n",
    "- \"specific-provider\": Forces use of a specific provider (e.g., “together”, “replicate”, “fal-ai”, …).\n",
    "- \"fastest\" or \"cheapest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0adb4b9-9fa7-45d9-8be7-529c5d97ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146c1657-1ac5-4a35-9d18-ee39ec48e9d1",
   "metadata": {},
   "source": [
    "## **Inferencing OpenAI's GPT OSS Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85515eb-e3e0-4db7-b57e-fe371ed5485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    provider=\"groq\",\n",
    "    api_key=\"ENTER_YOUR_API_KEY_HERE\",\n",
    ")\n",
    "\n",
    "result = client.chat.completions.create(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"provide 3 generative ai mcq questions with answers?\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ecbf43-c879-42ae-9151-1670e52784e5",
   "metadata": {},
   "source": [
    "## **Inferencing DeepSeek Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a76a9ae-f1c1-4175-a102-006843772085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    provider=\"novita\",\n",
    "    api_key=\"ENTER_YOUR_API_KEY_HERE\"\n",
    ")\n",
    "\n",
    "result = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Hello!\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417f811b-eae5-45f2-b415-dd3b0d0d2745",
   "metadata": {},
   "source": [
    "## **Inferencing Text-to-Image Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503efa84-ea62-4173-8b4e-bdfebdaa8c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    provider=\"auto\",\n",
    "    api_key=\"ENTER_YOUR_API_KEY_HERE\"\n",
    ")\n",
    "\n",
    "image = client.text_to_image(\n",
    "    prompt=\"A serene lake surrounded by mountains at sunset, photorealistic style\",\n",
    "    model=\"black-forest-labs/FLUX.1-dev\"\n",
    ")\n",
    "\n",
    "# Save the generated image\n",
    "image.save(\"generated_image.png\")\n",
    "\n",
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
