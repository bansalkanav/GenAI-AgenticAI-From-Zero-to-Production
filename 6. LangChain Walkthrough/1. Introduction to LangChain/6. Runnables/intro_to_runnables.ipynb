{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6c6d136-1c84-4c1b-b545-735fd03f951f",
   "metadata": {},
   "source": [
    "# **Runnables**\n",
    "\n",
    "Problem-solving typically involves a series of steps. When using LLMs, some applications have relatively complex requirements and a sequence of calls for performing an operation. \n",
    "\n",
    "Let's now learn how to put multiple chains together in an organized way.\n",
    "\n",
    "## **What's covered?**\n",
    "1. What is a Runnable?\n",
    "2. RunnableLambda - Converting Custom Python Functions to Runnables\n",
    "3. What are RunnableSequence?\n",
    "4. Example: Putting Multiple Runnable in RunnableSequence\n",
    "5. What are RunnableParallel?\n",
    "6. What are RunnablePassthrough?\n",
    "    - Just passing through\n",
    "    - Adding new keys to input\n",
    "7. Case Study: Building an AI Research Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36332f50-766f-4555-b62d-1fe99b9749bc",
   "metadata": {},
   "source": [
    "## **What is a Runnable?**\n",
    "In LangChain, runnables are powerful abstractions representing any callable unit of work. They encapsulate and manage various tasks, including LLM calls, database queries and calls to external APIs.\n",
    "\n",
    "All the components seen before i.e. Templates, Chat Models and Output Parsers inherit the RunnableInterface that enables them to work consistently. Methods like invoke(), stream() and batch() are common for all runnable components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3f9160-fc7d-4d44-b095-6da746aa6c3c",
   "metadata": {},
   "source": [
    "## **RunnableLambda - Converting Custom Python Functions to Runnables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c09e6414-1122-4ff2-a4aa-2afa094f2fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'>\n",
      "<class 'function'>\n"
     ]
    }
   ],
   "source": [
    "def sum_method(x: int) -> int:\n",
    "    return x + x\n",
    "\n",
    "def multiply_method(x: int) -> int:\n",
    "    return x * x\n",
    "\n",
    "\n",
    "print(type(sum_method))\n",
    "print(type(multiply_method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7942b9b4-e0a5-4c2d-8a11-216ffeb62654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableLambda'>\n",
      "<class 'langchain_core.runnables.base.RunnableLambda'>\n"
     ]
    }
   ],
   "source": [
    "# Converting methods to Runnables\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "runnable_1 = RunnableLambda(sum_method)\n",
    "runnable_2 = RunnableLambda(multiply_method)\n",
    "\n",
    "print(type(runnable_1))\n",
    "print(type(runnable_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56e029e-9b77-42bd-bb98-fb779a7bc301",
   "metadata": {},
   "source": [
    "## **What are `RunnableSequence`?**\n",
    "\n",
    "Note that all the chains are RunnableSequence.\n",
    "\n",
    "LangChain implements the RunnableInterface, which allows the composition or chaining of various components into a RunnableSequence.  \n",
    "\n",
    "You can compose these Runnable objects together to create a pipeline of operations.\n",
    "\n",
    "**Syntax A:**  \n",
    "```python\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "chain = RunnableSequence(\n",
    "    first=runnable_a, \n",
    "    middle=[runnable_b, runnable_c],\n",
    "    last=runnable_d\n",
    ")\n",
    "```\n",
    "\n",
    "**Syntax B:**  \n",
    "```python\n",
    "chain = runnable_a | runnable_b | runnable_c | runnable_d\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2021a71a-3824-42c7-9e9d-18994f4f26ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining a Runnable Sequence\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "runnable_sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n",
    "\n",
    "runnable_sequence.invoke(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e993af30-d3b2-4294-9797-d29976945c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another way to define Runnable Sequence\n",
    "\n",
    "runnable_sequence = runnable_1 | runnable_2\n",
    "\n",
    "runnable_sequence.invoke(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13b422ad-cb65-4e93-be0f-7f249133190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install grandalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32925fc0-8024-43f1-94f8-e8811340d36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   +------------------+    \n",
      "   | sum_method_input |    \n",
      "   +------------------+    \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "      +------------+       \n",
      "      | sum_method |       \n",
      "      +------------+       \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "    +-----------------+    \n",
      "    | multiply_method |    \n",
      "    +-----------------+    \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "+------------------------+ \n",
      "| multiply_method_output | \n",
      "+------------------------+ \n"
     ]
    }
   ],
   "source": [
    "runnable_sequence.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb04324b-94b7-4a6e-becd-710d62a592d1",
   "metadata": {},
   "source": [
    "## **Example: Putting Multiple Runnable in RunnableSequence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "059e0647-5aa6-43e1-9005-d4f8799e1055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO, ALICE! THE CURRENT DATE AND TIME IS 2025-12-01 19:07:37!\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda, RunnableSequence\n",
    "\n",
    "\n",
    "# Define the transformations as simple functions\n",
    "def greet(name):\n",
    "   return f\"Hello, {name}!\"\n",
    "\n",
    "\n",
    "def append_datetime(text):\n",
    "   current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "   return f\"{text} The current date and time is {current_datetime}\"\n",
    "\n",
    "\n",
    "def to_uppercase(text):\n",
    "   return text.upper()\n",
    "\n",
    "\n",
    "def add_exclamation(text):\n",
    "   return f\"{text}!\"\n",
    "\n",
    "\n",
    "# Wrap the functions in RunnableWrapper\n",
    "greet_runnable = RunnableLambda(lambda x: greet(x))\n",
    "datetime_runnable = RunnableLambda(lambda x: append_datetime(x))\n",
    "uppercase_runnable = RunnableLambda(lambda x: to_uppercase(x))\n",
    "exclamation_runnable = RunnableLambda(lambda x: add_exclamation(x))\n",
    "\n",
    "\n",
    "# Create a RunnableSequence with the wrapped runnables\n",
    "chain = RunnableSequence(\n",
    "   first=greet_runnable,\n",
    "   middle=[datetime_runnable, uppercase_runnable],\n",
    "   last=exclamation_runnable,\n",
    ")\n",
    "\n",
    "# Apply the chain to some input data\n",
    "input_data = \"Alice\"\n",
    "result = chain.invoke(input_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58be5920-3965-477f-80b5-7b331f04ce54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO, ALICE! THE CURRENT DATE AND TIME IS 2025-12-01 19:07:37!\n"
     ]
    }
   ],
   "source": [
    "# Create a RunnableSequence with the wrapped runnables\n",
    "chain = greet_runnable | datetime_runnable | uppercase_runnable | exclamation_runnable\n",
    "\n",
    "# Apply the chain to some input data\n",
    "input_data = \"Alice\"\n",
    "result = chain.invoke(input_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64e3363-3937-4414-84bf-73eddeec125b",
   "metadata": {},
   "source": [
    "## **What are `RunnableParallel`?**\n",
    "\n",
    "Runnable that runs a mapping of Runnables in parallel, and returns a mapping of their outputs.\n",
    "\n",
    "RunnableParallel is one of the two main composition primitives for the LCEL, alongside RunnableSequence. It invokes Runnables concurrently, providing the same input to each.\n",
    "\n",
    "**Syntax:**\n",
    "```python\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "par = RunnableParallel({\n",
    "    'key_a': runnable_a, \n",
    "    'key_b': runnable_b,\n",
    "    ...\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5c38d15-3713-4722-9b21-642ca40ccd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_method(x: int) -> int:\n",
    "    return x + x\n",
    "\n",
    "def multiply_method(x: int) -> int:\n",
    "    return x * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cd2a83e-1571-4ed1-8166-cea3b3bd70d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting methods to Runnables\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "runnable_1 = RunnableLambda(sum_method)\n",
    "runnable_2 = RunnableLambda(multiply_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60cd946a-2471-4c99-9f51-c401e539eee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sum_out': 10, 'mult_out': 25}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining a Runnable Parallel\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "runnable_parallel = RunnableParallel({'sum_out': runnable_1, 'mult_out': runnable_2})\n",
    "\n",
    "# # Another Way to Define RunnableParallel\n",
    "# runnable_parallel = RunnableParallel(sum_out=runnable_1, mult_out=runnable_2)\n",
    "\n",
    "runnable_parallel.invoke(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93e94639-baef-4a0d-ba4f-dcfa2fdcd6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +---------------------------------+         \n",
      "     | Parallel<sum_out,mult_out>Input |         \n",
      "     +---------------------------------+         \n",
      "              ***            ***                 \n",
      "            **                  **               \n",
      "          **                      **             \n",
      "+------------+              +-----------------+  \n",
      "| sum_method |              | multiply_method |  \n",
      "+------------+              +-----------------+  \n",
      "              ***            ***                 \n",
      "                 **        **                    \n",
      "                   **    **                      \n",
      "    +----------------------------------+         \n",
      "    | Parallel<sum_out,mult_out>Output |         \n",
      "    +----------------------------------+         \n"
     ]
    }
   ],
   "source": [
    "runnable_parallel.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d03fd43-8f42-4cad-8908-85f9a080f9f1",
   "metadata": {},
   "source": [
    "## **RunnablePassThrough**\n",
    "\n",
    "Imagine you have a message, and you want to hand that exact message over to the next person in line, without changing it at all. That's RunnablePassthrough! It simply takes whatever input it receives and passes it along, untouched, to the next runnable in your chain.\n",
    "\n",
    "On its own, it might seem pointless. **\"Why would I add a component that does nothing?\"** you might ask.\n",
    "> - The magic isn't in what it does by itself, but in how it enables other, more complex operations (especially when combined with .assign() or parallel processing, which we'll see next!).\n",
    "> - Think of it as a placeholder or a distributor of the original input.\n",
    "\n",
    "\n",
    "Runnable to passthrough inputs either:\n",
    "\n",
    "- unchanged - RunnablePassthrought()  \n",
    "(or)\n",
    "- with additional keys - with the help of assign method - RunnablePassthrough.assign(variable_name=value)\n",
    "\n",
    "This Runnable behaves almost like the identity function, except that it can be configured to add additional keys to the output, if the input is a dict.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f8c9fb-dec2-4811-906a-acc0fbb17ac7",
   "metadata": {},
   "source": [
    "### **Just passing through**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "040bf4e4-1533-443f-b629-2343a93ec645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "def inspect_input(input_data):\n",
    "    print(f\"Data Recieved from RunnableLambda: {input_data}\")\n",
    "    return input_data # Always return something for the chain to continue\n",
    "\n",
    "inspector = RunnableLambda(inspect_input)\n",
    "\n",
    "chain = RunnablePassthrough() | inspector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cca4032-23ff-414a-82d6-e64c15123d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Recieved from RunnableLambda: Hello LangChain!\n",
      "Hello LangChain!\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke(\"Hello LangChain!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bee9b980-5dbf-453b-b7d8-010fd7f93200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Recieved from RunnableLambda: {'name': 'Alice', 'age': 30}\n",
      "{'name': 'Alice', 'age': 30}\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\"name\": \"Alice\", \"age\": 30})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a2a6917-8ba5-4598-b518-8670871862c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'origin': 1, 'modified': 2}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "\n",
    "runnable = RunnableParallel({\n",
    "    \"origin\":RunnablePassthrough(),\n",
    "    \"modified\":lambda x: x+1\n",
    "})\n",
    "\n",
    "runnable.invoke(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "659467af-24e2-4b90-b684-56304fb38292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  +--------------------------------+  \n",
      "  | Parallel<origin,modified>Input |  \n",
      "  +--------------------------------+  \n",
      "            ***         **            \n",
      "           *              **          \n",
      "         **                 *         \n",
      "+-------------+          +--------+   \n",
      "| Passthrough |          | Lambda |   \n",
      "+-------------+          +--------+   \n",
      "            ***         **            \n",
      "               *      **              \n",
      "                **   *                \n",
      " +---------------------------------+  \n",
      " | Parallel<origin,modified>Output |  \n",
      " +---------------------------------+  \n"
     ]
    }
   ],
   "source": [
    "runnable.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b33e552-bebf-4cf6-b07b-5964165ad5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified Example\n",
    "\n",
    "runnable = {\"original\": RunnablePassthrough()} | RunnableLambda(lambda x: x[\"original\"]+1)\n",
    "\n",
    "# # Guess the output of the following\n",
    "# runnable.invoke(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cec1774-bd78-4b5f-93ac-a8bf2063044e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+  \n",
      "| Parallel<original>Input |  \n",
      "+-------------------------+  \n",
      "              *              \n",
      "              *              \n",
      "              *              \n",
      "      +-------------+        \n",
      "      | Passthrough |        \n",
      "      +-------------+        \n",
      "              *              \n",
      "              *              \n",
      "              *              \n",
      "        +--------+           \n",
      "        | Lambda |           \n",
      "        +--------+           \n",
      "              *              \n",
      "              *              \n",
      "              *              \n",
      "      +--------------+       \n",
      "      | LambdaOutput |       \n",
      "      +--------------+       \n"
     ]
    }
   ],
   "source": [
    "runnable.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0571aa4-22a6-4ffd-a49d-cfefe87c3212",
   "metadata": {},
   "source": [
    "### **Adding New Keys to Input**\n",
    "\n",
    "RunnablePassthrough.assign() takes your existing input, runs some other runnable(s) to create new keys in the input dictionary, and then passes this expanded dictionary to the next step.\n",
    "\n",
    "Imagine you have a file folder with some initial documents. You want to add new documents to that same folder, based on what's already inside, and then pass the entire, updated folder to the next person. assign() is like adding those new documents.\n",
    "\n",
    "Syntax:\n",
    "```python\n",
    "some_chain = RunnablePassthrough.assign(\n",
    "    new_key_1 = some_runnable_that_produces_value_for_new_key_1,\n",
    "    new_key_2 = some_runnable_that_produces_value_for_new_key_2,\n",
    "    # ... and so on\n",
    ")\n",
    "```\n",
    "\n",
    "**Note:**\n",
    "1. The `some_runnable_that_produces_value_for_new_key_X` will receive the original input that was passed to some_chain.\n",
    "2. The output of that runnable becomes the value for `new_key_X`.\n",
    "3. The output of assign() itself will be the original input dictionary merged with these newly added key-value pairs.\n",
    "\n",
    "**Important:**  \n",
    "In some cases, it may be useful to pass the input through while adding some keys to the output. In this case, you can use the assign method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c456be6-c967-482d-913d-1906b8ec7e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm1': 'genai', 'llm2': 'developer', 'total_chars': 14}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import random\n",
    "\n",
    "# Fake LLM for the example\n",
    "def fake_llm(prompt: str) -> str:\n",
    "    word_list = [\"python\", \"data\", \"science\", \"genai\", \"developer\"]\n",
    "    return random.choice(word_list)\n",
    "\n",
    "runnable = {\n",
    "    'llm1':  fake_llm,   # Passing the Function as a callback\n",
    "    'llm2':  fake_llm,   # Passing the Function as a callback\n",
    "} | RunnablePassthrough.assign(\n",
    "    total_chars=lambda inputs: len(inputs['llm1'] + inputs['llm2'])\n",
    ")\n",
    "\n",
    "runnable.invoke('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b1b6b6c-565c-4ff4-81d7-2e719768ff0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   +--------------------------+      \n",
      "   | Parallel<llm1,llm2>Input |      \n",
      "   +--------------------------+      \n",
      "           **         **             \n",
      "         **             **           \n",
      "        *                 *          \n",
      "+----------+          +----------+   \n",
      "| fake_llm |          | fake_llm |   \n",
      "+----------+          +----------+   \n",
      "           **         **             \n",
      "             **     **               \n",
      "               *   *                 \n",
      "  +---------------------------+      \n",
      "  | Parallel<llm1,llm2>Output |      \n",
      "  +---------------------------+      \n",
      "                 *                   \n",
      "                 *                   \n",
      "                 *                   \n",
      "  +----------------------------+     \n",
      "  | Parallel<total_chars>Input |     \n",
      "  +----------------------------+     \n",
      "           **         **             \n",
      "         **             **           \n",
      "        *                 *          \n",
      " +--------+          +-------------+ \n",
      " | Lambda |          | Passthrough | \n",
      " +--------+          +-------------+ \n",
      "           **         **             \n",
      "             **     **               \n",
      "               *   *                 \n",
      "  +-----------------------------+    \n",
      "  | Parallel<total_chars>Output |    \n",
      "  +-----------------------------+    \n"
     ]
    }
   ],
   "source": [
    "runnable.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d091f7eb-ce36-4244-86b4-ab4d02f14237",
   "metadata": {},
   "source": [
    "## **Case Study: Building an AI Research Assistant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e05e5c17-ca42-4fe2-9cfe-574ee5de1db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e0a69de-8699-40e8-bbc8-a3c0a8ec993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API Key\n",
    "\n",
    "f = open('keys/.openai_api_key.txt')\n",
    "\n",
    "OPENAI_API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ab2b3c57-4013-4eed-ae34-115df7475715",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()\n",
    "\n",
    "chat_model = ChatOpenAI(api_key=OPENAI_API_KEY,\n",
    "                               model=\"gpt-4o-mini\",\n",
    "                               temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8911bf30-77d0-434d-8271-f637845b4175",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a research assistant and scientific writer.\n",
    "                  You take in requests about the topics and write organized research reports on those topics.\n",
    "                  Also you share the appropriate references at the end of report.\"\"\"), \n",
    "    (\"human\", \"\"\"Write an organized research report about {topic}.\"\"\")\n",
    "])\n",
    "\n",
    "writer_chain = writer_chat_template | chat_model | output_parser\n",
    "\n",
    "# research_report = writer_chain.invoke({\"topic\": \"how transformers algorithm works?\"})\n",
    "\n",
    "# print(research_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c397f24e-60f5-4b93-8015-fc090613b03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a reviewer for research reports. \n",
    "                  You take in research reports and provide a feedback on them.\"\"\"\n",
    "    ), \n",
    "    (\"human\", \"\"\"Provide feedback as 5 concise bullet points on this research report: \n",
    "                 \n",
    "                 {report}\"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "reviewer_chain = reviewer_chat_template | chat_model | output_parser\n",
    "\n",
    "# report_feedback = reviewer_chain.invoke({\"report\": research_report})\n",
    "\n",
    "# print(report_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3ba7bb91-2ab1-40d0-8c65-1ce0d5be7778",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_writer_chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a research assistant and scientific writer.\n",
    "                  You take in a research report in a set of bullet points with feedback to improve.\n",
    "                  You revise the research report based on the feedback and write a final version.\"\"\"\n",
    "    ), \n",
    "    (\"human\", \"\"\"Write a reviewed and improved version of research report: \n",
    "\n",
    "                {report}\n",
    "                \n",
    "                based on this feedback:\n",
    "                \n",
    "                {feedback}\"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "final_writer_chain = final_writer_chat_template | chat_model | output_parser\n",
    "\n",
    "# final_report = final_writer_chain.invoke({\"report\": research_report, \"feedback\": report_feedback})\n",
    "\n",
    "# print(final_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a5abb2-11f7-421b-a9c2-0dd72287dc97",
   "metadata": {},
   "source": [
    "### **What are these chains?**\n",
    "\n",
    "Given that we have created three chains above, let's now analyse what these chains are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c8930c71-9e3b-47c5-b1cb-3884879bccc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "print(type(writer_chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f32febdb-69e5-4e75-837b-c0b2396ba727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "print(type(reviewer_chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ea149a3-2dd8-4745-b196-bdd27b1fd961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "print(type(final_writer_chain))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49229ea9-8be6-4501-901e-1dae918ce531",
   "metadata": {},
   "source": [
    "### **Composing the Chains together**\n",
    "\n",
    "**RunnablePassthrough** for passing data unchanged from previous steps for use as input in later steps.\n",
    "\n",
    "<img src=\"images/composing_chains.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3fc48abc-5986-493b-9dfd-dfcbbbfdb0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "composed_chain = {\"report\" : writer_chain} | RunnablePassthrough().assign(feedback=reviewer_chain) | final_writer_chain\n",
    "\n",
    "final_report = composed_chain.invoke({\"topic\": \"What are Runnables in LangChain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "156f5f8b-1d50-43fd-9853-cf594273ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Another Way\n",
    "\n",
    "# report_output = RunnableLambda(lambda out : {\"report\" : out})\n",
    "\n",
    "# composed_chain = writer_chain | report_output | RunnablePassthrough().assign(feedback=reviewer_chain) | final_writer_chain\n",
    "# final_report = composed_chain.invoke({\"topic\": \"What are Runnables in LangChain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd862c7e-bdd2-44aa-abee-8012f2fe6af9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Research Report: Runnables in LangChain\n",
       "\n",
       "## Introduction\n",
       "LangChain is an innovative framework designed to facilitate the development of applications that utilize language models. A key component of LangChain is the concept of \"Runnables,\" which serve as fundamental building blocks within the framework. Runnables enable developers to create modular and reusable components that can execute various tasks related to language processing. This report provides a comprehensive overview of Runnables in LangChain, including their definition, functionality, use cases, advantages, and future directions.\n",
       "\n",
       "## Definition of Runnables\n",
       "Runnables in LangChain are abstractions that encapsulate specific tasks or operations that can be executed independently or in conjunction with other Runnables. Each Runnable can take inputs, perform computations or transformations, and produce outputs. This modular approach allows developers to compose complex workflows by chaining multiple Runnables together.\n",
       "\n",
       "**Key Takeaway**: Runnables are self-contained units of work that enhance modularity and reusability in application development.\n",
       "\n",
       "## Functionality of Runnables\n",
       "Runnables in LangChain are designed to be flexible and versatile, representing a wide range of operations, including:\n",
       "\n",
       "1. **Text Generation**: Runnables can invoke language models to generate text based on given prompts.\n",
       "2. **Data Transformation**: They can process and transform data, such as cleaning or formatting text.\n",
       "3. **API Calls**: Runnables can make API requests to external services, integrating additional functionalities into the application.\n",
       "4. **Conditional Logic**: Developers can implement conditional logic within Runnables to control the flow of execution based on specific criteria.\n",
       "\n",
       "### Structure of Runnables\n",
       "A typical Runnable in LangChain consists of:\n",
       "- **Input Parameters**: The data or parameters required for execution.\n",
       "- **Execution Logic**: The core functionality that defines what the Runnable does.\n",
       "- **Output**: The result produced after execution, which can be passed to other Runnables or returned to the user.\n",
       "\n",
       "**Key Takeaway**: The structure of Runnables promotes clarity and organization in code, making it easier for developers to manage complex tasks.\n",
       "\n",
       "## Use Cases\n",
       "Runnables can be employed in various applications, including:\n",
       "\n",
       "1. **Chatbots**: Runnables can handle user queries, generate responses, and manage conversation flows.\n",
       "   - *Example*: A chatbot that uses Runnables to process user input, generate a response using a language model, and log the conversation for future analysis.\n",
       "   \n",
       "2. **Content Creation**: They can automate the generation of articles, summaries, or other written content based on user-defined parameters.\n",
       "   - *Example*: A content generation tool that utilizes Runnables to create blog posts based on keywords provided by the user.\n",
       "\n",
       "3. **Data Analysis**: Runnables can process and analyze text data, extracting insights or performing sentiment analysis.\n",
       "   - *Example*: A sentiment analysis application that uses Runnables to analyze customer feedback and categorize sentiments.\n",
       "\n",
       "4. **Workflow Automation**: By chaining multiple Runnables, developers can create complex workflows that automate repetitive tasks.\n",
       "   - *Example*: An automated reporting system that collects data, generates reports, and sends them via email using a series of interconnected Runnables.\n",
       "\n",
       "**Key Takeaway**: Runnables are versatile and can be applied across various domains, enhancing the functionality of applications.\n",
       "\n",
       "## Advantages of Runnables\n",
       "The use of Runnables in LangChain offers several advantages:\n",
       "\n",
       "1. **Modularity**: Runnables promote a modular design, allowing developers to build and maintain applications more easily.\n",
       "2. **Reusability**: Once created, Runnables can be reused across different projects, saving time and effort.\n",
       "3. **Scalability**: The ability to chain Runnables together enables the development of scalable applications that can handle complex tasks.\n",
       "4. **Ease of Integration**: Runnables can easily integrate with other components of LangChain and external services, enhancing the overall functionality of applications.\n",
       "\n",
       "**Key Takeaway**: Runnables enhance development efficiency and application performance through their modular and reusable nature.\n",
       "\n",
       "## Comparative Analysis\n",
       "When compared to similar concepts in other frameworks, Runnables in LangChain stand out due to their flexibility and ease of integration. For instance, while other frameworks may offer similar modular components, Runnables provide a more intuitive interface for chaining operations and managing dependencies. This unique approach allows developers to create complex workflows with minimal overhead.\n",
       "\n",
       "## Future Directions\n",
       "As the LangChain framework evolves, there are several potential developments for Runnables:\n",
       "\n",
       "1. **Enhanced Error Handling**: Future versions could introduce more robust error handling mechanisms to improve the reliability of Runnables.\n",
       "2. **Performance Optimization**: Ongoing efforts to optimize the execution speed of Runnables could enhance application performance.\n",
       "3. **Expanded Library of Runnables**: The introduction of a broader library of pre-built Runnables could facilitate quicker development for common tasks.\n",
       "4. **Community Contributions**: Encouraging community contributions could lead to innovative use cases and enhancements for Runnables.\n",
       "\n",
       "**Key Takeaway**: The future of Runnables in LangChain is promising, with opportunities for growth and improvement that can further enhance their utility.\n",
       "\n",
       "## Conclusion\n",
       "Runnables are a core feature of the LangChain framework, providing a powerful and flexible way to build language model applications. By encapsulating tasks into modular components, Runnables facilitate the development of complex workflows while promoting reusability and scalability. As the demand for language processing applications continues to grow, understanding and leveraging Runnables will be essential for developers working within the LangChain ecosystem.\n",
       "\n",
       "## References\n",
       "1. LangChain Documentation. (2023). Retrieved from [LangChain Documentation](https://langchain.readthedocs.io/en/latest/)\n",
       "2. LangChain GitHub Repository. (2023). Retrieved from [LangChain GitHub](https://github.com/hwchase17/langchain)\n",
       "3. OpenAI. (2023). Language Models: A Survey. Retrieved from [OpenAI Research](https://openai.com/research/language-models)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(final_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
