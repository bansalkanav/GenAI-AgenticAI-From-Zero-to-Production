{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca9e88c5-c154-4834-8175-6862ebc2227b",
   "metadata": {},
   "source": [
    "# **Chat Models**\n",
    "\n",
    "## **What's Covered?**\n",
    "1. Introduction to Chat Models\n",
    "    - What are Chat Models?\n",
    "    - Capabilities\n",
    "    - Integrations\n",
    "2. Building Chat Model - GoogleAI, Groq and OpenAI\n",
    "    - Installing the libraries\n",
    "    - Setting up the API Key\n",
    "    - Instantiating the Chat Model and Standard Parameters\n",
    "    - Key Methods\n",
    "    - invoke() Method\n",
    "    - stream() Method\n",
    "    - batch(list_of_message_lists) Method\n",
    "3. HuggingFace Chat Models\n",
    "    - What is HuggingFace?\n",
    "    - Why use HuggingFace Chat Models with LangChain?\n",
    "    - Installation\n",
    "    - Hugging Face Local Pipelines\n",
    "    - Huggingface Endpoints (COMING SOON)\n",
    "    - ChatHuggingFace (COMING SOON)\n",
    "4. LangChain New Unified Chat Model API\n",
    "    - Basic usage\n",
    "    - Initialize a model\n",
    "    - Parameters\n",
    "    - Example Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753337d-8fa1-4590-81a3-bedc01202905",
   "metadata": {},
   "source": [
    "## **Introduction to Chat Models**\n",
    "\n",
    "LangChain provides a unified interface for interacting with various chat models from different providers (OpenAI, Google, Anthropic, Cohere, etc.).\n",
    "\n",
    "LangChain chat models are named with a convention that prefixes \"Chat\" to their class names (e.g., ChatOllama, ChatAnthropic, ChatOpenAI, etc.).\n",
    "\n",
    "\n",
    "### **What are Chat Models?**\n",
    "At their core, chat models are a specialized type of Large Language Model (LLM) designed and fine-tuned to engage in conversational interactions. Unlike older \"text completion\" models (which simply predict the next word given a string of text), chat models understand and operate on a concept of \"messages\" with associated \"roles.\"\n",
    "\n",
    "Modern LLMs are typically accessed through a chat model interface that takes a list of messages as input and returns an AI message as output. Chat Models are customized for conversational usage. **[Click Here](https://python.langchain.com/docs/integrations/chat/)** to check the complete list of LLMs which can be used with LangChain.\n",
    "\n",
    "**Chat Models Input: A list of BaseMessage objects (typically SystemMessage, HumanMessage, AIMessage)**\n",
    "```\n",
    "[SystemMessage(content=\"You are a helpful assistant.\"), HumanMessage(content=\"What is the capital of France?\")]\n",
    "```\n",
    "**Chat Models Output: A single AIMessage object**\n",
    "```\n",
    "AIMessage(content=\"The capital of France is Paris.\")\n",
    "```\n",
    "\n",
    "### **Capabilities**\n",
    "\n",
    "The newest generation of chat models offer additional capabilities:\n",
    "1. [Tool calling](https://python.langchain.com/docs/concepts/tool_calling/): Many popular chat models offer a native tool calling API. This API allows developers to build rich applications that enable LLMs to interact with external services, APIs, and databases. Tool calling can also be used to extract structured information from unstructured data and perform various other tasks.\n",
    "2. [Structured output](https://python.langchain.com/docs/concepts/structured_outputs/): A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\n",
    "3. [Multimodality](https://python.langchain.com/docs/concepts/multimodality/): The ability to work with data other than text; for example, images, audio, and video.\n",
    "\n",
    "### **Integrations**\n",
    "LangChain has many chat model integrations that allow you to use a wide variety of models from different providers. These integrations are one of two types:\n",
    "\n",
    "1. **Official models:** These are models that are officially supported by LangChain and/or model provider. You can find these models in the **`langchain-<provider>`** packages.\n",
    "2. **Community models:** There are models that are mostly contributed and supported by the community. You can find these models in the **`langchain-community`** package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc81099d-8426-4f8e-8e9d-0cccd4915da3",
   "metadata": {},
   "source": [
    "## **Building Chat Model - GoogleAI, Groq and OpenAI**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b45c64-704c-45cd-b193-ed403ebbae43",
   "metadata": {},
   "source": [
    "### **Installing the libraries**\n",
    "\n",
    "```python\n",
    "! pip install --upgrade --quiet langchain-google-genai\n",
    "! pip install --upgrade --quiet langchain-openai\n",
    "! pip install --upgrade --quiet langchain-groq\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1698394c-12ae-4f5a-bf2f-f731f10a61dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade --quiet langchain-google-genai\n",
    "# ! pip install --upgrade --quiet langchain-openai\n",
    "# ! pip install --upgrade --quiet langchain-groq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e337e240-46e5-4667-a9dd-dd969d5771b2",
   "metadata": {},
   "source": [
    "### **Setting up the API Key**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e380313c-365e-4a48-b96b-9828f6ae9f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API Key\n",
    "\n",
    "f = open('keys/.gemini.txt')\n",
    "\n",
    "GOOGLE_API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4ddbf00-1071-45a7-88d4-0885150798f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API Key\n",
    "\n",
    "f = open('keys/.openai_api_key.txt')\n",
    "\n",
    "OPENAI_API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "887160d9-ef43-4642-b425-254cd8aaa999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API Key\n",
    "\n",
    "f = open('keys/.groq_api_key.txt')\n",
    "\n",
    "GROQ_API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952edee7-87d3-4c74-a6e9-f57859fe38fa",
   "metadata": {},
   "source": [
    "### **Instantiating the Chat Model and Standard Parameters**\n",
    "\n",
    "Standard parameters are currently only enforced on integrations that have their own integration packages (e.g. langchain-openai, langchain-anthropic, etc.), they're not enforced on models in langchain-community.\n",
    "\n",
    "| Parameter      | Description |\n",
    "|--------------|-------------|\n",
    "| model        | The name or identifier of the specific AI model you want to use (e.g., \"gpt-3.5-turbo\" or \"gpt-4\"). |\n",
    "| temperature  | Controls the randomness of the model's output. A higher value (e.g., 1.0) makes responses more creative, while a lower value (e.g., 0.0) makes them more deterministic and focused. |\n",
    "| timeout      | The maximum time (in seconds) to wait for a response from the model before canceling the request. Ensures the request doesn’t hang indefinitely. |\n",
    "| max_tokens   | Limits the total number of tokens (words and punctuation) in the response. This controls how long the output can be. |\n",
    "| stop         | Specifies stop sequences that indicate when the model should stop generating tokens. For example, you might use specific strings to signal the end of a response. |\n",
    "| max_retries  | The maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits. |\n",
    "| api_key      | The API key required for authenticating with the model provider. This is usually issued when you sign up for access to the model. |\n",
    "| base_url     | The URL of the API endpoint where requests are sent. This is typically provided by the model's provider and is necessary for directing your requests. |\n",
    "| rate_limiter | An optional BaseRateLimiter to space out requests to avoid exceeding rate limits. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "079087d1-4adc-437a-bd8e-899e8a255294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ChatModel\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Pass the standard parameters during initialization\n",
    "google_chat_model = ChatGoogleGenerativeAI(api_key=GOOGLE_API_KEY, \n",
    "                                           model=\"gemini-2.0-flash\", \n",
    "                                           temperature=1)\n",
    "\n",
    "openai_chat_model = ChatOpenAI(api_key=OPENAI_API_KEY, \n",
    "                               model=\"gpt-4o-mini\", \n",
    "                               temperature=1)\n",
    "\n",
    "gorq_chat_model = ChatGroq(api_key=GROQ_API_KEY, \n",
    "                           model=\"openai/gpt-oss-20b\", \n",
    "                           temperature=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64bc476-f0f8-415c-a506-f90cbe30c15d",
   "metadata": {},
   "source": [
    "### **Key Methods**\n",
    "The key methods of a chat model are:\n",
    "\n",
    "1. **invoke:** The primary method for interacting with a chat model. It takes a list of messages as input and returns a list of messages as output.\n",
    "2. **stream:** A method that allows you to stream the output of a chat model as it is generated, token by token. This is crucial for building responsive user interfaces.\n",
    "3. **batch:** A method that allows you to batch multiple requests to a chat model together for more efficient processing.\n",
    "4. **bind_tools:** A method that allows you to bind a tool to a chat model for use in the model's execution context.\n",
    "5. **with_structured_output:** A wrapper around the invoke method for models that natively support structured output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7e2c13-246e-46b9-84d3-3edd31bcdbae",
   "metadata": {},
   "source": [
    "### **invoke() Method**\n",
    "\n",
    "The primary method to send a list of messages to the model and get a single response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba51f646-ef6e-4f0d-9ce5-bf6c9936c195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a polite assistant.\"),\n",
    "    HumanMessage(content=\"Hello!\"),\n",
    "]\n",
    "\n",
    "response = google_chat_model.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7617c07a-9e81-46f2-a280-ddb4221f017e",
   "metadata": {},
   "source": [
    "### **stream() Method** \n",
    "\n",
    "Allows you to receive the model's response incrementally, token by token. This is crucial for building responsive user interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f85f2188-fa50-441d-a186-a7d8c66e4d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Streaming Response ---\n",
      "Hello! How can I assist you today?\n",
      "--- End Streaming ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Streaming Response ---\")\n",
    "\n",
    "for chunk in openai_chat_model.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "    \n",
    "print(\"\\n--- End Streaming ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9e8f97-1f46-4627-9649-d21de0124b78",
   "metadata": {},
   "source": [
    "### **batch(list_of_message_lists) Method** \n",
    "\n",
    "For sending multiple sets of messages in a single API call (if the provider supports it), which can be more efficient.\n",
    "- **Batch = parallel inference**\n",
    "- Used if you want speed, cheaper cost, or higher throughput  \n",
    "\n",
    "\n",
    "Batching has huge applications during:\n",
    "1. Embedding Large Corpora\n",
    "2. Multi-Chunk Summarization (eg: processing a book)\n",
    "3. Batch Evaluation (eg: evaluating the answer sheets)\n",
    "4. Dataset labeling with LLMs\n",
    "5. Chatbots with parallel multi-user requests (eg: You can reduce inference cost by grouping user messages into batches dynamically)\n",
    "\n",
    "**Important Note**  \n",
    "Even if a model can handle 1000 parallel calls, your app may not:\n",
    "- OS has max open sockets\n",
    "- your backend has max thread pool size\n",
    "- network bandwidth becomes a bottleneck\n",
    "\n",
    "**Final Notes**  \n",
    "- Batch processing ≠ Infinite parallel requests\n",
    "- Batch processing = Efficient organization of requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "243e47c1-d055-4e99-93d4-caecf9012272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 + 1 = 2\n",
      "The capital of India is **New Delhi**.\n"
     ]
    }
   ],
   "source": [
    "batch_messages = [\n",
    "    [HumanMessage(content=\"What is 1+1?\")],\n",
    "    [HumanMessage(content=\"What is the capital of India?\")],\n",
    "]\n",
    "responses = google_chat_model.batch(batch_messages)\n",
    "\n",
    "for res in responses:\n",
    "    print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be28542c-433a-4347-90b3-d0f435cb82d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 + 1 = 2\n",
      "The capital of India is **New Delhi**.\n"
     ]
    }
   ],
   "source": [
    "for response in google_chat_model.batch_as_completed(batch_messages):\n",
    "    print(response[1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2dee74-1dac-47de-9f6d-6472435e4b5a",
   "metadata": {},
   "source": [
    "## **HuggingFace Chat Models**\n",
    "\n",
    "HuggingFace is an incredibly popular platform and community that has democratized access to state-of-the-art machine learning models, especially in Natural Language Processing (NLP). When it comes to chat models, HuggingFace hosts a vast array of models, many of which can be used for conversational AI.\n",
    "\n",
    "### **What is HuggingFace?**\n",
    "\n",
    "HuggingFace is a giant online library and community for machine learning models. \n",
    "\n",
    "They provide:\n",
    "1. **Transformers Library:** A powerful Python library that makes it easy to download, train, and use pre-trained NLP models (including chat models).\n",
    "2. **HuggingFace Hub:** A platform where anyone can share and discover models, datasets, and demos. It's like GitHub, but for ML models.\n",
    "3. **Tools & Ecosystem:** A rich set of tools for fine-tuning, deploying, and evaluating models.\n",
    "\n",
    "Many LLMs, including those capable of chat, are available on the HuggingFace Hub. These can range from smaller, open-source models that you can run locally to larger models that might require more significant computational resources.\n",
    "\n",
    "### **Why use HuggingFace Chat Models with LangChain?**\n",
    "\n",
    "While cloud-based LLMs like OpenAI's GPT models or Google's Gemini are powerful, HuggingFace offers distinct advantages:\n",
    "\n",
    "1. **Open Source & Flexibility:** Many models on HuggingFace are open-source, giving you more control, transparency, and the ability to fine-tune them for very specific tasks.\n",
    "2. **Cost-Effectiveness (Potentially):** If you can run models locally or on your own infrastructure, you can potentially reduce API costs associated with commercial LLM providers.\n",
    "3. **Privacy/Security:** For sensitive data, running models locally or on your private cloud can offer better privacy and security controls.\n",
    "4. **Experimentation:** A vast playground for trying out different model architectures and sizes.\n",
    "5. **Community Support:** A very active and helpful community.\n",
    "\n",
    "LangChain acts as a crucial bridge here. It provides a consistent interface (`ChatHuggingFace` class) that allows you to easily integrate models from the HuggingFace ecosystem into your LangChain applications, abstracting away much of the underlying complexity of the transformers library.\n",
    "\n",
    "### **Installation**\n",
    "```python\n",
    "! pip install --upgrade --quiet langchain-huggingface transformers huggingface_hub\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f53c92e-c423-478d-8a4c-befa1dd93c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade --quiet langchain-huggingface transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876a1814-31c9-4941-ade9-038d4de80129",
   "metadata": {},
   "source": [
    "### **Hugging Face Local Pipelines**\n",
    "\n",
    "Hugging Face models can be run locally through the **HuggingFacePipeline** class.\n",
    "\n",
    "When we use the HuggingFacePipeline, it downloads the complete model from the HFHub into our local system and infer it locally.\n",
    "\n",
    "There are two ways in which you can use the HuggingFacePipeline:\n",
    "- **Way 1:** Models can be loaded by specifying the model parameters using the `from_model_id()` method.\n",
    "- **Way 2:** Models can also be loaded by passing in an existing transformers `pipeline()` directly.\n",
    "\n",
    "**Note: HuggingFacePipeline only supports text-generation, text2text-generation, image-text-to-text, summarization and translation for now. (As per 26th June 2025)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "812c8f53-4045-42c4-9c82-4089e57ced62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"What should I study to become a data scientist?\\n\\nI've worked at the Department of Energy, where I've been responsible for research on energy efficiency and climate change for more than 15 years. That's why I have a broad definition of what I do and what I think about.\\n\\nWhat I do is work on the ground floor of the energy transition, where we can deliver a new kind of energy, that will serve as a catalyst for the next generation of clean energy.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Way 1\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"gpt2\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 512},\n",
    ")\n",
    "\n",
    "hf.invoke(\"What should I study to become a data scientist?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4446ce20-8a38-4afc-a36c-b888887a803d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What should I study to become a data scientist?\\n\\nThis is a very difficult question. It is very difficult to define exactly what one should study. It is also very difficult to define what one should study. Many different types of research require a special set of skills, and many different types of research require a wide range of skills.\\n\\nThe main thing is to understand what is expected of the researcher. This is very important in the field of data science. There are many different types of research to address the questions you should be asking.\\n\\nThe types of research that you should study are:\\n\\nHuman Factors\\n\\nAnalytical Methods\\n\\nPhysical Studies\\n\\nData Mining\\n\\nStatistical Methods\\n\\nPsychological Methods\\n\\nA major task of most researchers is to understand how information is stored. The question of how information is stored can be very confusing. The main problem with data is that it cannot always be made available. The best way to understand how information is stored is to understand how data is used in other ways.\\n\\nIf you are studying human factors, the main focus of your research is how data is used, and how data is used in other ways. You should also understand how data is used in other ways. This is called understanding data.\\n\\nWhy should I study data science?\\n\\nData science is a very important area for many people. You should study data science to see what is going on in the world. Here are a few of the reasons that I think data science needs to be a major focus in data science:\\n\\nInformation is often stored in data warehouses. Data warehouses are the source of huge amounts of data, so people are always looking for ways to save and store the information that they have. Data warehouses also allow us to take the data that we have in a data warehouse and reassemble it into something useful.\\n\\ndata warehouses are the source of huge amounts of data, so people are always looking for ways to save and store the information that they have. Data warehouses also allow us to take the data that we have in a data warehouse and reassemble it into something useful. Data is a very powerful resource. Data is a very powerful resource. When you are studying data science, you want to be able to understand the data, and how it was stored and stored in the world that you are studying.\\n\\nData is a very powerful resource. When you are studying data science, you want to be able to understand the data, and how it was stored and stored in the world that you are studying'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Way 2\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "hf = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "hf.invoke(\"What should I study to become a data scientist?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f6db42-9d4f-43be-ad70-088ca8197e35",
   "metadata": {},
   "source": [
    "### **Huggingface Endpoints (COMING SOON)**\n",
    "\n",
    "This works with any model that supports text generation (i.e. text completion) task. \n",
    "\n",
    "To use this class, do the following first:\n",
    "1. Install the `huggingface_hub` package using this command: `! pip install huggingface_hub`\n",
    "2. The environment variable `HUGGINGFACEHUB_API_TOKEN` set with your API token, or given as a named parameter to the constructor.\n",
    "\n",
    "#### **Understanding the Inference Providers**\n",
    "Hugging Face’s Inference Providers give developers streamlined, unified access to hundreds of machine learning models, powered by our serverless inference partners.\n",
    "[Click here](https://huggingface.co/settings/inference-providers) to get the list of all the inference providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb1e205-5396-48ca-8a36-06ad892c928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup API Key\n",
    "\n",
    "# f = open('keys/.hf_api_key.txt')\n",
    "\n",
    "# HF_API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fc67ed5-5f11-4561-b62d-73439ae40a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_API_KEY\n",
    "# os.environ[\"HF_TOKEN\"] = HF_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2d4949-40b1-4805-8caa-5e6caffed099",
   "metadata": {},
   "source": [
    "Here is an example of how you can access HuggingFaceEndpoint integration of the free Serverless Endpoints API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02974041-d4cd-48e4-8f7b-83e01f801b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "# repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id=repo_id,\n",
    "#     max_length=128,\n",
    "#     temperature=0.5,\n",
    "#     huggingfacehub_api_token=HF_API_KEY,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95a3eece-87f8-4a61-9221-6f55177ad8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm.invoke(\"What did foo say about bar?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64dd451d-82d7-4cc2-a1ba-c04b0e15293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "# repo_id = \"deepseek-ai/DeepSeek-R1\"\n",
    "# inference_provider = \"nebius\"\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id=repo_id,\n",
    "#     provider=inference_provider,\n",
    "#     max_length=128,\n",
    "#     temperature=0.5,\n",
    "#     task=\"conversational\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3549ab14-5b8e-4480-adc7-8da2263591b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_huggingface import ChatHuggingFace\n",
    "\n",
    "# chat_llm = ChatHuggingFace(\n",
    "#     llm=llm,\n",
    "#     repo_id=repo_id,\n",
    "#     provider=inference_provider,\n",
    "#     max_length=128,\n",
    "#     temperature=0.5,\n",
    "#     task=\"conversational\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d84fb98-7cf8-4c37-ae34-8ff56762e6ed",
   "metadata": {},
   "source": [
    "### **ChatHuggingFace (COMING SOON)**\n",
    "\n",
    "Works with HuggingFaceTextGenInference, HuggingFaceEndpoint, HuggingFaceHub, and HuggingFacePipeline LLMs.\n",
    "\n",
    "Upon instantiating this class, the model_id is resolved from the url provided to the LLM, and the appropriate tokenizer is loaded from the HuggingFace Hub.\n",
    "\n",
    "#### **Setup**\n",
    "\n",
    "```python\n",
    "from huggingface_hub import login\n",
    "login() # You will be prompted for your HF key, which will then be saved locally\n",
    "```\n",
    "\n",
    "#### **ChatHuggingFace() Args**\n",
    "- llm: HuggingFaceTextGenInference, HuggingFaceEndpoint, HuggingFaceHub, or HuggingFacePipeline LLM to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdce213b-a9a3-43c9-ab1e-14295a4b1c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08ac9655-e06d-4e68-8945-aeeb98b5df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_huggingface import ChatHuggingFace\n",
    "\n",
    "# chat_llm = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325bd17c-fae2-4b16-98c7-55c2ec3fc950",
   "metadata": {},
   "source": [
    "## **LangChain New Unified Chat Model API**\n",
    "\n",
    "LangChain’s standard model interfaces give you access to many different provider integrations, which makes it easy to experiment with and switch between models to find the best fit for your use case.\n",
    "\n",
    "### **Basic usage**\n",
    "Models can be utilized in two ways:\n",
    "1. Standalone - Models can be called directly for tasks like text generation, classification, or extraction without the need for an agent framework.\n",
    "2. With agents - Models can be dynamically specified when creating an agent.\n",
    "\n",
    "### **Initialize a model**\n",
    "The easiest way to get started with a standalone model in LangChain is to use init_chat_model to initialize one from a chat model provider of your choice.\n",
    "\n",
    "**Syntax:**\n",
    "```python\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-4o-mini\")\n",
    "```\n",
    "\n",
    "### **Parameters**\n",
    "\n",
    "A chat model takes parameters that can be used to configure its behavior. The full set of supported parameters varies by model and provider, but standard ones include:\n",
    "1. **model** - The name or identifier of the specific model you want to use with a provider. You can also specify both the model and its provider in a single argument using the '{model_provider}:{model}' format, for example, ‘openai:o1’. Will attempt to infer model_provider from model if not specified. The following providers will be inferred based on these model prefixes:\n",
    "    - gpt-... | o1... | o3... -> openai\n",
    "    - claude... -> anthropic\n",
    "    - amazon... -> bedrock\n",
    "    - gemini... -> google_vertexai\n",
    "    - command... -> cohere\n",
    "    - accounts/fireworks... -> fireworks\n",
    "    - mistral... -> mistralai\n",
    "    - deepseek... -> deepseek\n",
    "    - grok... -> xai\n",
    "    - sonar... -> perplexity\n",
    "2. **model_provider** - The model provider if not specified as part of the model arg (see above). Supported model_provider values and the corresponding integration package are:\n",
    "    - openai -> langchain-openai\n",
    "    - anthropic -> langchain-anthropic\n",
    "    - azure_openai -> langchain-openai\n",
    "    - azure_ai -> langchain-azure-ai\n",
    "    - google_vertexai -> langchain-google-vertexai\n",
    "    - google_genai -> langchain-google-genai\n",
    "    - bedrock -> langchain-aws\n",
    "    - bedrock_converse -> langchain-aws\n",
    "    - cohere -> langchain-cohere\n",
    "    - fireworks -> langchain-fireworks\n",
    "    - together -> langchain-together\n",
    "    - mistralai -> langchain-mistralai\n",
    "    - huggingface -> langchain-huggingface\n",
    "    - groq -> langchain-groq\n",
    "    - ollama -> langchain-ollama\n",
    "    - google_anthropic_vertex -> langchain-google-vertexai\n",
    "    - deepseek -> langchain-deepseek\n",
    "    - ibm -> langchain-ibm\n",
    "    - nvidia -> langchain-nvidia-ai-endpoints\n",
    "    - xai -> langchain-xai\n",
    "    - perplexity -> langchain-perplexity\n",
    "\n",
    "3. **api_key** - The key required for authenticating with the model’s provider. This is usually issued when you sign up for access to the model.\n",
    "4.  ****kwargs** - Additional model-specific keyword args to pass to the underlying chat model's __init__ method. Common parameters include:\n",
    "    - temperature: Model temperature for controlling randomness.\n",
    "    - max_tokens: Maximum number of output tokens.\n",
    "    - timeout: Maximum time (in seconds) to wait for a response.\n",
    "    - max_retries: Maximum number of retry attempts for failed requests.\n",
    "    - base_url: Custom API endpoint URL.\n",
    "    - rate_limiter: A BaseRateLimiter instance to control request rate.\n",
    "\n",
    "**Syntax:**\n",
    "```python\n",
    "model = init_chat_model(\n",
    "    \"claude-sonnet-4-5-20250929\",\n",
    "    # Kwargs passed to the model:\n",
    "    temperature=0.7,\n",
    "    timeout=30,\n",
    "    max_tokens=1000,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d64e167-2cee-40e9-88c7-ea6962610ed4",
   "metadata": {},
   "source": [
    "### **Example Usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62494e9a-eb3b-4ac4-bf1b-06a4a229b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(model=\"google_genai:gemini-2.0-flash\", \n",
    "                        api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2ecc43e-b821-4a85-b533-6b3f6178cd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a polite assistant.\"),\n",
    "    HumanMessage(content=\"Hello!\"),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
