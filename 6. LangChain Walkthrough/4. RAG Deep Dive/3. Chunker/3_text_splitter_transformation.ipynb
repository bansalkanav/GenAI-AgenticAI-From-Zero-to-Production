{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d74d45c5-d056-4d93-99d8-bbc66b3cf1fe",
   "metadata": {},
   "source": [
    "# **Chunking Strategies**\n",
    "\n",
    "## **What's Covered?**\n",
    "1. Why Chunking Matters?\n",
    "2. What is Chunking?\n",
    "3. Visualize Chunking\n",
    "4. How Chunking Works?\n",
    "5. Choosing Chunk Size\n",
    "6. Types of Text Splitters\n",
    "7. Length based splitting - CharacterTextSplitter\n",
    "8. Recursively Character Splitting\n",
    "9. Recursive Character Splitting with tiktoken\n",
    "10. Document Structure-Based Splitting\n",
    "    - MarkdownHeaderTextSplitter\n",
    "    - HTMLHeaderTextSplitter\n",
    "    - RecursiveJsonSplitter\n",
    "    - Code Splitter\n",
    "11. Passing Metadata with Chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b355560-7854-4da4-8340-74e406194393",
   "metadata": {},
   "source": [
    "## **Why Chunking Matters?**\n",
    "Notice that after you load a Documnet object from a source, you end up with strings by grabbing them from page_content. In certain situations, the length of the strings may be too large to feed into a model, both embedding and chat model.\n",
    "\n",
    "**Note: LLMs have fixed maximum context window. If you document contains more token then the maximum context window size, LLMs won't be able to process it. So, it is important to break the documents into chunks.**\n",
    "\n",
    "In short:\n",
    "- Good chunks â†’ high retrieval accuracy\n",
    "- Bad chunks â†’ LLM hallucinations + incomplete answers\n",
    "- Chunking is 50% of RAG performance.\n",
    "\n",
    "## **What is Chunking?**\n",
    "Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example is you may want to split a long document into smaller chunks that can fit into your model's context window. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.\n",
    "\n",
    "When you want to deal with long pieces of text, it is necessary to split up that text into chunks. As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What \"semantically related\" means could depend on the type of text. This notebook showcases several ways to do that.\n",
    "\n",
    "**Note: Think of it like organizing a massive book into chapters and sections. When someone asks a question, you only retrieve the relevant chapters, not the entire book, making retrieval faster and more accurate.**\n",
    "\n",
    "A good chunk has:\n",
    "- one clear idea/theme (not 5 unrelated things)\n",
    "- long enough to contain useful info\n",
    "- not too long for embedding models\n",
    "- fits the retrieval goal (FAQs vs Legal Docs vs Code need different chunk sizes)\n",
    "\n",
    "## **Visualize Chunking**  \n",
    "You can evaluate text splitters with the [Chunkviz utility](https://www.chunkviz.com/) created by Greg Kamradt. Chunkviz is a great tool for visualizing how your text splitter is working. It will show you how your text is being split up and help in tuning up the splitting parameters.\n",
    "\n",
    "## **How Chunking Works?**\n",
    "1. Split the text up into small, semantically meaningful chunks (often sentences).\n",
    "2. Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).\n",
    "3. Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).\n",
    "\n",
    "That means there are two different axes along which you can customize your text splitter:\n",
    "\n",
    "1. How the text is split\n",
    "2. How the chunk size is measured\n",
    "\n",
    "## **Choosing Chunk Size**\n",
    "- **Small chunks (250-500 tokens):** Better for specific factual questions, reduces noise in retrieval\n",
    "- **Medium chunks (500-1000 tokens):** Balanced approach, good for general Q&A\n",
    "- **Large chunks (1000+ tokens):** Preserves more context, better for complex reasoning but retrieves more text\n",
    "\n",
    "Pro tip: Start with **chunk_size=1000**, **chunk_overlap=200** and adjust if retrieval quality is poor.\n",
    "\n",
    "- chunk_size=1000: Target chunk size in characters. Adjust based on your model's context window\n",
    "- chunk_overlap=200: Characters to overlap between consecutive chunks. Prevents context loss at chunk boundaries\n",
    "\n",
    "## **Types of Text Splitters**\n",
    "LangChain offers many different types of text splitters. These all live in the `langchain-text-splitters` package. Below is a table listing all of them, along with a few characteristics:\n",
    "> **Name:** Name of the text splitter  \n",
    "> **Splits On:** How this text splitter splits text  \n",
    "> **Adds Metadata:** Whether or not this text splitter adds metadata about where each chunk came from.  \n",
    "> **Description:** Description of the splitter, including recommendation on when to use it.\n",
    "\n",
    "#### **1. Length Based Splitting**\n",
    "- CharacterTextSplitter()\n",
    "\n",
    "#### **2. Recursive Character Splitting**\n",
    "- RecursiveCharacterTextSplitter()\n",
    "- RecursiveCharacterTextSplitter.from_tiktoken_encoder()\n",
    "\n",
    "#### **3. Document Structure-Based Splitting**\n",
    "- Markdown Splitting()\n",
    "- HTML Splitting()\n",
    "- JSON Splitting()\n",
    "- Code Splitting()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c7f3c6-7c0d-4103-a6ef-7af1c16a2515",
   "metadata": {},
   "source": [
    "## **Length based splitting - CharacterTextSplitter**\n",
    "\n",
    "**When to use? - Use when you need simple, consistent chunk sizes.**\n",
    "\n",
    "This is the simplest method. This splits based on characters (by default \"\") and measure chunk length by number of characters.\n",
    "\n",
    "Splits documents based purely on lengthâ€”either characters or tokens. This is straightforward but doesn't respect document structure, so it can split sentences or ideas mid-way.\n",
    "\n",
    "1. **How the text is split:** by single character.\n",
    "2. **How the chunk size is measured:** by number of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ff841d-dcf6-4f19-b401-db61d939276a",
   "metadata": {},
   "source": [
    "### **Loading the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "eb23470b-1ba7-497c-aeb1-411128b8a73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is pretty much\n",
      "what's happened so far.\n",
      "\n",
      "Ross was in love\n",
      "with Rachel since forever.\n",
      "\n",
      "Every time he tried to tell her,\n",
      "something got in the way\n",
      "\n",
      "Iike cats, Italian guys.\n",
      "\n",
      "And finally, Chandler was,\n",
      "like, \"Forget about her.\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"data/misc_files/text_file.txt\")\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "doc_content = data[0].page_content\n",
    "\n",
    "print(doc_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3426c7d3-7b0b-4000-a535-f0eb24306073",
   "metadata": {},
   "source": [
    "### **Applying the CharacterText Splitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cf304c1a-9477-4eaf-ab4f-c5e5a9023339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of texts variable: <class 'list'>\n",
      "\n",
      "Type of each object inside the list: <class 'str'>\n",
      "\n",
      "Total number of documents inside texts list: 3\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(doc_content)\n",
    "\n",
    "print(\"Type of texts variable:\", type(chunks))\n",
    "print()\n",
    "print(\"Type of each object inside the list:\", type(chunks[0]))\n",
    "print()\n",
    "print(\"Total number of documents inside texts list:\", len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b2c9502a-7024-491f-93f2-f6e78ad97bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Content of chunk:\n",
      "This is pretty much\n",
      "what's happened so far.\n",
      "\n",
      "Ross was in love\n",
      "with Rachel since forever.\n"
     ]
    }
   ],
   "source": [
    "print(\"* Content of chunk:\")\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3be3bfa4-cfaa-4ae2-b901-ae73b972d2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Size:  88\n",
      "Chunk Size:  83\n",
      "Chunk Size:  52\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    print(\"Chunk Size: \", len(chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976cd4f6-d44f-4272-9aba-9697b0880cd2",
   "metadata": {},
   "source": [
    "## **Recursively Character Splitting**\n",
    "\n",
    "**When to use? - Use this for most casesâ€”it's the default strategy that works well out of the box.**\n",
    "\n",
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is `[\"\\n\\n\", \"\\n\", \" \", \"\"]`. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "**Note: The RecursiveCharacterTextSplitter intelligently splits text while preserving meaning. It works hierarchically: first tries to split at paragraphs, then sentences, then words, until chunks fit the size limit. This maintains natural language flow and semantic coherence.**\n",
    "\n",
    "1. **How the text is split:** by list of characters.\n",
    "2. **How the chunk size is measured:** by number of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b84f0c-7903-4391-987a-659b4e288e95",
   "metadata": {},
   "source": [
    "### **Loading the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f9925ab4-e903-4d90-997a-4b69020ab980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 4146.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of Data Variable:  <class 'list'>\n",
      "Number of Documents: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader('data/subtitles', glob=\"*.srt\", show_progress=True, loader_cls=TextLoader)\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "print(\"Type of Data Variable: \", type(data))\n",
    "\n",
    "print(\"Number of Documents:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c3e8a534-9f27-4bef-97ed-31127371bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_contents = [doc.page_content for doc in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf9dcab-e759-48f6-9277-fc2c9836bc28",
   "metadata": {},
   "source": [
    "### **Applying RecursiveCharacterTextSplitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e6ba0ce5-22e1-443f-a4b4-f2d8976a924f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of variable: <class 'list'>\n",
      "\n",
      "Type of each object inside the list: <class 'langchain_core.documents.base.Document'>\n",
      "\n",
      "Total number of documents inside list: 514\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.create_documents(doc_contents)\n",
    "\n",
    "print(\"Type of variable:\", type(chunks))\n",
    "print()\n",
    "print(\"Type of each object inside the list:\", type(chunks[0]))\n",
    "print()\n",
    "print(\"Total number of documents inside list:\", len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0021226c-f2bd-40d4-8639-e61fd2260fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Content of first chunk:\n",
      "1\n",
      "00:00:01,435 --> 00:00:04,082\n",
      "This is pretty much\n",
      "what's happened so far.\n",
      "\n",
      "2\n",
      "00:00:04,395 --> 00:00:07,179\n",
      "Ross was in love\n",
      "with Rachel since forever.\n",
      "\n",
      "3\n",
      "00:00:07,423 --> 00:00:10,437\n",
      "Every time he tried to tell her,\n",
      "something got in the way...\n",
      "\n",
      "4\n",
      "00:00:10,651 --> 00:00:12,529\n",
      "...Iike cats, Italian guys.\n",
      "\n",
      "5\n",
      "00:00:12,736 --> 00:00:15,922\n",
      "And finally, Chandler was,\n",
      "like, \"Forget about her.\"\n",
      "\n",
      "6\n",
      "00:00:16,166 --> 00:00:20,762\n",
      "When Ross was in China, Chandler\n",
      "let it slip that Ross loved Rachel.\n"
     ]
    }
   ],
   "source": [
    "print(\"* Content of first chunk:\")\n",
    "print(chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c8694a0f-9b4c-4457-9907-8e9c206d47f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Chunk Size:  459.02334630350197\n"
     ]
    }
   ],
   "source": [
    "sum_of_chunk_size = 0\n",
    "for chunk in chunks:\n",
    "    sum_of_chunk_size += len(chunk.page_content)\n",
    "\n",
    "print(\"Average Chunk Size: \", sum_of_chunk_size/len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3da0d3-0f93-4371-9c7a-b249c2c18d49",
   "metadata": {},
   "source": [
    "## **Recursive Character Splitting with tiktoken**\n",
    "\n",
    "**When to use? - More precise for LLM context windows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9fcd015-ce15-4d81-a1d6-f648b672f219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d3167ee4-bcc6-4e39-9577-ceb6b3ea271d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of variable: <class 'list'>\n",
      "\n",
      "Type of each object inside the list: <class 'langchain_core.documents.base.Document'>\n",
      "\n",
      "Total number of documents inside list: 1161\n"
     ]
    }
   ],
   "source": [
    "# tiktoken is a fast BPE tokenizer created by OpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "                        encoding_name=\"cl100k_base\",      # OpenAI encoding\n",
    "                        chunk_size=100,                   # represents tokens, not characters\n",
    "                        chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = text_splitter.create_documents(doc_contents)\n",
    "\n",
    "print(\"Type of variable:\", type(chunks))\n",
    "print()\n",
    "print(\"Type of each object inside the list:\", type(chunks[0]))\n",
    "print()\n",
    "print(\"Total number of documents inside list:\", len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "be4e96f1-8bb3-4fb2-b3fe-3852e2ac538f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Content of first chunk:\n",
      "1\n",
      "00:00:01,435 --> 00:00:04,082\n",
      "This is pretty much\n",
      "what's happened so far.\n",
      "\n",
      "2\n",
      "00:00:04,395 --> 00:00:07,179\n",
      "Ross was in love\n",
      "with Rachel since forever.\n",
      "\n",
      "3\n",
      "00:00:07,423 --> 00:00:10,437\n",
      "Every time he tried to tell her,\n",
      "something got in the way...\n"
     ]
    }
   ],
   "source": [
    "print(\"* Content of first chunk:\")\n",
    "print(chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74f2475-6134-49fd-9257-080bbf4c4a47",
   "metadata": {},
   "source": [
    "## **4. Document Structure-Based Splitting**\n",
    "Use when documents have inherent structure (Markdown, HTML, JSON, code).\n",
    "\n",
    "These strategies leverage document structure to keep semantically related content together.\n",
    "\n",
    "### **4.1 MarkdownHeaderTextSplitter**\n",
    "\n",
    "For structured docs in Markdown. This preserves the document's hierarchical structure, so each chunk knows its context (e.g., \"Section 2.1 under Chapter 3\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8a55434c-b6c7-4a01-943b-201f968580f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"data/misc_files/langchain_chunking.md\")\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "markdown_text = data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "414dd91f-8d2d-404a-9c83-2c6760cd5b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of variable: <class 'list'>\n",
      "\n",
      "Type of each object inside the list: <class 'langchain_core.documents.base.Document'>\n",
      "\n",
      "Total number of documents inside list: 5\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "md_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=[\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chunks = md_splitter.split_text(markdown_text)\n",
    "\n",
    "print(\"Type of variable:\", type(chunks))\n",
    "print()\n",
    "print(\"Type of each object inside the list:\", type(chunks[0]))\n",
    "print()\n",
    "print(\"Total number of documents inside list:\", len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a20b7972-7dc9-430b-b7b5-2cf22c8fa685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Content of first chunk:\n",
      "Notice that after you load a Documnet object from a source, you end up with strings by grabbing them from page_content. In certain situations, the length of the strings may be too large to feed into a model, both embedding and chat model.  \n",
      "**Note: LLMs have fixed maximum context window. If you document contains more token then the maximum context window size, LLMs won't be able to process it. So, it is important to break the documents into chunks.**  \n",
      "In short:\n",
      "- Good chunks â†’ high retrieval accuracy\n",
      "- Bad chunks â†’ LLM hallucinations + incomplete answers\n",
      "- Chunking is 50% of RAG performance.\n"
     ]
    }
   ],
   "source": [
    "print(\"* Content of first chunk:\")\n",
    "print(chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "067dce69-b692-41aa-8377-1da14a91a73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Size:  599\n",
      "Chunk Size:  953\n",
      "Chunk Size:  284\n",
      "Chunk Size:  519\n",
      "Chunk Size:  1758\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    print(\"Chunk Size: \", len(chunk.page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddd35ee-15b2-40a5-8503-067373ac275a",
   "metadata": {},
   "source": [
    "### **4.2. HTMLHeaderTextSplitter**\n",
    "\n",
    "This is great for websites.\n",
    "\n",
    "Handles:\n",
    "- `<h1>`\n",
    "- `<h2>`\n",
    "- `<h3>`\n",
    "- `paragraphs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "736e3a6d-a847-4547-a232-06d43404c296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"data/misc_files/web_page.html\")\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "doc_content = data[0].page_content\n",
    "\n",
    "# print(doc_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "917cd04f-5936-4486-8c71-ff0940b42f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_string = data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2375aa4f-9d8a-4e6f-9078-883ad6151e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of variable: <class 'list'>\n",
      "\n",
      "Type of each object inside the list: <class 'langchain_core.documents.base.Document'>\n",
      "\n",
      "Total number of documents inside list: 12\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(\n",
    "    headers_to_split_on=[\n",
    "        (\"h1\", \"Header 1\"),\n",
    "        (\"h2\", \"Header 2\"),\n",
    "        (\"h3\", \"Header 3\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chunks = html_splitter.split_text(html_string)\n",
    "\n",
    "print(\"Type of variable:\", type(chunks))\n",
    "print()\n",
    "print(\"Type of each object inside the list:\", type(chunks[0]))\n",
    "print()\n",
    "print(\"Total number of documents inside list:\", len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5e5c967f-4d58-4942-a723-5b63e7beda58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Content of a chunk:\n",
      "ThatAIGuy's Avatar  \n",
      "Hello! I'm ThatAIGuy, a developer obsessed with **Artificial Intelligence** and **Machine Learning**. My passion lies in creating tools that simplify complex tasks using cutting-edge models.  \n",
      "Click for a quick quote...  \n",
      "\"The best way to predict the future is to invent it.\"  \n",
      "â€”  \n",
      "ThatAIGuy\n",
      "{'Header 1': 'ThatAIGuy: The Future is Now', 'Header 2': 'ðŸ¤– About ThatAIGuy'}\n"
     ]
    }
   ],
   "source": [
    "print(\"* Content of a chunk:\")\n",
    "print(chunks[3].page_content)\n",
    "print(chunks[3].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb7c0d1-73fc-43a9-ad1a-97ea54b570d3",
   "metadata": {},
   "source": [
    "### **4.3. RecursiveJsonSplitter**\n",
    "\n",
    "Splits JSON data into smaller, structured chunks while preserving hierarchy.\n",
    "\n",
    "This class provides methods to split JSON data into smaller dictionaries or JSON-formatted strings based on configurable maximum and minimum chunk sizes. It supports nested JSON structures, optionally converts lists into dictionaries for better chunking, and allows the creation of document objects for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ef63c263-a397-439a-95cb-f9a26f23e9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"data/misc_files/product_list.json\")\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "doc_content = data[0].page_content\n",
    "\n",
    "# print(doc_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dcdbdf-f080-4111-af44-5de928a18ac5",
   "metadata": {},
   "source": [
    "### **Applying RecursiveJsonSplitter**\n",
    "\n",
    "- **.split_json():** Expects a single JSON object (dict) - This function splits JSON into a list of JSON chunks\n",
    "- **.create_documents():** Expects a list of JSON objects - This function splits list of JSON into a list of Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ab3c9a61-179b-409f-acbf-e62aa4b6f2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "{'product_id': 'ELC001', 'name': 'Noise-Cancelling Over-Ear Headphones', 'brand': 'AcoustiVox', 'category': 'Electronics', 'sku': 'AV-HP-NC2024', 'description': 'Premium over-ear headphones with industry-leading noise cancellation and 40-hour battery life. Perfect for travel and focus.', 'price_data': {'base_price': 249.99, 'currency': 'USD', 'is_on_sale': True, 'sale_price': 199.99}, 'inventory': {'stock_level': 350, 'is_available': True}, 'attributes': [{'key': 'Color', 'value': 'Midnight Black'}, {'key': 'Connection', 'value': 'Bluetooth 5.2'}], 'reviews': [{'user_id': 101, 'rating': 5, 'comment': 'Incredible sound quality and comfort!'}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "prod_json = json.loads(doc_content)\n",
    "\n",
    "print(type(prod_json))\n",
    "\n",
    "print(prod_json[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "98b627ea-2e18-4acc-9793-907d3bdf3591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of variable: <class 'list'>\n",
      "\n",
      "Type of each object inside the list: <class 'langchain_core.documents.base.Document'>\n",
      "\n",
      "Total number of documents inside list: 28\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "\n",
    "json_splitter = RecursiveJsonSplitter(\n",
    "    max_chunk_size=300\n",
    ")\n",
    "\n",
    "chunks = json_splitter.create_documents(prod_json)\n",
    "\n",
    "print(\"Type of variable:\", type(chunks))\n",
    "print()\n",
    "print(\"Type of each object inside the list:\", type(chunks[0]))\n",
    "print()\n",
    "print(\"Total number of documents inside list:\", len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "92e9cc79-7f13-4408-afa4-28bffb1ec24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Content of a chunk:\n",
      "{\"product_id\": \"FSH005\", \"name\": \"Organic Cotton Crewneck T-Shirt\", \"brand\": \"EcoThread\", \"category\": \"Apparel\", \"sku\": \"ET-TS-CRC005\", \"description\": \"Soft, breathable, and sustainably sourced cotton t-shirt. Available in multiple sizes.\"}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(\"* Content of a chunk:\")\n",
    "print(chunks[3].page_content)\n",
    "print(chunks[3].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcc5e64-cca0-460a-9ee3-43f45f29c17a",
   "metadata": {},
   "source": [
    "### **4.4. Code Splitter**\n",
    "\n",
    "For code, uses language-specific delimiters (functions, classes, etc.) instead of generic newlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9cd10699-f862-4ad5-9c6f-aacfbd328e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def fun(x):\n",
      "    print(f\"Input: {x}\")\n",
      "    return x**2\n",
      "\n",
      "def main():\n",
      "    fun(3)\n",
      "\n",
      "main()\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"data/misc_files/code.py\")\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "doc_content = data[0].page_content\n",
    "\n",
    "print(doc_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f8722930-af3e-4ac3-8ce5-5e38b6c4f2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of variable: <class 'list'>\n",
      "\n",
      "Type of each object inside the list: <class 'str'>\n",
      "\n",
      "Total number of documents inside list: 3\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import Language, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Split code by language-specific delimiters\n",
    "code_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "chunks = code_splitter.split_text(doc_content)\n",
    "\n",
    "print(\"Type of variable:\", type(chunks))\n",
    "print()\n",
    "print(\"Type of each object inside the list:\", type(chunks[0]))\n",
    "print()\n",
    "print(\"Total number of documents inside list:\", len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "fcdedcba-ac05-4aac-b41f-466252d10ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Content of a chunk:\n",
      "def fun(x):\n",
      "    print(f\"Input: {x}\")\n"
     ]
    }
   ],
   "source": [
    "print(\"* Content of a chunk:\")\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b589f0c-8ef8-4419-9c7b-f21159812365",
   "metadata": {},
   "source": [
    "## **Passing Metadata with Chunks**\n",
    "\n",
    "If there are more than one document, you should add some metadata with each chunk to identify which document it belongs to. \n",
    "\n",
    "Hereâ€™s an example of passing metadata along with the documents, notice that it is split along with the documents.\n",
    "\n",
    "We will use **create_documents()** function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bd1f114-b536-43ca-8966-53476ca7ce68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 2397.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of Data Variable:  <class 'list'>\n",
      "Number of Documents: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader('data/text', glob=\"*.txt\", show_progress=True, loader_cls=TextLoader)\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "print(\"Type of Data Variable: \", type(data))\n",
    "\n",
    "print(\"Number of Documents:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82abe001-dac1-44a5-ac57-fa0239045135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of variable: <class 'list'>\n",
      "\n",
      "Type of each object inside the list: <class 'langchain_core.documents.base.Document'>\n",
      "\n",
      "Total number of documents inside chunks: 3\n",
      "\n",
      "Content of chunks: [Document(metadata={'source': 'data/text/file_1.txt'}, page_content=\"This is pretty much\\nwhat's happened so far.\\n\\nRoss was in love\\nwith Rachel since forever.\\n\\nEvery time he tried to tell her,\\nsomething got in the way\\n\\nIike cats, Italian guys.\"), Document(metadata={'source': 'data/text/file_1.txt'}, page_content='Iike cats, Italian guys.\\n\\nAnd finally, Chandler was,\\nlike, \"Forget about her.\"'), Document(metadata={'source': 'data/text/file_2.txt'}, page_content=\"These were unbelievely expensive\\nand he'll grow out of them\\n\\nin 20 minutes,\\nbut I couldn't resist!\\n\\nLook at these.\")]\n"
     ]
    }
   ],
   "source": [
    "# metadatas = [{\"document\": 1}, {\"document\": 2}]\n",
    "doc_contents = [doc.page_content for doc in data]\n",
    "meta_datas = [doc.metadata for doc in data]\n",
    "\n",
    "chunks = text_splitter.create_documents(doc_contents, meta_datas)\n",
    "\n",
    "print(\"Type of variable:\", type(chunks))\n",
    "print()\n",
    "print(\"Type of each object inside the list:\", type(chunks[0]))\n",
    "print()\n",
    "print(\"Total number of documents inside chunks:\", len(chunks))\n",
    "print()\n",
    "print(\"Content of chunks:\", chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
