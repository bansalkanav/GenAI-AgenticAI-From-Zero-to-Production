{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1966cd9-09a3-4335-a06f-65b469aec4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/langchain-ai/langchain/issues/29153"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eccaccc5-54db-43f8-b040-590ce2125928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "\n",
    "\n",
    "input_data = {\n",
    "  \"projects\": {\n",
    "    \"AS\": {\n",
    "      \"AS-1\": {}\n",
    "    },\n",
    "    \"DLP\": {\n",
    "      \"DLP-7\": {},\n",
    "      \"DLP-6\": {},\n",
    "      \"DLP-5\": {},\n",
    "      \"DLP-4\": {},\n",
    "      \"DLP-3\": {},\n",
    "      \"DLP-2\": {},\n",
    "      \"DLP-1\": {}\n",
    "    },\n",
    "    \"GTMS\": {\n",
    "      \"GTMS-22\": {},\n",
    "      \"GTMS-21\": {},\n",
    "      \"GTMS-20\": {},\n",
    "      \"GTMS-19\": {},\n",
    "      \"GTMS-18\": {},\n",
    "      \"GTMS-17\": {},\n",
    "      \"GTMS-16\": {},\n",
    "      \"GTMS-15\": {},\n",
    "      \"GTMS-14\": {},\n",
    "      \"GTMS-13\": {},\n",
    "      \"GTMS-12\": {},\n",
    "      \"GTMS-11\": {},\n",
    "      \"GTMS-10\": {},\n",
    "      \"GTMS-9\": {},\n",
    "      \"GTMS-8\": {},\n",
    "      \"GTMS-7\": {},\n",
    "      \"GTMS-6\": {},\n",
    "      \"GTMS-5\": {},\n",
    "      \"GTMS-4\": {},\n",
    "      \"GTMS-3\": {},\n",
    "      \"GTMS-2\": {},\n",
    "      \"GTMS-1\": {}\n",
    "    },\n",
    "    \"IT\": {\n",
    "      \"IT-3\": {},\n",
    "      \"IT-2\": {},\n",
    "      \"IT-1\": {}\n",
    "    },\n",
    "    \"ITSAMPLE\": {\n",
    "      \"ITSAMPLE-12\": {},\n",
    "      \"ITSAMPLE-11\": {},\n",
    "      \"ITSAMPLE-10\": {},\n",
    "      \"ITSAMPLE-9\": {},\n",
    "      \"ITSAMPLE-8\": {},\n",
    "      \"ITSAMPLE-7\": {},\n",
    "      \"ITSAMPLE-6\": {},\n",
    "      \"ITSAMPLE-5\": {},\n",
    "      \"ITSAMPLE-4\": {},\n",
    "      \"ITSAMPLE-3\": {},\n",
    "      \"ITSAMPLE-2\": {},\n",
    "      \"ITSAMPLE-1\": {}\n",
    "    },\n",
    "    \"MAR\": {\n",
    "      \"MAR-2\": {},\n",
    "      \"MAR-1\": {}\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76d97e7b-354c-48ef-9cae-ef96a1f422b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULTS: 299\n"
     ]
    }
   ],
   "source": [
    "for i in range(299, 300):\n",
    "    splitter = RecursiveJsonSplitter(max_chunk_size=i)\n",
    "    json_chunks = splitter.split_json(json_data=input_data)\n",
    "    \n",
    "    input_data_DLP_5 = input_data.get(\"projects\", {}).get(\"DLP\", {}).get(\"DLP-5\", None)\n",
    "    input_data_GTMS_10 = input_data.get(\"projects\", {}).get(\"GTMS\", {}).get(\"GTMS-10\", None)\n",
    "    input_data_ITSAMPLE_2 = input_data.get(\"projects\", {}).get(\"ITSAMPLE\", {}).get(\"ITSAMPLE-2\", None)\n",
    "    \n",
    "    chunk_DLP_5 = None\n",
    "    chunk_GTMS_10 = None\n",
    "    chunk_ITSAMPLE_2 = None\n",
    "    \n",
    "    for chunk in json_chunks:\n",
    "        # print(chunk)\n",
    "        node = chunk.get(\"projects\", {}).get(\"DLP\", {}).get(\"DLP-5\", None)\n",
    "        if isinstance(node, dict):\n",
    "            chunk_DLP_5 = node\n",
    "        node = chunk.get(\"projects\", {}).get(\"GTMS\", {}).get(\"GTMS-10\", None)\n",
    "        if isinstance(node, dict):\n",
    "            chunk_GTMS_10 = node\n",
    "        node = chunk.get(\"projects\", {}).get(\"ITSAMPLE\", {}).get(\"ITSAMPLE-2\", None)\n",
    "        if isinstance(node, dict):\n",
    "            chunk_ITSAMPLE_2 = node\n",
    "    \n",
    "    print(f\"\\nRESULTS: {i}\")\n",
    "    if isinstance(chunk_DLP_5, dict):\n",
    "        # print(f\"[PASS] - Node DLP-5 was found both in input_data and json_chunks\")\n",
    "        pass\n",
    "    else:\n",
    "        print(f\"[TEST FAILED] - Node DLP-5 from input_data was NOT FOUND in json_chunks\")\n",
    "    \n",
    "    if isinstance(chunk_GTMS_10, dict):\n",
    "        # print(f\"[PASS] - Node GTMS-10 was found both in input_data and json_chunks\")\n",
    "        pass\n",
    "    else:\n",
    "        print(f\"[TEST FAILED] - Node GTMS-10 from input_data was NOT FOUND in json_chunks\")\n",
    "    \n",
    "    if isinstance(chunk_ITSAMPLE_2, dict):\n",
    "        # print(f\"[PASS] - Node ITSAMPLE-2 was found both in input_data and json_chunks\")\n",
    "        pass\n",
    "    else:\n",
    "        print(f\"[TEST FAILED] - Node ITSAMPLE-2 from input_data was NOT FOUND in json_chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f4bef8d-f513-4159-a031-916c0bd9b14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "{'projects': {'AS': {'AS-1': {}}, 'DLP': {'DLP-7': {}, 'DLP-6': {}, 'DLP-5': {}, 'DLP-4': {}, 'DLP-3': {}, 'DLP-2': {}, 'DLP-1': {}}}}\n",
      "\n",
      "********************\n",
      "{'projects': {'GTMS': {'GTMS-22': {}, 'GTMS-21': {}, 'GTMS-20': {}, 'GTMS-19': {}, 'GTMS-18': {}, 'GTMS-17': {}, 'GTMS-16': {}, 'GTMS-15': {}, 'GTMS-14': {}, 'GTMS-13': {}, 'GTMS-12': {}, 'GTMS-11': {}, 'GTMS-10': {}, 'GTMS-9': {}, 'GTMS-8': {}, 'GTMS-7': {}, 'GTMS-6': {}, 'GTMS-5': {}}}}\n",
      "\n",
      "********************\n",
      "{'projects': {'GTMS': {'GTMS-3': {}, 'GTMS-2': {}, 'GTMS-1': {}}, 'IT': {'IT-3': {}, 'IT-2': {}, 'IT-1': {}}}}\n",
      "\n",
      "********************\n",
      "{'projects': {'ITSAMPLE': {'ITSAMPLE-12': {}, 'ITSAMPLE-11': {}, 'ITSAMPLE-10': {}, 'ITSAMPLE-9': {}, 'ITSAMPLE-8': {}, 'ITSAMPLE-7': {}, 'ITSAMPLE-6': {}, 'ITSAMPLE-5': {}, 'ITSAMPLE-4': {}, 'ITSAMPLE-3': {}, 'ITSAMPLE-2': {}, 'ITSAMPLE-1': {}}, 'MAR': {'MAR-2': {}, 'MAR-1': {}}}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in json_chunks:\n",
    "    print(\"*\"*20)\n",
    "    print(chunk)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4004c76d-c72a-4276-b45f-678d6b24182f",
   "metadata": {},
   "source": [
    "********************\n",
    "{'projects': {'AS': {'AS-1': {}}, 'DLP': {'DLP-7': {}, 'DLP-6': {}, 'DLP-5': {}, 'DLP-4': {}, 'DLP-3': {}, 'DLP-2': {}, 'DLP-1': {}}}}\n",
    "\n",
    "********************\n",
    "{'projects': {'GTMS': {'GTMS-22': {}, 'GTMS-21': {}, 'GTMS-20': {}, 'GTMS-19': {}, 'GTMS-18': {}, 'GTMS-17': {}, 'GTMS-16': {}, 'GTMS-15': {}, 'GTMS-14': {}, 'GTMS-13': {}, 'GTMS-12': {}, 'GTMS-11': {}}}}\n",
    "\n",
    "********************\n",
    "{'projects': {'GTMS': {'GTMS-9': {}, 'GTMS-8': {}, 'GTMS-7': {}, 'GTMS-6': {}, 'GTMS-5': {}, 'GTMS-4': {}, 'GTMS-3': {}, 'GTMS-2': {}, 'GTMS-1': {}}, 'IT': {'IT-3': {}, 'IT-2': {}, 'IT-1': {}}}}\n",
    "\n",
    "********************\n",
    "{'projects': {'ITSAMPLE': {'ITSAMPLE-12': {}, 'ITSAMPLE-11': {}, 'ITSAMPLE-10': {}, 'ITSAMPLE-9': {}, 'ITSAMPLE-8': {}, 'ITSAMPLE-7': {}, 'ITSAMPLE-6': {}, 'ITSAMPLE-5': {}, 'ITSAMPLE-4': {}, 'ITSAMPLE-3': {}}}}\n",
    "\n",
    "********************\n",
    "{'projects': {'ITSAMPLE': {'ITSAMPLE-1': {}}, 'MAR': {'MAR-2': {}, 'MAR-1': {}}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86358298-664a-45e8-b99f-c9ba405e4cb7",
   "metadata": {},
   "source": [
    "********************\n",
    "{'projects': {'AS': {'AS-1': {}}, 'DLP': {'DLP-7': {}, 'DLP-6': {}, 'DLP-5': {}, 'DLP-4': {}, 'DLP-3': {}, 'DLP-2': {}, 'DLP-1': {}}}}\n",
    "\n",
    "********************\n",
    "{'projects': {'GTMS': {'GTMS-22': {}, 'GTMS-21': {}, 'GTMS-20': {}, 'GTMS-19': {}, 'GTMS-18': {}, 'GTMS-17': {}, 'GTMS-16': {}, 'GTMS-15': {}, 'GTMS-14': {}, 'GTMS-13': {}, 'GTMS-12': {}, 'GTMS-11': {}, 'GTMS-10': {}, 'GTMS-9': {}, 'GTMS-8': {}, 'GTMS-7': {}, 'GTMS-6': {}, 'GTMS-5': {}}}}\n",
    "\n",
    "********************\n",
    "{'projects': {'GTMS': {'GTMS-3': {}, 'GTMS-2': {}, 'GTMS-1': {}}, 'IT': {'IT-3': {}, 'IT-2': {}, 'IT-1': {}}}}\n",
    "\n",
    "********************\n",
    "{'projects': {'ITSAMPLE': {'ITSAMPLE-12': {}, 'ITSAMPLE-11': {}, 'ITSAMPLE-10': {}, 'ITSAMPLE-9': {}, 'ITSAMPLE-8': {}, 'ITSAMPLE-7': {}, 'ITSAMPLE-6': {}, 'ITSAMPLE-5': {}, 'ITSAMPLE-4': {}, 'ITSAMPLE-3': {}, 'ITSAMPLE-2': {}, 'ITSAMPLE-1': {}}, 'MAR': {'MAR-2': {}, 'MAR-1': {}}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7b26c1-7ae2-473a-ba7d-a51fd386b100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593fc6cc-6b83-4f06-98ee-618d518f3df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8940c3-14dc-4570-8c96-cbc655a06729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b38ff7-9b49-463c-b443-2af51e00a0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a34c6b6e-bfd7-4ab2-a8c6-d93d676b0bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'a': 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter = RecursiveJsonSplitter(max_chunk_size=2000)\n",
    "splitter.split_json({\"a\": \"x\" * 3000})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ae55e8-9356-46c5-a8b0-2c1c4e235dba",
   "metadata": {},
   "source": [
    "#### **ISSUE DETECTED**\n",
    "\n",
    "Line: 97-100\n",
    "```python\n",
    "if size < remaining:\n",
    "    # Add item to current chunk\n",
    "    self._set_nested_dict(chunks[-1], new_path, value)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db7ea716-4bd3-480a-934c-37ee065d6c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"JSON text splitter.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import copy\n",
    "import json\n",
    "from typing import Any\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "class RecursiveJsonSplitter_v2:\n",
    "    \"\"\"Splits JSON data into smaller, structured chunks while preserving hierarchy.\n",
    "\n",
    "    This class provides methods to split JSON data into smaller dictionaries or\n",
    "    JSON-formatted strings based on configurable maximum and minimum chunk sizes.\n",
    "    It supports nested JSON structures, optionally converts lists into dictionaries\n",
    "    for better chunking, and allows the creation of document objects for further use.\n",
    "    \"\"\"\n",
    "\n",
    "    max_chunk_size: int = 2000\n",
    "    \"\"\"The maximum size for each chunk.\"\"\"\n",
    "    min_chunk_size: int = 1800\n",
    "    \"\"\"The minimum size for each chunk, derived from `max_chunk_size` if not\n",
    "    explicitly provided.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, max_chunk_size: int = 2000, min_chunk_size: int | None = None\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the chunk size configuration for text processing.\n",
    "\n",
    "        This constructor sets up the maximum and minimum chunk sizes, ensuring that\n",
    "        the `min_chunk_size` defaults to a value slightly smaller than the\n",
    "        `max_chunk_size` if not explicitly provided.\n",
    "\n",
    "        Args:\n",
    "            max_chunk_size: The maximum size for a chunk.\n",
    "            min_chunk_size: The minimum size for a chunk. If `None`,\n",
    "                defaults to the maximum chunk size minus 200, with a lower bound of 50.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.max_chunk_size = max_chunk_size\n",
    "        self.min_chunk_size = (\n",
    "            min_chunk_size\n",
    "            if min_chunk_size is not None\n",
    "            else max(max_chunk_size - 200, 50)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _json_size(data: dict[str, Any]) -> int:\n",
    "        \"\"\"Calculate the size of the serialized JSON object.\"\"\"\n",
    "        return len(json.dumps(data))\n",
    "\n",
    "    @staticmethod\n",
    "    def _set_nested_dict(\n",
    "        d: dict[str, Any],\n",
    "        path: list[str],\n",
    "        value: Any,  # noqa: ANN401\n",
    "    ) -> None:\n",
    "        \"\"\"Set a value in a nested dictionary based on the given path.\"\"\"\n",
    "        for key in path[:-1]:\n",
    "            d = d.setdefault(key, {})\n",
    "        d[path[-1]] = value\n",
    "\n",
    "    def _list_to_dict_preprocessing(\n",
    "        self,\n",
    "        data: Any,  # noqa: ANN401\n",
    "    ) -> Any:  # noqa: ANN401\n",
    "        if isinstance(data, dict):\n",
    "            # Process each key-value pair in the dictionary\n",
    "            return {k: self._list_to_dict_preprocessing(v) for k, v in data.items()}\n",
    "        if isinstance(data, list):\n",
    "            # Convert the list to a dictionary with index-based keys\n",
    "            return {\n",
    "                str(i): self._list_to_dict_preprocessing(item)\n",
    "                for i, item in enumerate(data)\n",
    "            }\n",
    "        # Base case: the item is neither a dict nor a list, so return it unchanged\n",
    "        return data\n",
    "\n",
    "    def _json_split(\n",
    "        self,\n",
    "        data: Any,  # noqa: ANN401\n",
    "        current_path: list[str] | None = None,\n",
    "        chunks: list[dict[str, Any]] | None = None,\n",
    "    ) -> list[dict[str, Any]]:\n",
    "        \"\"\"Split json into maximum size dictionaries while preserving structure.\"\"\"\n",
    "        current_path = current_path or []\n",
    "        chunks = chunks if chunks is not None else [{}]\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                new_path = [*current_path, key]\n",
    "                chunk_size = self._json_size(chunks[-1])\n",
    "                size = self._json_size({key: value})\n",
    "                remaining = self.max_chunk_size - chunk_size\n",
    "\n",
    "                if size <= remaining:\n",
    "                    # Add item to current chunk\n",
    "                    self._set_nested_dict(chunks[-1], new_path, value)\n",
    "                else:\n",
    "                    if chunks[-1]:\n",
    "                        # Chunk is big enough, start a new chunk\n",
    "                        chunks.append({})\n",
    "                    # Iterate\n",
    "                    self._json_split(value, new_path, chunks)\n",
    "        else:\n",
    "            # handle single item\n",
    "            self._set_nested_dict(chunks[-1], current_path, data)\n",
    "        return chunks\n",
    "\n",
    "    def split_json(\n",
    "        self,\n",
    "        json_data: dict[str, Any],\n",
    "        convert_lists: bool = False,  # noqa: FBT001,FBT002\n",
    "    ) -> list[dict[str, Any]]:\n",
    "        \"\"\"Splits JSON into a list of JSON chunks.\"\"\"\n",
    "        if convert_lists:\n",
    "            chunks = self._json_split(self._list_to_dict_preprocessing(json_data))\n",
    "        else:\n",
    "            chunks = self._json_split(json_data)\n",
    "\n",
    "        # Remove the last chunk if it's empty\n",
    "        # if not chunks[-1]:\n",
    "        #     chunks.pop()\n",
    "        return chunks\n",
    "\n",
    "    def split_text(\n",
    "        self,\n",
    "        json_data: dict[str, Any],\n",
    "        convert_lists: bool = False,  # noqa: FBT001,FBT002\n",
    "        ensure_ascii: bool = True,  # noqa: FBT001,FBT002\n",
    "    ) -> list[str]:\n",
    "        \"\"\"Splits JSON into a list of JSON formatted strings.\"\"\"\n",
    "        chunks = self.split_json(json_data=json_data, convert_lists=convert_lists)\n",
    "\n",
    "        # Convert to string\n",
    "        return [json.dumps(chunk, ensure_ascii=ensure_ascii) for chunk in chunks]\n",
    "\n",
    "    def create_documents(\n",
    "        self,\n",
    "        texts: list[dict[str, Any]],\n",
    "        convert_lists: bool = False,  # noqa: FBT001,FBT002\n",
    "        ensure_ascii: bool = True,  # noqa: FBT001,FBT002\n",
    "        metadatas: list[dict[Any, Any]] | None = None,\n",
    "    ) -> list[Document]:\n",
    "        \"\"\"Create a list of `Document` objects from a list of json objects (`dict`).\"\"\"\n",
    "        metadatas_ = metadatas or [{}] * len(texts)\n",
    "        documents = []\n",
    "        for i, text in enumerate(texts):\n",
    "            for chunk in self.split_text(\n",
    "                json_data=text, convert_lists=convert_lists, ensure_ascii=ensure_ascii\n",
    "            ):\n",
    "                metadata = copy.deepcopy(metadatas_[i])\n",
    "                new_doc = Document(page_content=chunk, metadata=metadata)\n",
    "                documents.append(new_doc)\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6432ac8-47e8-46c8-a820-feadb2a60298",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\n",
    "  \"projects\": {\n",
    "    \"AS\": {\n",
    "      \"AS-1\": {}\n",
    "    },\n",
    "    \"DLP\": {\n",
    "      \"DLP-7\": {},\n",
    "      \"DLP-6\": {},\n",
    "      \"DLP-5\": {},\n",
    "      \"DLP-4\": {},\n",
    "      \"DLP-3\": {},\n",
    "      \"DLP-2\": {},\n",
    "      \"DLP-1\": {}\n",
    "    },\n",
    "    \"GTMS\": {\n",
    "      \"GTMS-22\": {},\n",
    "      \"GTMS-21\": {},\n",
    "      \"GTMS-20\": {},\n",
    "      \"GTMS-19\": {},\n",
    "      \"GTMS-18\": {},\n",
    "      \"GTMS-17\": {},\n",
    "      \"GTMS-16\": {},\n",
    "      \"GTMS-15\": {},\n",
    "      \"GTMS-14\": {},\n",
    "      \"GTMS-13\": {},\n",
    "      \"GTMS-12\": {},\n",
    "      \"GTMS-11\": {},\n",
    "      \"GTMS-10\": {},\n",
    "      \"GTMS-9\": {},\n",
    "      \"GTMS-8\": {},\n",
    "      \"GTMS-7\": {},\n",
    "      \"GTMS-6\": {},\n",
    "      \"GTMS-5\": {},\n",
    "      \"GTMS-4\": {},\n",
    "      \"GTMS-3\": {},\n",
    "      \"GTMS-2\": {},\n",
    "      \"GTMS-1\": {}\n",
    "    },\n",
    "    \"IT\": {\n",
    "      \"IT-3\": {},\n",
    "      \"IT-2\": {},\n",
    "      \"IT-1\": {}\n",
    "    },\n",
    "    \"ITSAMPLE\": {\n",
    "      \"ITSAMPLE-12\": {},\n",
    "      \"ITSAMPLE-11\": {},\n",
    "      \"ITSAMPLE-10\": {},\n",
    "      \"ITSAMPLE-9\": {},\n",
    "      \"ITSAMPLE-8\": {},\n",
    "      \"ITSAMPLE-7\": {},\n",
    "      \"ITSAMPLE-6\": {},\n",
    "      \"ITSAMPLE-5\": {},\n",
    "      \"ITSAMPLE-4\": {},\n",
    "      \"ITSAMPLE-3\": {},\n",
    "      \"ITSAMPLE-2\": {},\n",
    "      \"ITSAMPLE-1\": {}\n",
    "    },\n",
    "    \"MAR\": {\n",
    "      \"MAR-2\": {},\n",
    "      \"MAR-1\": {}\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88736653-8164-49c4-89f8-c080dece1389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULTS: 200\n",
      "\n",
      "RESULTS: 201\n",
      "\n",
      "RESULTS: 202\n",
      "\n",
      "RESULTS: 203\n",
      "\n",
      "RESULTS: 204\n",
      "[TEST FAILED] - Node GTMS-10 from input_data was NOT FOUND in json_chunks\n",
      "\n",
      "RESULTS: 205\n",
      "[TEST FAILED] - Node GTMS-10 from input_data was NOT FOUND in json_chunks\n",
      "\n",
      "RESULTS: 206\n",
      "[TEST FAILED] - Node GTMS-10 from input_data was NOT FOUND in json_chunks\n",
      "\n",
      "RESULTS: 207\n",
      "[TEST FAILED] - Node GTMS-10 from input_data was NOT FOUND in json_chunks\n",
      "\n",
      "RESULTS: 208\n",
      "[TEST FAILED] - Node GTMS-10 from input_data was NOT FOUND in json_chunks\n",
      "\n",
      "RESULTS: 209\n",
      "[TEST FAILED] - Node GTMS-10 from input_data was NOT FOUND in json_chunks\n",
      "\n",
      "RESULTS: 210\n",
      "[TEST FAILED] - Node GTMS-10 from input_data was NOT FOUND in json_chunks\n",
      "\n",
      "RESULTS: 211\n",
      "[TEST FAILED] - Node GTMS-10 from input_data was NOT FOUND in json_chunks\n",
      "[TEST FAILED] - Node ITSAMPLE-2 from input_data was NOT FOUND in json_chunks\n",
      "\n",
      "RESULTS: 212\n",
      "[TEST FAILED] - Node GTMS-10 from input_data was NOT FOUND in json_chunks\n",
      "[TEST FAILED] - Node ITSAMPLE-2 from input_data was NOT FOUND in json_chunks\n",
      "\n",
      "RESULTS: 213\n",
      "[TEST FAILED] - Node GTMS-10 from input_data was NOT FOUND in json_chunks\n",
      "[TEST FAILED] - Node ITSAMPLE-2 from input_data was NOT FOUND in json_chunks\n",
      "\n",
      "RESULTS: 214\n",
      "[TEST FAILED] - Node GTMS-10 from input_data was NOT FOUND in json_chunks\n",
      "[TEST FAILED] - Node ITSAMPLE-2 from input_data was NOT FOUND in json_chunks\n",
      "\n",
      "RESULTS: 215\n",
      "[TEST FAILED] - Node GTMS-10 from input_data was NOT FOUND in json_chunks\n",
      "[TEST FAILED] - Node ITSAMPLE-2 from input_data was NOT FOUND in json_chunks\n",
      "\n",
      "RESULTS: 216\n",
      "[TEST FAILED] - Node GTMS-10 from input_data was NOT FOUND in json_chunks\n",
      "[TEST FAILED] - Node ITSAMPLE-2 from input_data was NOT FOUND in json_chunks\n",
      "\n",
      "RESULTS: 217\n",
      "[TEST FAILED] - Node GTMS-10 from input_data was NOT FOUND in json_chunks\n",
      "[TEST FAILED] - Node ITSAMPLE-2 from input_data was NOT FOUND in json_chunks\n",
      "\n",
      "RESULTS: 218\n",
      "[TEST FAILED] - Node GTMS-10 from input_data was NOT FOUND in json_chunks\n",
      "[TEST FAILED] - Node ITSAMPLE-2 from input_data was NOT FOUND in json_chunks\n",
      "\n",
      "RESULTS: 219\n",
      "[TEST FAILED] - Node ITSAMPLE-2 from input_data was NOT FOUND in json_chunks\n",
      "\n",
      "RESULTS: 220\n",
      "[TEST FAILED] - Node ITSAMPLE-2 from input_data was NOT FOUND in json_chunks\n",
      "\n",
      "RESULTS: 221\n",
      "[TEST FAILED] - Node ITSAMPLE-2 from input_data was NOT FOUND in json_chunks\n",
      "\n",
      "RESULTS: 222\n",
      "[TEST FAILED] - Node ITSAMPLE-2 from input_data was NOT FOUND in json_chunks\n",
      "\n",
      "RESULTS: 223\n",
      "[TEST FAILED] - Node ITSAMPLE-2 from input_data was NOT FOUND in json_chunks\n",
      "\n",
      "RESULTS: 224\n",
      "[TEST FAILED] - Node ITSAMPLE-2 from input_data was NOT FOUND in json_chunks\n",
      "\n",
      "RESULTS: 225\n",
      "[TEST FAILED] - Node ITSAMPLE-2 from input_data was NOT FOUND in json_chunks\n",
      "\n",
      "RESULTS: 226\n",
      "[TEST FAILED] - Node ITSAMPLE-2 from input_data was NOT FOUND in json_chunks\n",
      "\n",
      "RESULTS: 227\n",
      "[TEST FAILED] - Node ITSAMPLE-2 from input_data was NOT FOUND in json_chunks\n",
      "\n",
      "RESULTS: 228\n",
      "[TEST FAILED] - Node ITSAMPLE-2 from input_data was NOT FOUND in json_chunks\n",
      "\n",
      "RESULTS: 229\n",
      "\n",
      "RESULTS: 230\n",
      "\n",
      "RESULTS: 231\n",
      "\n",
      "RESULTS: 232\n",
      "\n",
      "RESULTS: 233\n",
      "\n",
      "RESULTS: 234\n",
      "\n",
      "RESULTS: 235\n",
      "\n",
      "RESULTS: 236\n",
      "\n",
      "RESULTS: 237\n",
      "\n",
      "RESULTS: 238\n",
      "\n",
      "RESULTS: 239\n",
      "\n",
      "RESULTS: 240\n",
      "\n",
      "RESULTS: 241\n",
      "\n",
      "RESULTS: 242\n",
      "\n",
      "RESULTS: 243\n",
      "\n",
      "RESULTS: 244\n",
      "\n",
      "RESULTS: 245\n",
      "\n",
      "RESULTS: 246\n",
      "\n",
      "RESULTS: 247\n",
      "\n",
      "RESULTS: 248\n",
      "\n",
      "RESULTS: 249\n"
     ]
    }
   ],
   "source": [
    "for i in range(200, 250):\n",
    "    splitter = RecursiveJsonSplitter_v2(max_chunk_size=i)\n",
    "    json_chunks = splitter.split_json(json_data=input_data)\n",
    "    \n",
    "    input_data_DLP_5 = input_data.get(\"projects\", {}).get(\"DLP\", {}).get(\"DLP-5\", None)\n",
    "    input_data_GTMS_10 = input_data.get(\"projects\", {}).get(\"GTMS\", {}).get(\"GTMS-10\", None)\n",
    "    input_data_ITSAMPLE_2 = input_data.get(\"projects\", {}).get(\"ITSAMPLE\", {}).get(\"ITSAMPLE-2\", None)\n",
    "    \n",
    "    chunk_DLP_5 = None\n",
    "    chunk_GTMS_10 = None\n",
    "    chunk_ITSAMPLE_2 = None\n",
    "    \n",
    "    for chunk in json_chunks:\n",
    "        # print(chunk)\n",
    "        node = chunk.get(\"projects\", {}).get(\"DLP\", {}).get(\"DLP-5\", None)\n",
    "        if isinstance(node, dict):\n",
    "            chunk_DLP_5 = node\n",
    "        node = chunk.get(\"projects\", {}).get(\"GTMS\", {}).get(\"GTMS-10\", None)\n",
    "        if isinstance(node, dict):\n",
    "            chunk_GTMS_10 = node\n",
    "        node = chunk.get(\"projects\", {}).get(\"ITSAMPLE\", {}).get(\"ITSAMPLE-2\", None)\n",
    "        if isinstance(node, dict):\n",
    "            chunk_ITSAMPLE_2 = node\n",
    "    \n",
    "    print(f\"\\nRESULTS: {i}\")\n",
    "    if isinstance(chunk_DLP_5, dict):\n",
    "        # print(f\"[PASS] - Node DLP-5 was found both in input_data and json_chunks\")\n",
    "        pass\n",
    "    else:\n",
    "        print(f\"[TEST FAILED] - Node DLP-5 from input_data was NOT FOUND in json_chunks\")\n",
    "    \n",
    "    if isinstance(chunk_GTMS_10, dict):\n",
    "        # print(f\"[PASS] - Node GTMS-10 was found both in input_data and json_chunks\")\n",
    "        pass\n",
    "    else:\n",
    "        print(f\"[TEST FAILED] - Node GTMS-10 from input_data was NOT FOUND in json_chunks\")\n",
    "    \n",
    "    if isinstance(chunk_ITSAMPLE_2, dict):\n",
    "        # print(f\"[PASS] - Node ITSAMPLE-2 was found both in input_data and json_chunks\")\n",
    "        pass\n",
    "    else:\n",
    "        print(f\"[TEST FAILED] - Node ITSAMPLE-2 from input_data was NOT FOUND in json_chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d685d-6e46-4ad1-b7c3-71c66748f252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
