{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d4089c9-645a-43b1-bd0e-ca87264b5306",
   "metadata": {},
   "source": [
    "# **Building Retrieval-Augmented Generation**\n",
    "\n",
    "## **What's Covered?**\n",
    "1. Introduction to RAG\n",
    "    - Why RAG?\n",
    "    - Benefits of RAG\n",
    "    - RAG Building Blocks\n",
    "    - Application Areas\n",
    "2. How to Build a RAG System?\n",
    "    - The Process\n",
    "    - What is an Index?\n",
    "    - What is a Retriever?\n",
    "3. Building an End-to-End RAG Chain\n",
    "    - Step 1: Initialize the Chroma DB Connection\n",
    "    - Step 2: Create a Retriever Object\n",
    "    - Step 3: Initialize a Chat Prompt Template\n",
    "    - Step 4: Initialize a Generator (i.e. Chat Model)\n",
    "    - Step 5: Initialize a Output Parser\n",
    "    - Step 6: Define a RAG Chain\n",
    "    - Step 7: Invoke the Chain\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc758212-6883-457c-ae67-8926fc72d296",
   "metadata": {},
   "source": [
    "## **Introduction to RAG**\n",
    "\n",
    "### **Why RAG?**\n",
    "We know that LLMs have the capability to generate stuff by themselves. But these tools aren't perfect.\n",
    "\n",
    "Even though they're super smart, they sometimes get things wrong, especially if they need to be really precise or use the latest information. So, to fix this, some of the brightest minds at Meta AI came up with a new trick called retrieval-augmented generation, or RAG for short, in 2020.\n",
    "\n",
    "Think of it as giving our language models an assistant. This assistant digs through a massive pile of updated information and feeds the most relevant and recent bits to the LLM.\n",
    "\n",
    "### **Benefits of RAG**  \n",
    "1. **Enhanced factual accuracy and Domain Specific Expertise:** Imagine a customer service chatbot trained on general conversation data. It might struggle with technical domain specific questions. RAGs allow you to integrate domain-specific knowledge bases, enabling the chatbot to handle these inquires with expertise.\n",
    "2. **Reduce Hallucination:** LLMs can generate false information, a phenomenon known as hallucination. The knowledge base provided can help support the claims of generative model.\n",
    "\n",
    "### **RAG Building Blocks**  \n",
    "1. **Retrieval:** When a user asks a question or provides a prompt, retrievals first help fetch relevant passages from a vast knowledge base. This Knowledge Base could be the company's internal documents, or any other source of text data.\n",
    "2. **Augmentation:** The retrieved passages are then used to \"augment\" the LLM's knowledge. This can include various techniques, such as summarization or encoding the key information.\n",
    "3. **Generation:** Finally LLM leverages its understanding of language along with the augmented information to generate a response. This response can be an answer to a question, a creative text format based on a prompt, etc...\n",
    "\n",
    "### **Application Areas**  \n",
    "1. Question Answering: A RAG powered customer care chatbot can answer customer queries by retrieving product information, FAQs and guides to provide a well-rounded response.\n",
    "2. Document Summarization: A research paper summarization tool can use RAG to retrieve relevant sections and then generate a summary highlighting main points.\n",
    "3. Creative Text Generation: A story writing assistant can use RAG to retrieve information about historical periods or fictional creation, helping LLM to generate more deeply engaging stories.\n",
    "4. Code Generation: A code completion tool can use RAG to retrieve relevant code examples and API documentation, helping developers write code more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0a88f2-06bc-414b-bec8-ab9235b27ab7",
   "metadata": {},
   "source": [
    "## **How to Build a RAG System?**\n",
    "\n",
    "### **The Process**\n",
    "**Step 1: Create an Index on available Knowledge Base i.e. the preprocessing strategy**\n",
    "- This step is about preprocessing and organizing the raw data so it can be searched efficiently later.\n",
    "- Data from formats like PDF, HTML, etc is cleaned and converted into plain text. This text is then divided into smaller parts (i.e chunks) and turned into vector representations by passing the chunks into the embedding model to make it easier to find later.\n",
    "- **Key Outcome:** You now have a vector store that contains all chunks from your knowledge base in vector format.\n",
    "\n",
    "**Step 2: Create a Retrieval i.e. the search strategy**\n",
    "- This step defines how you want to search inside the index at query time. It does not store embeddings or build new indexes—it only queries the index.\n",
    "- When someone asks a question, the Retrieval system turns that question into vector embedding using the same method used in indexing. Then, it compares this vector to the vectors of the indexed text parts to fing the `k` most similar chunks. These `k` most similar chunks are used in the next step as a context.\n",
    "- You must specify:\n",
    "    - How many results to return (k)\n",
    "    - What similarity metric to use (eucledian, cosine, etc...)\n",
    "    - Whether to use MMR, hybrid search, metadata filters, reranking, etc...\n",
    "- **Key Outcome:** This stage is real-time and repeated every query. When the user asks a question, the retriever:\n",
    "    - embeds the question\n",
    "    - runs similarity search on the existing index\n",
    "    - returns top relevant chunks\n",
    "\n",
    "**Step 3: Generation**\n",
    "- The system combines the retrieved text parts (i.e. context) with the original question to create a prompt. The LLM uses this prompt to answer the question.\n",
    "\n",
    "\n",
    "### **What are Retrievals?**\n",
    "Understand that the retrievals are specialized in navigating through vast amounts of data to find information that is relevant to a specific query or context. \n",
    "\n",
    "Retrieval models focus on the precision of matching query criteria with the data they have access to. Note that retrieval models rely heavily on the quality and structure of the data they access. Their performance depends on the relevance and accuracy of the information stored in the databases they query. \n",
    "\n",
    "In simple terms, retrievals search and identify relevant data from a large corpus for a given query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975535f2-73e5-4389-91d4-1b9e2594394d",
   "metadata": {},
   "source": [
    "## **Building an End-to-End RAG Chain**\n",
    "\n",
    "**Step 1: Initialize the Chroma DB Connection**  \n",
    "**Step 2: Create a Retriever Object**   \n",
    "**Step 3: Initialize a Chat Prompt Template**  \n",
    "**Step 4: Initialize a Generator (i.e. Chat Model)**  \n",
    "**Step 5: Initialize a Output Parser**   \n",
    "**Step 6: Define a RAG Chain**  \n",
    "**Step 7: Invoke the Chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09718171-f4fe-4886-9c37-e9ba72130758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1004\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize the Chroma DB Connection\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Initialize the database connection\n",
    "# If database exist, it will connect with the collection_name and persist_directory\n",
    "# Otherwise a new collection will be created\n",
    "db = Chroma(collection_name=\"vector_database\", \n",
    "            embedding_function=embedding_model, \n",
    "            persist_directory=\"./chroma_db_\")\n",
    "\n",
    "# We can check the already existing values\n",
    "print(len(db.get()[\"ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e12a3280-28ef-432f-a76a-53922080bfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a Retriever Object \n",
    "\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19c140db-1f33-440c-98c9-21daec76b55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize a Chat Prompt Template\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "Answer the question based on the above context: {question}.\n",
    "Provide a detailed answer.\n",
    "Don’t justify your answers.\n",
    "Don’t give information not mentioned in the CONTEXT INFORMATION.\n",
    "Do not say \"according to the context\" or \"mentioned in the context\" or similar.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0466bc9-8813-45ba-af63-8d58f79d532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Initialize a Generator (i.e. Chat Model)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "289ed96b-977b-46e9-982a-bee809b5081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Initialize a Output Parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0eac1d29-fe72-449e-99be-8cf5a05dcf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Define a RAG Chain\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "rag_chain = {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} | prompt_template | chat_model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ff4f3fd-f29a-4f5f-b3c1-4997db26fa3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rachem is not a person, it is a misinterpretation of the name Rachel. The speaker is questioning what the term \"Rachem\" means and if it is a term related to paleontology. They clarify that they wouldn\\'t know because they are just a waitress.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the Chain\n",
    "\n",
    "query = 'Who is Rachem?'\n",
    "\n",
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2109d2d6-a41a-4498-ad16-8d8e9fd198e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rachel is described as being a waitress, while Julie is mentioned to have a lot in common with the speaker as they are both paleontologists.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the Chain\n",
    "\n",
    "query = 'What is there on the List comparing Rachel and Julie?'\n",
    "\n",
    "rag_chain.invoke(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
